file_name,question,answer,label
1706.09147,What is the new initialization method proposed in this paper?,They initialize their word and entity embeddings with vectors pre-trained over a large corpus of unlabeled data.,"methodology,introduction,experiment"
1706.09147,How was a quality control performed so that the text is noisy but the annotations are accurate?,The authors believe that the Wikilinks corpus  contains ground truth annotations while being noisy. They discard mentions that cannot have ground-truth verified by comparison with Wikipedia.,"methodology,experiment,result_discussion"
1706.09673,Which dataset do they use?,"Paraphrase Database (PPDB) ,  book corpus","dataset,methodology,experiment"
1706.09673,Do they evaluate their learned representations on downstream tasks?,No|,"experiment,result_discussion,methodology"
1706.09673,How do they encourage understanding of literature as part of their objective function?,"They group the existing works in terms of the objective function they optimize - within-tweet relationships, inter-tweet relationships, autoencoder, and weak supervision","""introduction"",""methodology"",""result_discussion"""
1707.0011,Which baseline methods are used?,Standard parametrized attention and a non-attention baseline,"experiment,methodology,related_work"
1707.0011,Which datasets are used in experiments?,Sequence Copy Task and WMT'17,"""dataset"",""experiment"",""methodology"""
1707.00995,What misbehavior is identified?,"""if the attention loose track of the objects in the picture and â€˜gets lostâ€™ the model still takes it into account and somehow overrides the information brought by the text-based annotations""","""result_discussion"",""experiment"",""conclusion"""
1707.00995,Which attention mechanisms do they compare?,Soft attention Hard Stochastic attention Local Attention| | ,"methodology,experiment,result_discussion"
1706.08198,Is pre-training effective in their evaluation?,Yes|,"experiment,result_discussion,conclusion"
1706.08198,What parallel corpus did they use?,"Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0,NTCIR PatentMT Parallel Corpus BIBREF1|","dataset,methodology,experiment"
1706.01875,What was the baseline?,"stochastic gradient descent, naive bayes, decision tree","""methodology"",""experiment"",""result_discussion"""
1706.01875,What was their system's performance?,accuracy and F1-score of 89.6% and 89.2% respectively,"result_discussion,experiment,conclusion"
1706.01875,What other political events are included in the database?,"US presidential primaries, Democratic and Republican National Conventions| ","""dataset"",""background"",""result_discussion"""
1706.01875,What classifier did they use?,Random Forest,"methodology,experiment,result_discussion"
1801.02243,Do the authors give any examples of major events which draw the public's attention and the impact they have on stock price?,Yes,"introduction,experiment,result_discussion"
1801.02243,Which tweets are used to output the daily sentiment signal?,Tesla and Ford are investigated on how Twitter sentiment could impact the stock price,"methodology,experiment,result_discussion"
1801.02243,What is the baseline machine learning prediction approach?,linear logistic regression to a set of stock technical signals,"methodology,background,experiment"
1801.02073,Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?,Yes,"experiment,dataset,abstract"
1801.02073,How many question types do they find in the datasets analyzed?,"Seven,7.","dataset,experiment,introduction"
1801.02073,How do they analyze contextual similaries across datasets?,They compare the tasks that the datasets are suitable for  average number of answer candidates per question number of token types average answer candidate lengths average question lengths question-answer word overlap.| | ,"methodology,experiment,introduction"
1707.02377,Which language models do they compare against?,RNNLM BIBREF11,"experiment,related_work,methodology"
1707.02377,Is their approach similar to making an averaged weighted sum of word vectors where weights reflect word frequencies?,Different from their work we choose to corrupt the original document by randomly removing significant portion of words and represent the document using only the embeddings of the words remained.,"methodology,experiment,result_discussion"
1707.02377,How do they determine which words are informative?,Informative are those that will not be suppressed by regularization performed.,"methodology,experiment,result_discussion"