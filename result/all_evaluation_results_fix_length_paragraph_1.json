{
  "individual_document_results": {
    "Literature_paper.pdf": {
      "error": "No valid question-answer pairs to evaluate",
      "average_exact_match": 0.0,
      "average_bleu_score": 0.0,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        },
        "rouge-2": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        },
        "rouge-L": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        }
      },
      "per_question_metrics": []
    },
    "1706.09147.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.09934975213152254,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.23636363636363636,
          "recall": 0.5462962962962963,
          "f1": 0.31838267786124613
        },
        "rouge-2": {
          "precision": 0.1111111111111111,
          "recall": 0.35294117647058826,
          "f1": 0.16901408450704225
        },
        "rouge-L": {
          "precision": 0.20779220779220778,
          "recall": 0.5092592592592593,
          "f1": 0.28612461334511713
        }
      },
      "per_question_metrics": [
        {
          "question": "What is the new initialization method proposed in this paper?",
          "generated_answer": "The new initialization method proposed in this paper involves explicitly initializing word and entity embeddings with vectors pre-trained over a large collection of unlabeled data. This is achieved through an approach based on the Skip-Gram with Negative-Sampling (SGNS) algorithm by Mikolov et al. (2013), which simultaneously trains both word and entity vectors.",
          "reference_answer": "They initialize their word and entity embeddings with vectors pre-trained over a large corpus of unlabeled data.",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.2727272727272727,
              "recall": 0.8333333333333334,
              "f1": 0.4109589041095891
            },
            "rouge-2": {
              "precision": 0.2222222222222222,
              "recall": 0.7058823529411765,
              "f1": 0.3380281690140845
            },
            "rouge-L": {
              "precision": 0.2727272727272727,
              "recall": 0.8333333333333334,
              "f1": 0.4109589041095891
            }
          },
          "bleu_score": 0.18408707168815167,
          "exact_match": 0.0
        },
        {
          "question": "How was a quality control performed so that the text is noisy but the annotations are accurate?",
          "generated_answer": "The provided context does not explicitly describe a quality control process to ensure that while the text remains noisy, the annotations are accurate. Therefore, I cannot directly answer the question based on the given text.",
          "reference_answer": "The authors believe that the Wikilinks corpus  contains ground truth annotations while being noisy. They discard mentions that cannot have ground-truth verified by comparison with Wikipedia.",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.2,
              "recall": 0.25925925925925924,
              "f1": 0.22580645161290322
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.14285714285714285,
              "recall": 0.18518518518518517,
              "f1": 0.16129032258064516
            }
          },
          "bleu_score": 0.014612432574893431,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "What is the new initialization method proposed in this paper?",
        "How was a quality control performed so that the text is noisy but the annotations are accurate?"
      ],
      "generated_answers": [
        {
          "answer": "The new initialization method proposed in this paper involves explicitly initializing word and entity embeddings with vectors pre-trained over a large collection of unlabeled data. This is achieved through an approach based on the Skip-Gram with Negative-Sampling (SGNS) algorithm by Mikolov et al. (2013), which simultaneously trains both word and entity vectors.",
          "relevant_chunks": [
            "4.2 Training\nWe assume our model is only given training ex-\namples for correct entity assignments and there-\nfore use corrupt-sampling, where we automati-\ncally generate examples of wrong assignments.\nFor each context-entity pair (c, e), where e is the\ncorrect assignment for c, we produce k corrupt ex-\namples with the same context c but with a differ-\nent, corrupt entity e\u2032. We considered two alterna-\ntives for corrupt sampling and provide an empiri-\ncal comparison of the two approaches (Section 5):\nNear-Misses: Sampling out of the candidate set\nof each mention. We have found this to be\nmore effective where the training data reli-\nably re\ufb02ects the test-set distribution.\nAll-Entity: Sampling from the entire dictionary\nof entities. Better suited to cases where the\ntraining data or candidate generation does not\nre\ufb02ect the test-set well. Has an added bene\ufb01t\nof allowing us to utilize unambiguous train-\ning examples where only a single candidate\nis found.\nWe sample corrupt examples uniformly in both\nalternatives since with uniform sampling the ratio\nbetween the number of positive and negative ex-\namples of an entity is higher for popular entities,\nthus biasing the network towards popular entities.\nIn the All-Entity case, this ratio is approximately\nproportional to the prior probability of the entity.\nWe note that preliminary experiments revealed\nthat corrupt-sampling according to the distribution\nof entities in the dataset (as is done by Mikolov\nat el. (2013)), rather than uniform sampling, did\nnot perform well in our settings due to the lack of\nbiasing toward popular entities.\nModel optimization was carried out using stan-\ndard backpropagation and an AdaGrad optimizer\n(Duchi et al., 2011). We allowed the error to prop-\nagate through all parts of the network and \ufb01ne\ntune all trainable parameters, including the word\nand entity embeddings themselves. We found the\nperformance of our model substantially improves\nfor the \ufb01rst few epochs and then continues to\nslowly converge with marginal gains, and there-\nfore trained all models for 8 epochs with k = 5\nfor corrupt-sampling.\n4.3 Embedding Initialization\nTraining our model implicitly embeds the vocabu-\nlary of words and collection of entities in a com-\nmon space. However, we found that explicitly\ninitializing these embeddings with vectors pre-\ntrained over a large collection of unlabeled data\nsigni\ufb01cantly improved performance (see Section\n5.3). To this end, we implemented an approach\nbased on the Skip-Gram with Negative-Sampling\n(SGNS) algorithm by Mikolov et al. (2013) that si-\nmultaneously trains both word and entity vectors.\nWe used word2vecf4 (Levy and Goldberg,\n2014a), which allows one to train word and con-\ntext embeddings using arbitrary de\ufb01nitions of\n\u201dword\u201d and \u201dcontext\u201d by providing a dataset of\nword-context pairs (w, c), rather than a textual\ncorpus. In our usage, we de\ufb01ne a context as an en-\ntity e. To compile a dataset of(w, e) pairs, we con-\nsider every word w that appeared in the Wikipedia\narticle describing entity e. We limit our vocabular-\nies to words that appeared at least 20 times in the\ncorpus and entities that contain at least 20 words\nin their articles. We ran the process for 10 epochs\nand produced vectors of 300 dimensions; other hy-\nperparameters were set to their defaults.\nLevy and Goldberg (2014b) showed that SGNS\nimplicitly factorizes the word-context PMI matrix.\nOur approach is doing the same for the word-entity\nPMI matrix, which is highly related to the word-\nentity TFIDF matrix used in Explicit Semantic\nAnalysis (Gabrilovich and Markovitch, 2007).\n5 Evaluation\nIn this section, we describe our experimental setup\nand compare our model to the state of the art\non two datasets: our new WikilinksNED dataset,\nas well as the commonly-used CoNLL-Y AGO\ndataset (Hoffart et al., 2011). We also examine the\neffect of different corrupt-sampling schemes, and\nof initializing our model with pre-trained word and\nentity embeddings.\nIn all experiments, our model was trained with\n\ufb01xed-size left and right contexts (20 words in each\nside). We used a special padding symbol when\nthe actual context was shorter than the window.\nFurther, we \ufb01ltered stopwords using NLTK\u2019s stop-\nword list prior to selecting the window in order to\nfocus on more informative words. Our model was\nimplemented using the Keras (Chollet, 2015) and\nTensor\ufb02ow (Abadi et al., 2015) libraries.\n4http://bitbucket.org/yoavgo/word2vecf\n\nfor entity and context, allowing it to extract signals\nfrom noisy and unexpected context patterns.\nWhile convolutional neural networks (Sun\net al., 2015; Francis-Landau et al., 2016) and prob-\nabilistic attention (Lazic et al., 2015) have been\napplied to the task, this is the \ufb01rst model to use\nRNNs and a neural attention model for NED.\nRNNs account for the sequential nature of textual\ncontext while the attention model is applied to re-\nduce the impact of noise in the text.\nOur experiments show that our model signi\ufb01-\ncantly outperforms existing state-of-the-art NED\nalgorithms on WikilinksNED, suggesting that\nRNNs with attention are able to model short and\nnoisy context better than current approaches. In\naddition, we evaluate our algorithm on CoNLL-\nY AGO (Hoffart et al., 2011), a dataset of annotated\nnews articles. We use a simple domain adapta-\ntion technique since CoNLL-Y AGO lacks a large\nenough training set for our model, and achieve\ncomparable results to other state-of-the-art meth-\nods. These experiments highlight the difference\nbetween the two datasets, indicating that our NED\nbenchmark is substantially more challenging.\nCode and data used for our experiments\ncan be found at https://github.com/\nyotam-happy/NEDforNoisyText\n2 Related Work\nLocal vs Global NED Early work on Named\nEntity Disambiguation, such as Bunescu and\nPas \u00b8ca (2006) and Mihalcea and Csomai (2007) fo-\ncused on local approaches where each mention is\ndisambiguated separately using hand-crafted fea-\ntures. While local approaches provide a hard-to-\nbeat baseline (Ratinov et al., 2011), recent work\nhas largely focused on global approaches. These\ndisambiguate all mentions within a document si-\nmultaneously by considering the coherency of en-\ntity assignments within a document. For exam-\nple the local component of the GLOW algorithm\n(Ratinov et al., 2011) was used as part of the re-\nlational inference system suggested by Cheng and\nRoth (2013). Similarly, Globerson et al. (2016)\nachieved state-of-the-art results by extending the\nlocal-based selective-context model of Lazic et al.\n(2015) with an attention-like coherence model.\nGlobal models can tap into highly-\ndiscriminative semantic signals (e.g. coreference\nand entity relatedness) that are unavailable to local\nmethods, and have signi\ufb01cantly outperformed the\nlocal approach on standard datasets (Guo and Bar-\nbosa, 2014; Pershina et al., 2015; Globerson et al.,\n2016). However, global approaches are dif\ufb01cult\nto apply in domains where only short and noisy\ntext is available, as often occurs in social media,\nquestions and answers, and other short web docu-\nments. For example, Huang et al. (2014) collected\nmany tweets from the same author in order to\napply a global disambiguation method. Since this\nwork focuses on disambiguating entities within\nshort fragments of text, our algorithmic approach\ntries to extract as much information from the local\ncontext, without resorting to external signals.\nNeural Approaches The \ufb01rst neural approach\nfor NED (He et al., 2013) used stacked auto-\nencoders to learn a similarity measure between\nmention-context structures and entity candidates.\nMore recently, convolutional neural networks\n(CNNs) were employed for learning semantic sim-\nilarity between context, mention, and candidate\ninputs (Sun et al., 2015; Francis-Landau et al.,\n2016). Neural embedding techniques have also\ninspired a number of works that measure entity-\ncontext relatedness using their embeddings (Ya-\nmada et al., 2016; Hu et al., 2015). In this paper,\nwe train a recurrent neural network (RNN) model,\nwhich unlike CNNs and embeddings, is designed\nto exploit the sequential nature of text. We also\nutilize an attention mechanism, inspired by results\nfrom Lazic et al. (2015) that successfully used a\nprobabilistic attention-like model for NED.\nNoisy Data Chisholm and Hachey (2015)\nshowed that despite the noisy nature of web data,\naugmenting Wikipedia-derived data with web-\nlinks from the Wikilinks corpus (Singh et al.,\n2012) can improve performance on standard\ndatasets. In our work, we \ufb01nd noisy web data to\nbe a unique and challenging test case for disam-\nbiguation. We therefore use Wikilinks to construct\na new stand-alone disambiguation benchmark that\nfocuses on noisy text, rather than use it for training\nalone. Moreover, we differ from Chisholm at el.\nby taking a neural approach that implicitly discov-\ners useful signals from contexts, instead of manu-\nally crafting features.\nCommonly-used benchmarks for NED sys-\ntems have mostly focused on news-based cor-\npora. CoNLL-Y AGO (Hoffart et al., 2011) is\na dataset based on Reuters, created by hand-\nannotating the CoNLL 2003 Named Entity Recog-\n\nNamed Entity Disambiguation for Noisy Text\nYotam Eshel1 Noam Cohen1 Kira Radinsky1,2\nShaul Markovitch1 Ikuya Yamada3 Omer Levy4\n1Technion - Israel Institute of Technology, Haifa, Israel\n2eBay Research, Israel\n3Studio Ousia, Fujisawa, Kanagawa, Japan\n4University of Washington, Seattle, W A\nAbstract\nWe address the task of Named Entity\nDisambiguation (NED) for noisy text.\nWe present WikilinksNED, a large-scale\nNED dataset of text fragments from the\nweb, which is signi\ufb01cantly noisier and\nmore challenging than existing news-\nbased datasets. To capture the limited\nand noisy local context surrounding each\nmention, we design a neural model and\ntrain it with a novel method for sam-\npling informative negative examples. We\nalso describe a new way of initializing\nword and entity embeddings that signi\ufb01-\ncantly improves performance. Our model\nsigni\ufb01cantly outperforms existing state-of-\nthe-art methods on WikilinksNED while\nachieving comparable performance on a\nsmaller newswire dataset.\n1 Introduction\nNamed Entity Disambiguation (NED) is the task\nof linking mentions of entities in text to a given\nknowledge base, such as Freebase or Wikipedia.\nNED is a key component in Entity Linking (EL)\nsystems, focusing on the disambiguation task it-\nself, independently from the tasks of Named En-\ntity Recognition (detecting mention bounds) and\nCandidate Generation (retrieving the set of poten-\ntial candidate entities). NED has been recognized\nas an important component in NLP tasks such as\nsemantic parsing (Berant and Liang, 2014).\nCurrent research on NED is mostly driven by\na number of standard datasets, such as CoNLL-\nY AGO (Hoffart et al., 2011), TAC KBP (Ji et al.,\n2010) and ACE (Bentivogli et al., 2010). These\ndatasets are based on news corpora and Wikipedia,\nwhich are naturally coherent, well-structured, and\nrich in context. Global disambiguation models\n(Guo and Barbosa, 2014; Pershina et al., 2015;\nGloberson et al., 2016) leverage this coherency by\njointly disambiguating all the mentions in a sin-\ngle document. However, domains such as web-\npage fragments, social media, or search queries,\nare often short, noisy, and less coherent; such do-\nmains lack the necessary contextual information\nfor global methods to pay off, and present a more\nchallenging setting in general.\nIn this work, we investigate the task of NED\nin a setting where only local and noisy context\nis available. In particular, we create a dataset\nof 3.2M short text fragments extracted from web\npages, each containing a mention of a named en-\ntity. Our dataset is far larger than previously col-\nlected datasets, and contains 18K unique mentions\nlinking to over 100K unique entities. We have em-\npirically found it to be noisier and more challeng-\ning than existing datasets. For example:\n\u201cI had no choice but to experiment with\nother indoor games. I was born in At-\nlantic City so the obvious next choice\nwas Monopoly. I played until I became\na successful Captain of Industry.\u201d\nThis short fragment is considerably less structured\nand with a more personal tone than a typical news\narticle. It references the entity Monopoly (Game),\nhowever expressions such as \u201cexperiment\u201d and\n\u201cIndustry\u201d can distract a naive disambiguation\nmodel because they are also related the much\nmore common entity Monopoly (economics term).\nSome sense of local semantics must be considered\nin order to separate the useful signals (e.g. \u201cindoor\ngames\u201d, \u201cplayed\u201d) from the noisy ones.\nWe therefore propose a new model that lever-\nages local contextual information to disambiguate\nentities. Our neural approach (based on RNNs\nwith attention) leverages the vast amount of train-\ning data in WikilinksNED to learn representations\narXiv:1706.09147v2  [cs.CL]  1 Jul 2017",
            0.5873094170000002
          ]
        },
        {
          "answer": "The provided context does not explicitly describe a quality control process to ensure that while the text remains noisy, the annotations are accurate. Therefore, I cannot directly answer the question based on the given text.",
          "relevant_chunks": [
            "bility, number of candidates for the given mention\nand maximum conditional probability of the entity\nin the document. The string based features include\nedit distance between mention and entity title and\ntwo boolean features indicating whether the entity\ntitle starts or ends with the mention and vice versa.\nThe GBRT model parameters where set to the val-\nues reported as optimal by Yamada6.\nCandidate Generation For comparability with\nexisting methods we used two publicly available\ncandidates datasets: (1) PPRforNED - Pershina at\nel. (2015); (2) Y AGO - Hoffart at el. (2011).\nBaselines As a baseline we took the standard\nMost Probable Sense (MPS) prediction, which se-\nlects the entity that was seen most with the given\nmention during training. We also compare to the\nfollowing papers - Francis-Landau et al. (2016),\nYamada at el. (2016), and Chisholm et al. (2015),\nas they are all strong local approaches and a good\nsource for comparison.\nResults Table 2 displays the micro and macro\nP@1 scores on CoNLL-Y AGO test-b for the dif-\nferent training steps. We \ufb01nd that when using only\nthe training set of CoNLL-Y AGO our model is\nunder-trained and that the domain adaptation sig-\nni\ufb01cant boosts performance. We \ufb01nd that incorpo-\nrating extra statistical and string features yields a\nsmall extra improvement in performance.\nThe \ufb01nal micro and macro P@1 scores on\nCoNLL-Y AGO test-b are displayed in table 3. On\nthis dataset our model achieves comparable re-\nsults, however it does not outperform the state-\nof-the-art, probably because of the relatively small\ntraining set and our reliance on domain adaptation.\n5.3 Effects of initialized embeddings and\ncorrupt-sampling schemes\nWe performed a study of the effects of using\npre-initialized embeddings for our model, and of\nusing either All-Entity or Near-Misses corrupt-\nsampling. The evaluation was done on a10% sam-\nple of the evaluation set of the WikilinksNED cor-\npus and can be seen in Table 4.\nWe have found that using pre-initialized embed-\ndings results in signi\ufb01cant performance gains, due\nto the better starting point. We have also found\nthat using Near-Misses, our model achieves sig-\nni\ufb01cantly improved performance. We attribute this\n6Learning rate of 0.02; maximal tree depth of 4; 10, 000\ntrees.\nCoNLL-Y AGO test-b - Training Steps Eval\nModel Micro\nP@1\nMacro\nP@1\nPPRforNED\nCoNLL training set 82 82\n+ domain adaptation 86.6 87 .7\n+ GBRT 87.3 88 .6\nYago\nCoNLL training set 74.8 73 .5\n+ domain adaptation 83.6 85 .1\n+ GBRT 83.3 86 .3\nTable 2: Evaluation of training steps on CoNLL-\nY AGO.\nCoNLL-Y AGO test-b (Local methods)\nModel Micro\nP@1\nMacro\nP@1\nPPRforNED\nOur ARNN + GBRT 87.3 88 .6\nYamada (2016) local 90.9 92 .4\nYamada (2016) global 93.1 92 .6\nYago\nOur ARNN + GBRT 83.3 86 .3\nYamada (2016) local 87.2 89 .6\nFrancis-Landau (2016) 85.5 -\nChisholm (2015) local 86.1 -\nYamada (2016) global 91.5 90 .9\nChisholm (2015) global 88.7 -\nTable 3: Evaluation on CoNLL-Y AGO.\ndifference to the more ef\ufb01cient nature of training\nwith near misses. Both these results were found to\nbe statistically signi\ufb01cant.\n6 Error Analysis\nWe randomly sampled and manually analyzed200\ncases of prediction errors made by our model. This\nset was obtained from WikilinksNED\u2019s validation\nset that was not used for training.\nWorking with crowd-sourced data, we expected\nsome errors to result from noise in the ground\ntruths themselves. Indeed, we found that 19.5%\n(39/200) of the errors were not false, out of which\n5% (2) where wrong labels, 33% (13) were pre-\ndictions with an equivalent meaning as the correct\nentity, and in 61.5% (24) our model suggested a\nmore convincing solution than the original author\nby using speci\ufb01c hints from the context. In this\nmanner, the mention\u2019Supreme leader\u2019, which was\n\nNamed Entity Disambiguation for Noisy Text\nYotam Eshel1 Noam Cohen1 Kira Radinsky1,2\nShaul Markovitch1 Ikuya Yamada3 Omer Levy4\n1Technion - Israel Institute of Technology, Haifa, Israel\n2eBay Research, Israel\n3Studio Ousia, Fujisawa, Kanagawa, Japan\n4University of Washington, Seattle, W A\nAbstract\nWe address the task of Named Entity\nDisambiguation (NED) for noisy text.\nWe present WikilinksNED, a large-scale\nNED dataset of text fragments from the\nweb, which is signi\ufb01cantly noisier and\nmore challenging than existing news-\nbased datasets. To capture the limited\nand noisy local context surrounding each\nmention, we design a neural model and\ntrain it with a novel method for sam-\npling informative negative examples. We\nalso describe a new way of initializing\nword and entity embeddings that signi\ufb01-\ncantly improves performance. Our model\nsigni\ufb01cantly outperforms existing state-of-\nthe-art methods on WikilinksNED while\nachieving comparable performance on a\nsmaller newswire dataset.\n1 Introduction\nNamed Entity Disambiguation (NED) is the task\nof linking mentions of entities in text to a given\nknowledge base, such as Freebase or Wikipedia.\nNED is a key component in Entity Linking (EL)\nsystems, focusing on the disambiguation task it-\nself, independently from the tasks of Named En-\ntity Recognition (detecting mention bounds) and\nCandidate Generation (retrieving the set of poten-\ntial candidate entities). NED has been recognized\nas an important component in NLP tasks such as\nsemantic parsing (Berant and Liang, 2014).\nCurrent research on NED is mostly driven by\na number of standard datasets, such as CoNLL-\nY AGO (Hoffart et al., 2011), TAC KBP (Ji et al.,\n2010) and ACE (Bentivogli et al., 2010). These\ndatasets are based on news corpora and Wikipedia,\nwhich are naturally coherent, well-structured, and\nrich in context. Global disambiguation models\n(Guo and Barbosa, 2014; Pershina et al., 2015;\nGloberson et al., 2016) leverage this coherency by\njointly disambiguating all the mentions in a sin-\ngle document. However, domains such as web-\npage fragments, social media, or search queries,\nare often short, noisy, and less coherent; such do-\nmains lack the necessary contextual information\nfor global methods to pay off, and present a more\nchallenging setting in general.\nIn this work, we investigate the task of NED\nin a setting where only local and noisy context\nis available. In particular, we create a dataset\nof 3.2M short text fragments extracted from web\npages, each containing a mention of a named en-\ntity. Our dataset is far larger than previously col-\nlected datasets, and contains 18K unique mentions\nlinking to over 100K unique entities. We have em-\npirically found it to be noisier and more challeng-\ning than existing datasets. For example:\n\u201cI had no choice but to experiment with\nother indoor games. I was born in At-\nlantic City so the obvious next choice\nwas Monopoly. I played until I became\na successful Captain of Industry.\u201d\nThis short fragment is considerably less structured\nand with a more personal tone than a typical news\narticle. It references the entity Monopoly (Game),\nhowever expressions such as \u201cexperiment\u201d and\n\u201cIndustry\u201d can distract a naive disambiguation\nmodel because they are also related the much\nmore common entity Monopoly (economics term).\nSome sense of local semantics must be considered\nin order to separate the useful signals (e.g. \u201cindoor\ngames\u201d, \u201cplayed\u201d) from the noisy ones.\nWe therefore propose a new model that lever-\nages local contextual information to disambiguate\nentities. Our neural approach (based on RNNs\nwith attention) leverages the vast amount of train-\ning data in WikilinksNED to learn representations\narXiv:1706.09147v2  [cs.CL]  1 Jul 2017\n\nWikilinks Evaluation-Set\nModel Micro\naccuracy\nNear-misses, with init. 72.5\nNear-misses, random init. 67.2\nAll-Entity, with init. 70\nAll-Entity, random init. 67.1\nTable 4: Corrupt-sampling and Initialization\ncontextually associated to the Iranian leader Ali\nKhamenei, was linked by our model with\u2019supreme\nleader of Iran\u2019while the \u201dcorrect\u201d tag was the gen-\neral \u2019supreme leader\u2019entity.\nIn addition, 15.5% (31/200) were cases where\na Wikipedia disambiguation-page was either the\ncorrect or predicted entity (2.5% and 14%, respec-\ntively). We considered the rest of the 130 errors as\ntrue semantic errors, and analyzed them in-depth.\nError type Fraction\nFalse errors\nNot errors 19.5% (39 /200)\n- Annotation error 5% (2 /39)\n- Better suggestion 61.5% (24 /39)\n- Equivalent entities 33% (13 /39)\nDisambiguation page 15.5% (31 /200)\nTrue semantic errors\nToo speci\ufb01c/general 31.5% (41 /130)\n\u2019almost correct\u2019 errors 26% (34 /130)\ninsuf\ufb01cient training 21.5% (28 /130)\nTable 5: Error distribution in 200 samples. Cate-\ngories of true errors are not fully distinct.\nFirst, we noticed that in31.5% of the true errors\n(41/130) our model selected an entity that can be\nunderstood as a speci\ufb01c ( 6.5%) or general ( 25%)\nrealization of the correct solution. For example,\ninstead of predicting \u2019Aroma of wine\u2019for a text on\nthe scent and \ufb02avor of Turkish wine, the model\nassigned the mention \u2019Aroma\u2019with the general\n\u2019Odor\u2019entity. We observed that in 26% (34/130)\nof the error cases, the predicted entity had a very\nstrong semantic relationship to the correct entity.\nA closer look discovered two prominent types of\n\u2019almost correct\u2019 errors occurred repeatedly in the\ndata. The \ufb01rst was a \ufb01lm/book/theater type of er-\nror (8.4%), where the actual and the predicted enti-\nties were a different display of the same narrative.\nEven though having different jargon and produc-\ners, those \ufb01elds share extremely similar content,\nwhich may explain why they tend to be frequently\nconfused by the algorithm. A third (4/14) of those\ncases were tagged as truly ambiguous even for hu-\nman reader. The second prominent type of \u2019almost\ncorrect\u2019 errors where differentiating between ad-\njectives that are used to describe properties of a na-\ntion. Particularity, mentions such as \u2019Germanic\u2019,\n\u2019Chinese\u2019and \u2019Dutch\u2019were falsely assigned to en-\ntities that describe language instead of people, and\nvice versa. We observed this type of mistake in\n8.4% of the errors (11/130).\nAnother interesting type of errors where in\ncases where the correct entity had insuf\ufb01cient\ntraining. We de\ufb01ned insuf\ufb01cient training errors as\nerrors where the correct entity appeared less than\n10 times in the training data. We saw that the\nmodel followed the MPS in 75% of these cases,\nshowing that our model tends to follow the base-\nline in such cases. Further, the amount of gen-\neralization error in insuf\ufb01cient-training conditions\nwas also signi\ufb01cant (35.7%), as our model tended\nto select more general entities.\n7 Conclusions\nOur results indicate that the expressibility of\nattention-RNNs indeed allows us to extract use-\nful features from noisy context, when suf\ufb01cient\namounts of training examples are available. This\nallows our model to signi\ufb01cantly out-perform ex-\nisting state-of-the-art models. We \ufb01nd that both\nusing pre-initialized embedding vocabularies, and\nthe corrupt-sampling method employed are very\nimportant for properly training our model.\nHowever, the gap between results of all systems\ntested on both CoNLL-Y AGO and WikilinksNED\nindicates that mentions with noisy context are in-\ndeed a challenging test. We believe this to be\nan important real-world scenario, that represents\na distinct test-case that \ufb01lls a gap between existing\nnews-based datasets and the much noisier Twitter\ndata (Ritter et al., 2011) that has received increas-\ning attention. We \ufb01nd recurrent neural models are\na promising direction for this task.\nFinally, our error analysis shows a number of\npossible improvements that should be addressed.\nSince we use the training set for candidate genera-\ntion, non-nonsensical candidates (i.e. disambigua-\ntion pages) cause our model to err and should be\nremoved from the candidate set. In addition, we\nobserve that lack of suf\ufb01cient training for long-",
            0.8308668749999999
          ]
        }
      ],
      "reference_answers": [
        "They initialize their word and entity embeddings with vectors pre-trained over a large corpus of unlabeled data.",
        "The authors believe that the Wikilinks corpus  contains ground truth annotations while being noisy. They discard mentions that cannot have ground-truth verified by comparison with Wikipedia."
      ]
    },
    "Risk_Prediction_of_Digital_Human_Resource_Manageme.pdf": {
      "error": "No valid question-answer pairs to evaluate",
      "average_exact_match": 0.0,
      "average_bleu_score": 0.0,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        },
        "rouge-2": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        },
        "rouge-L": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        }
      },
      "per_question_metrics": []
    },
    "1707.02377.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.006372497511228851,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.1343285799168152,
          "recall": 0.5898989898989898,
          "f1": 0.18903580791058103
        },
        "rouge-2": {
          "precision": 0.03571428571428571,
          "recall": 0.06896551724137931,
          "f1": 0.047058823529411764
        },
        "rouge-L": {
          "precision": 0.07679504018513307,
          "recall": 0.4292929292929293,
          "f1": 0.11099999588462123
        }
      },
      "per_question_metrics": [
        {
          "question": "Which language models do they compare against?",
          "generated_answer": "The language models they compare against include:\n\n- Bag-of-words (BoW)\n- Denoising Autoencoders (DEA)\n- Word2Vec (Mikolov et al., 2013a) + IDF\n- Doc2Vec (Le and Mikolov, 2014)\n- Skip-thought Vectors (Kiros et al., 2015)\n- RNNLM (Mikolov et al., 2010)\n\nAdditionally, for the semantic relatedness task, they compare against LSTM-based methods reported in (Tai et al., 2015).",
          "reference_answer": "RNNLM BIBREF11",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.01818181818181818,
              "recall": 0.5,
              "f1": 0.03508771929824561
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.01818181818181818,
              "recall": 0.5,
              "f1": 0.03508771929824561
            }
          },
          "bleu_score": 0.002237050690105942,
          "exact_match": 0.0
        },
        {
          "question": "Is their approach similar to making an averaged weighted sum of word vectors, where weights reflect word frequencies?",
          "generated_answer": "No, their approach does not use a weighted average of word vectors where weights reflect word frequencies. Instead, it represents each document as a simple average of the word embeddings of all the words in the document, with a corruption model that randomly removes words from the document during learning, without explicitly weighting words by their frequencies.",
          "reference_answer": "Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained.",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.3333333333333333,
              "recall": 0.6333333333333333,
              "f1": 0.43678160919540227
            },
            "rouge-2": {
              "precision": 0.10714285714285714,
              "recall": 0.20689655172413793,
              "f1": 0.1411764705882353
            },
            "rouge-L": {
              "precision": 0.17543859649122806,
              "recall": 0.3333333333333333,
              "f1": 0.22988505747126434
            }
          },
          "bleu_score": 0.014980620180224391,
          "exact_match": 0.0
        },
        {
          "question": "How do they determine which words are informative?",
          "generated_answer": "The determination of which words are informative is based on the effect of data-dependent regularization introduced by the corruption model in Doc2VecC. This model suppresses the embeddings of frequent but uninformative words, such as symbols and stop words, by acting as a form of data-dependent regularization. Additionally, the use of subsampling of frequent words, a trick introduced by Mikolov and Dean (2013), helps to counter the imbalance between frequent and rare words, improving the performance of models like Word2Vec by diminishing the contribution of common words in the final document representation. This approach naturally allows Doc2VecC to exert stronger regularization towards embeddings of words that are frequent but uninformative, without relying on subsampling, thereby helping to determine the informativeness of words based on their frequency and contribution to the document's meaning or sentiment.",
          "reference_answer": "Informative are those that will not be suppressed by regularization performed.",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.051470588235294115,
              "recall": 0.6363636363636364,
              "f1": 0.09523809523809523
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.03676470588235294,
              "recall": 0.45454545454545453,
              "f1": 0.06802721088435375
            }
          },
          "bleu_score": 0.0018998216633562212,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "Which language models do they compare against?",
        "Is their approach similar to making an averaged weighted sum of word vectors, where weights reflect word frequencies?",
        "How do they determine which words are informative?"
      ],
      "generated_answers": [
        {
          "answer": "The language models they compare against include:\n\n- Bag-of-words (BoW)\n- Denoising Autoencoders (DEA)\n- Word2Vec (Mikolov et al., 2013a) + IDF\n- Doc2Vec (Le and Mikolov, 2014)\n- Skip-thought Vectors (Kiros et al., 2015)\n- RNNLM (Mikolov et al., 2010)\n\nAdditionally, for the semantic relatedness task, they compare against LSTM-based methods reported in (Tai et al., 2015).",
          "relevant_chunks": [
            "Published as a conference paper at ICLR 2017\nTable 2: Learning time and representation generation time required by different representation learn-\ning algorithms.\nModel Learning time Generation time\nDenoising Autoencoders 3m 23s 7s\nWord2Vec + IDF 2m 33s 7s\nParagraph Vectors 4m 54s 4m 17s\nSkip-thought 2h 2h\nDoc2VecC 4m 30s 7s\nTable 3: Words with embeddings closest to 0 learned by different algorithms.\nWord2Vec harp(118) distasteful(115) switzerland(101) shabby(103) \ufb01reworks(101) heav-\nens(100) thornton(108) endeavor(100) dense(108) circumstance(119) debacle(103)\nParaVectors harp(118) dense(108) reels(115) \ufb01reworks(101) its\u2019(103) unnoticed(112) pony(102)\nful\ufb01lled(107) heavens(100) bliss(110) canned(114) shabby(103) debacle(103)\nDoc2VecC ,(1099319) .(1306691) the(1340408) of(581667) and(651119) up(49871) to(537570)\nthat(275240) time(48205) endeavor(100) here(21118) way(31302) own(13456)\nrepresentation. Paragraph Vectors, on the other hand, requires another round of inference to produce\nthe vector representation of unseen test documents. It takes Paragraph Vectors 4 minutes and 17\nseconds to infer the vector representations for the 25,000 test documents, in comparison to 7 seconds\nfor the other methods. As we did not re-train the Skip-thought vector models on this dataset, the\ntraining time2 reported in the table is the time it takes to generate the embeddings for the 25,000\ntraining documents. Due to repeated high-dimensional matrix operations required for encoding long\nparagraphs, it takes fairly long time to generate the representations for these documents. Similarly\nfor testing. The experiments were conducted on a desktop with Intel i7 2.2Ghz cpu.\nData dependent regularization. As explained in Section 3.1, the corruption introduced in\nDoc2VecC acts as a data-dependent regularization that suppresses the embeddings of frequent but\nuninformative words. Here we conduct an experiment to exam the effect. We used a cutoff of 100\nin this experiment. Table 3 lists the words having the smallest l2 norm of embeddings found by\ndifferent algorithms. The number inside the parenthesis after each word is the number of times this\nword appears in the learning set. In word2Vec or Paragraph Vectors, the least frequent words have\nembeddings that are close to zero, despite some of them being indicative of sentiment such as deba-\ncle, bliss and shabby. In contrast, Doc2VecC manages to clamp down the representation of words\nfrequently appear in the training set, but are uninformative, such as symbols and stop words.\nSubsampling frequent words. Note that for all the numbers reported, we applied the trick of\nsubsampling of frequent words introduced in (Mikolov and Dean, 2013) to counter the imbalance\nbetween frequent and rare words. It is critical to the performance of simple Word2Vec+A VG as the\nsole remedy to diminish the contribution of common words in the \ufb01nal document representation. If\nwe were to remove this step, the error rate of Word2Vec+A VG will increases from12.1% to 13.2%.\nDoc2VecC on the other hand naturally exerts a stronger regularization toward embeddings of words\nthat are frequent but uninformative, therefore does not rely on this trick.\n4.3 W ORD ANALOGY\nIn table 3, we demonstrated that the corruption model introduced in Doc2VecC dampens the embed-\ndings of words which are common and non-discriminative (stop words). In this experiment, we are\ngoing to quantatively compare the word embeddings generated by Doc2VecC to the ones generated\nby Word2Vec, or Paragraph Vectors on the word analogy task introduced by Mikolov et al. (2013a).\nThe dataset contains \ufb01ve types of semantic questions, and nine types of syntactic questions, with a\ntotal of 8,869 semantic and 10,675 syntactic questions. The questions are answered through simple\nlinear algebraic operations on the word embeddings generated by different methods. Please refer to\nthe original paper for more details on the evaluation protocol.\n2As reported in the original paper, training of the skip-thought vector model on the book corpus dataset\ntakes around 2 weeks on GPU.\n7\n\nPublished as a conference paper at ICLR 2017\nTable 6: Test set results on the SICK semantic relatedness task. The \ufb01rst group of results are from the\nsubmission to the 2014 SemEval competition; the second group includes several baseline methods\nreported in (Tai et al., 2015); the third group are methods based on LSTM reported in (Tai et al.,\n2015) as well as the skip-thought vectors (Kiros et al., 2015).\nMethod Pearson\u2019s\u03b3 Spearman\u2019s\u03c1 MSE\nIllinois-LH 0.7993 0.7538 0.3692\nUNAL-NLP 0.8070 0.7489 0.3550\nMeaning Factory 0.8268 0.7721 0.3224\nECNU 0.8279 0.7689 0.3250\nMean vectors (Word2Vec + avg) 0.7577 0.6738 0.4557\nDT-RNN (Socher et al., 2014) 0.7923 0.7319 0.3822\nSDT-RNN (Socher et al., 2014) 0.7900 0.7304 0.3848\nLSTM (Tai et al., 2015) 0.8528 0.7911 0.2831\nBidirectional LSTM (Tai et al., 2015) 0.8567 0.7966 0.2736\nDependency Tree-LSTM (Tai et al., 2015) 0.8676 0.8083 0.2532\ncombine-skip (Kiros et al., 2015) 0.8584 0.7916 0.2687\nDoc2VecC 0.8381 0.7621 0.3053\nREFERENCES\nBlei, D. M., Ng, A. Y ., and Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine\nLearning research, 3(Jan):993\u20131022.\nChen, M., Weinberger, K. Q., Sha, F., and Bengio, Y . (2014). Marginalized denoising auto-encoders\nfor nonlinear representations. In ICML, pages 1476\u20131484.\nChen, M., Xu, Z., Weinberger, K., and Sha, F. (2012). Marginalized denoising autoencoders for\ndomain adaptation. arXiv preprint arXiv:1206.4683.\nCroft, B. and Lafferty, J. (2013).Language modeling for information retrieval, volume 13. Springer\nScience & Business Media.\nDai, A. M. and Le, Q. V . (2015). Semi-supervised sequence learning. In Advances in Neural\nInformation Processing Systems, pages 3079\u20133087.\nDai, A. M., Olah, C., and Le, Q. V . (2015). Document embedding with paragraph vectors. arXiv\npreprint arXiv:1507.07998.\nDeerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. (1990). Indexing\nby latent semantic analysis. Journal of the American society for information science, 41(6):391.\nFan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and Lin, C.-J. (2008). Liblinear: A library for\nlarge linear classi\ufb01cation. JMLR, 9(Aug):1871\u20131874.\nGlorot, X., Bordes, A., and Bengio, Y . (2011). Domain adaptation for large-scale sentiment classi-\n\ufb01cation: A deep learning approach. In ICML, pages 513\u2013520.\nGrefenstette, E., Dinu, G., Zhang, Y .-Z., Sadrzadeh, M., and Baroni, M. (2013). Multi-step regres-\nsion learning for compositional distributional semantics. arXiv preprint arXiv:1301.6939.\nHuang, E. H., Socher, R., Manning, C. D., and Ng, A. Y . (2012). Improving word representations\nvia global context and multiple word prototypes. In ACL, pages 873\u2013882.\nKim, Y ., Jernite, Y ., Sontag, D., and Rush, A. M. (2015). Character-aware neural language models.\narXiv preprint arXiv:1508.06615.\nKiros, R., Zhu, Y ., Salakhutdinov, R. R., Zemel, R., Urtasun, R., Torralba, A., and Fidler, S. (2015).\nSkip-thought vectors. In Advances in neural information processing systems, pages 3294\u20133302.\nKusner, M. J., Sun, Y ., Kolkin, N. I., and Weinberger, K. Q. (2015). From word embeddings to\ndocument distances. In Proceedings of the 32nd International Conference on Machine Learning\n(ICML 2015), pages 957\u2013966.\n11\n\nPublished as a conference paper at ICLR 2017\nAs each word in a document is corrupted independently of others, the variance matrix \u03a3x is simpli-\n\ufb01ed to a diagonal matrix with jth element equals q\n1\u2212q x2\nj. As a result, we only need to compute the\ndiagonal terms of the Hessian matrix \u22072\n\u02dcxf.\nThe jth dimension of the Hessian\u2019s diagonal evaluated at the meanx is given by\n\u22022f\n\u2202x2\nj\n= \u2212\u03c3w,c,x(1 \u2212\u03c3w,c,x)( 1\nTv\u22a4\nwuj)2 \u2212\n\u2211\nw\u2032\u223cPv\n\u03c3w\u2032,c,x(1 \u2212\u03c3w\u2032,c,x)( 1\nTv\u22a4\nw\u2032 uj)2\nPlug the Hessian matrix and the variance matrix back into eq.(6), and then back to the loss de\ufb01ned\nin eq.(4), we can see that Doc2VecC intrinsically minimizes\n\u2113= \u2212\nn\u2211\ni=1\nTi\u2211\nt=1\nf(wt\ni,ct\ni,xi) + q\n1 \u2212q\nv\u2211\nj=1\nR(uj) (7)\nEach f(wt\ni,ct\ni,xi) in the \ufb01rst term measures the log likelihood of observing the target word wt\ni\ngiven its local context ct\ni and the document vector di = 1\nT Uxi. As such, Doc2VecC enforces that a\ndocument vector generated by averaging word embeddings can capture the global semantics of the\ndocument, and \ufb01ll in information missed in the local context.\nThe second term here is a data-dependent regularization. The regularization on the embedding uj\nof each word jtakes the following form,\nR(uj) \u221d\nn\u2211\ni=1\nTi\u2211\nt=1\nx2\nij\n[\n\u03c3wt\ni,ct\ni,xi (1 \u2212\u03c3wt\ni,ct\ni,xi )( 1\nTv\u22a4\nwt\ni\nuj)2 +\n\u2211\nw\u2032\u223cPv\n\u03c3w\u2032,ct\ni,xi (1 \u2212\u03c3w\u2032,ct\ni,xi )( 1\nTv\u22a4\nw\u2032 uj)2\n]\nwhere \u03c3w,c,x = \u03c3(v\u22a4\nw(Uc+ 1\nT Ux)) prescribes the con\ufb01dence of predicting the target wordwgiven\nits neighboring context c as well as the document vector d = 1\nT Ux.\nClosely examining R(uj) leads to several interesting \ufb01ndings: 1. the regularizer penalizes more\non the embeddings of common words. A word jthat frequently appears across the training corpus,\ni.e, xij = 1 often, will have a bigger regularization than a rare word; 2. on the other hand, the\nregularization is modulated by \u03c3w,c,x(1 \u2212\u03c3w,c,x), which is small if \u03c3w,c,x \u21921 or 0. In other\nwords, if uj is critical to a con\ufb01dent prediction \u03c3w,c,x when it is active, then the regularization is\ndiminished. Similar effect was observed for dropout training for logistic regression model (Wager\net al., 2013) and denoising autoencoders (Chen et al., 2014).\n4 E XPERIMENTS\nWe evaluate Doc2VecC on a sentiment analysis task, a document classi\ufb01cation task and a semantic\nrelatedness task, along with several document representation learning algorithms. All experiments\ncan be reproduced using the code available at https://github.com/mchen24/iclr2017\n4.1 B ASELINES\nWe compare against the following document representation baselines: bag-of-words (BoW); De-\nnoising Autoencoders (DEA) (Vincent et al., 2008), a representation learned from reconstruct-\ning original document x using corrupted one \u02dcx. SDAs have been shown to be the state-of-the-art\nfor sentiment analysis tasks (Glorot et al., 2011). We used Kullback-Liebler divergence as the re-\nconstruction error and an af\ufb01ne encoder. To scale up the algorithm to large vocabulary, we only\ntake into account the non-zero elements of x in the reconstruction error and employed negative\nsampling for the remainings; Word2Vec (Mikolov et al., 2013a)+IDF, a representation generated\nthrough weighted average of word vectors learned using Word2Vec; Doc2Vec (Le and Mikolov,\n2014); Skip-thought Vectors(Kiros et al., 2015), a generic, distributed sentence encoder that ex-\ntends the Word2Vec skip-gram model to sentence level. It has been shown to produce highly generic\nsentence representations that apply to various natural language processing tasks. We also include\nRNNLM (Mikolov et al., 2010), a recurrent neural network based language model in the compar-\nison. In the semantic relatedness task, we further compare to LSTM-based methods (Tai et al.,\n2015) that have been reported on this dataset.\n5",
            1.071078666
          ]
        },
        {
          "answer": "No, their approach does not use a weighted average of word vectors where weights reflect word frequencies. Instead, it represents each document as a simple average of the word embeddings of all the words in the document, with a corruption model that randomly removes words from the document during learning, without explicitly weighting words by their frequencies.",
          "relevant_chunks": [
            "Published as a conference paper at ICLR 2017\n2015) have been studied to generate vector representations of documents from word embeddings,\namong which the simplest approach is to use weighted average of word embeddings. Similarly,\nour method forms document representation by averaging word embeddings of all the words in the\ndocument. Differently, as our model encodes the compositionality of words in the learned word\nembeddings, heuristic weighting at test time is not required.\nParagraph Vectors. Paragraph Vectors, on the other hands, explicitly learns a document vector\nwith the word embeddings. It introduces another projection matrix D \u2208Rh\u00d7n. Each column of D\nacts as a memory of the global topic of the corresponding document. It then de\ufb01nes the probability\nof observing the target word wt in a document Dgiven its local context ct as\nP(wt|ct,d) = exp(v\u22a4\nwt (Uct + d))\u2211\nw\u2032\u2208V exp(v\u22a4\nw\u2032 (Uct + d))\nwhere d \u2208D is the vector representation of the document. As we can see from this formula, the\ncomplexity of Paragraph Vectors grows with not only the size of the vocabulary, but also the size of\nthe training corpus. While we can reasonably limit the size of a vocabulary to be within a million\nfor most datasets, the size of a training corpus can easily go to billions. What is more concerning is\nthat, in order to come up with the vector representations of unseen documents, we need to perform\nan expensive inference by appending more columns to D and gradient descent on D while \ufb01xing\nother parameters of the learned model.\n3 M ETHOD\nSeveral works (Mikolov and Dean, 2013; Mikolov et al., 2013b) showcased that syntactic and se-\nmantic regularities of phrases and sentences are reasonably well preserved by adding or subtract-\ning word embeddings learned through Word2Vec. It prompts us to explore the option of simply\nrepresenting a document as an average of word embeddings. Figure 1 illustrates the new model\narchitecture.\nwt#1 Wt+1 Wt+2 wp wq wr\nwt\nopening for the performance praised brazil\nceremony\nword<vectors\ndocument< vector\nAverage/Concatenate\nFigure 1: A new framework for learning document vectors.\nSimilar to Word2Vec or Paragraph Vectors, Doc2VecC consists of an input layer, a projection layer\nas well as an output layer to predict the target word, \u201cceremony\u201d in this example. The embeddings of\nneighboring words (\u201copening\u201d, \u201cfor\u201d, \u201cthe\u201d) provide local context while the vector representation of\nthe entire document (shown in grey) serves as the global context. In contrast to Paragraph Vectors,\nwhich directly learns a unique vector for each document, Doc2VecC represents each document as\nan average of the embeddings of words randomly sampled from the document (\u201cperformance\u201d at\nposition p, \u201cpraised\u201d at position q, and \u201cbrazil\u201d at position r).\nHuang et al. (2012) also proposed the idea of using average of word embeddings to represent the\nglobal context of a document. Different from their work, we choose to corrupt the original document\nby randomly removing signi\ufb01cant portion of words, and represent the document using only the\nembeddings of the words remained. This corruption mechanism offers us great speedup during\ntraining as it signi\ufb01cantly reduces the number of parameters to update in back propagation. At the\nsame time, as we are going to detail in the next section, it introduces a special form of regularization,\nwhich brings great performance improvement.\n3\n\nPublished as a conference paper at ICLR 2017\nvec(\u201cking\u201d) - vec(\u201cman\u201d) + vec(\u201cwomen\u201d) is close to vec(\u201cqueen\u201d) (Mikolov et al., 2013b). In\nDoc2VecC, we represent each document as a simple average of the word embeddings of all the\nwords in the document. In contrast to existing approaches which post-process learned word em-\nbeddings to form document representation (Socher et al., 2013; Mesnil et al., 2014), Doc2VecC\nenforces a meaningful document representation can be formed by averaging the word embeddings\nduring learning. Furthermore, we include a corruption model that randomly remove words from a\ndocument during learning, a mechanism that is critical to the performance and learning speed of our\nalgorithm.\nDoc2VecC has several desirable properties: 1. The model complexity of Doc2VecC is decoupled\nfrom the size of the training corpus, depending only on the size of the vocabulary; 2. The model\narchitecture of Doc2VecC resembles that of Word2Vec, and can be trained very ef\ufb01ciently; 3. The\nnew framework implicitly introduces a data-dependent regularization, which favors rare or informa-\ntive words and suppresses words that are common but not discriminative; 4. Vector representation\nof a document can be generated by simply averaging the learned word embeddings of all the words\nin the document, which signi\ufb01cantly boost test ef\ufb01ciency; 5. The vector representation generated by\nDoc2VecC matches or beats the state-of-the-art for sentiment analysis, document classi\ufb01cation as\nwell as semantic relatedness tasks.\n2 R ELATED WORKS AND NOTATIONS\nText representation learning has been extensively studied. Popular representations range from the\nsimplest BoW and its term-frequency based variants (Salton and Buckley, 1988), language model\nbased methods (Croft and Lafferty, 2013; Mikolov et al., 2010; Kim et al., 2015), topic mod-\nels (Deerwester et al., 1990; Blei et al., 2003), Denoising Autoencoders and its variants (Vincent\net al., 2008; Chen et al., 2012), and distributed vector representations (Mesnil et al., 2014; Le and\nMikolov, 2014; Kiros et al., 2015). Another prominent line of work includes learning task-speci\ufb01c\ndocument representation with deep neural networks, such as CNN (Zhang and LeCun, 2015) or\nLSTM based approaches (Tai et al., 2015; Dai and Le, 2015).\nIn this section, we brie\ufb02y introduce Word2Vec and Paragraph Vectors, the two approaches that are\nmost similar to ours. There are two well-know model architectures used for both methods, referred\nto as Continuous Bag-of-Words (CBoW) and Skipgram models (Mikolov et al., 2013a). In this\nwork, we focus on CBoW. Extending to Skipgram is straightforward. Here are the notations we are\ngoing to use throughout the paper:\nD= {D1,\u00b7\u00b7\u00b7 ,Dn}: a training corpus of size n, in which each document Di contains a variable-\nlength sequence of words w1\ni ,\u00b7\u00b7\u00b7 ,wTi\ni ;\nV: the vocabulary used in the training corpus, of sizes v;\nx \u2208Rv\u00d71: BoW of a document, where xj = 1 iff word jdoes appear in the document.\nct \u2208Rv\u00d71: BoW of the local context wt\u2212k,\u00b7\u00b7\u00b7 ,wt\u22121,wt+1,\u00b7\u00b7\u00b7 ,wt+k at the target position t.\nct\nj = 1 iff word jappears within the sliding window of the target;\nU \u2208Rh\u00d7v: the projection matrix from the input space to a hidden space of size h. We use uw to\ndenote the column in U for word w, i.e., the \u201cinput\u201c vector of wordw;\nV\u22a4 \u2208Rv\u00d7h: the projection matrix from the hidden space to output. Similarly, we use vw to\ndenote the column in V for word w, i.e., the \u201coutput\u201c vector of wordw.\nWord2Vec. Word2Vec proposed a neural network architecture of an input layer, a projection layer\nparameterized by the matrix U and an output layer by V\u22a4. It de\ufb01nes the probability of observing\nthe target word wt in a document Dgiven its local context ct as\nP(wt|ct) = exp(v\u22a4\nwt Uct)\u2211\nw\u2032\u2208V exp(v\u22a4\nw\u2032 Uct)\nThe word vectors are then learned to maximize the log likelihood of observing the target word at\neach position of the document. Various techniques (Mitchell and Lapata, 2010; Zanzotto et al.,\n2010; Yessenalina and Cardie, 2011; Grefenstette et al., 2013; Socher et al., 2013; Kusner et al.,\n2\n\nPublished as a conference paper at ICLR 2017\nAs each word in a document is corrupted independently of others, the variance matrix \u03a3x is simpli-\n\ufb01ed to a diagonal matrix with jth element equals q\n1\u2212q x2\nj. As a result, we only need to compute the\ndiagonal terms of the Hessian matrix \u22072\n\u02dcxf.\nThe jth dimension of the Hessian\u2019s diagonal evaluated at the meanx is given by\n\u22022f\n\u2202x2\nj\n= \u2212\u03c3w,c,x(1 \u2212\u03c3w,c,x)( 1\nTv\u22a4\nwuj)2 \u2212\n\u2211\nw\u2032\u223cPv\n\u03c3w\u2032,c,x(1 \u2212\u03c3w\u2032,c,x)( 1\nTv\u22a4\nw\u2032 uj)2\nPlug the Hessian matrix and the variance matrix back into eq.(6), and then back to the loss de\ufb01ned\nin eq.(4), we can see that Doc2VecC intrinsically minimizes\n\u2113= \u2212\nn\u2211\ni=1\nTi\u2211\nt=1\nf(wt\ni,ct\ni,xi) + q\n1 \u2212q\nv\u2211\nj=1\nR(uj) (7)\nEach f(wt\ni,ct\ni,xi) in the \ufb01rst term measures the log likelihood of observing the target word wt\ni\ngiven its local context ct\ni and the document vector di = 1\nT Uxi. As such, Doc2VecC enforces that a\ndocument vector generated by averaging word embeddings can capture the global semantics of the\ndocument, and \ufb01ll in information missed in the local context.\nThe second term here is a data-dependent regularization. The regularization on the embedding uj\nof each word jtakes the following form,\nR(uj) \u221d\nn\u2211\ni=1\nTi\u2211\nt=1\nx2\nij\n[\n\u03c3wt\ni,ct\ni,xi (1 \u2212\u03c3wt\ni,ct\ni,xi )( 1\nTv\u22a4\nwt\ni\nuj)2 +\n\u2211\nw\u2032\u223cPv\n\u03c3w\u2032,ct\ni,xi (1 \u2212\u03c3w\u2032,ct\ni,xi )( 1\nTv\u22a4\nw\u2032 uj)2\n]\nwhere \u03c3w,c,x = \u03c3(v\u22a4\nw(Uc+ 1\nT Ux)) prescribes the con\ufb01dence of predicting the target wordwgiven\nits neighboring context c as well as the document vector d = 1\nT Ux.\nClosely examining R(uj) leads to several interesting \ufb01ndings: 1. the regularizer penalizes more\non the embeddings of common words. A word jthat frequently appears across the training corpus,\ni.e, xij = 1 often, will have a bigger regularization than a rare word; 2. on the other hand, the\nregularization is modulated by \u03c3w,c,x(1 \u2212\u03c3w,c,x), which is small if \u03c3w,c,x \u21921 or 0. In other\nwords, if uj is critical to a con\ufb01dent prediction \u03c3w,c,x when it is active, then the regularization is\ndiminished. Similar effect was observed for dropout training for logistic regression model (Wager\net al., 2013) and denoising autoencoders (Chen et al., 2014).\n4 E XPERIMENTS\nWe evaluate Doc2VecC on a sentiment analysis task, a document classi\ufb01cation task and a semantic\nrelatedness task, along with several document representation learning algorithms. All experiments\ncan be reproduced using the code available at https://github.com/mchen24/iclr2017\n4.1 B ASELINES\nWe compare against the following document representation baselines: bag-of-words (BoW); De-\nnoising Autoencoders (DEA) (Vincent et al., 2008), a representation learned from reconstruct-\ning original document x using corrupted one \u02dcx. SDAs have been shown to be the state-of-the-art\nfor sentiment analysis tasks (Glorot et al., 2011). We used Kullback-Liebler divergence as the re-\nconstruction error and an af\ufb01ne encoder. To scale up the algorithm to large vocabulary, we only\ntake into account the non-zero elements of x in the reconstruction error and employed negative\nsampling for the remainings; Word2Vec (Mikolov et al., 2013a)+IDF, a representation generated\nthrough weighted average of word vectors learned using Word2Vec; Doc2Vec (Le and Mikolov,\n2014); Skip-thought Vectors(Kiros et al., 2015), a generic, distributed sentence encoder that ex-\ntends the Word2Vec skip-gram model to sentence level. It has been shown to produce highly generic\nsentence representations that apply to various natural language processing tasks. We also include\nRNNLM (Mikolov et al., 2010), a recurrent neural network based language model in the compar-\nison. In the semantic relatedness task, we further compare to LSTM-based methods (Tai et al.,\n2015) that have been reported on this dataset.\n5",
            0.5356210000000026
          ]
        },
        {
          "answer": "The determination of which words are informative is based on the effect of data-dependent regularization introduced by the corruption model in Doc2VecC. This model suppresses the embeddings of frequent but uninformative words, such as symbols and stop words, by acting as a form of data-dependent regularization. Additionally, the use of subsampling of frequent words, a trick introduced by Mikolov and Dean (2013), helps to counter the imbalance between frequent and rare words, improving the performance of models like Word2Vec by diminishing the contribution of common words in the final document representation. This approach naturally allows Doc2VecC to exert stronger regularization towards embeddings of words that are frequent but uninformative, without relying on subsampling, thereby helping to determine the informativeness of words based on their frequency and contribution to the document's meaning or sentiment.",
          "relevant_chunks": [
            "Published as a conference paper at ICLR 2017\nTable 2: Learning time and representation generation time required by different representation learn-\ning algorithms.\nModel Learning time Generation time\nDenoising Autoencoders 3m 23s 7s\nWord2Vec + IDF 2m 33s 7s\nParagraph Vectors 4m 54s 4m 17s\nSkip-thought 2h 2h\nDoc2VecC 4m 30s 7s\nTable 3: Words with embeddings closest to 0 learned by different algorithms.\nWord2Vec harp(118) distasteful(115) switzerland(101) shabby(103) \ufb01reworks(101) heav-\nens(100) thornton(108) endeavor(100) dense(108) circumstance(119) debacle(103)\nParaVectors harp(118) dense(108) reels(115) \ufb01reworks(101) its\u2019(103) unnoticed(112) pony(102)\nful\ufb01lled(107) heavens(100) bliss(110) canned(114) shabby(103) debacle(103)\nDoc2VecC ,(1099319) .(1306691) the(1340408) of(581667) and(651119) up(49871) to(537570)\nthat(275240) time(48205) endeavor(100) here(21118) way(31302) own(13456)\nrepresentation. Paragraph Vectors, on the other hand, requires another round of inference to produce\nthe vector representation of unseen test documents. It takes Paragraph Vectors 4 minutes and 17\nseconds to infer the vector representations for the 25,000 test documents, in comparison to 7 seconds\nfor the other methods. As we did not re-train the Skip-thought vector models on this dataset, the\ntraining time2 reported in the table is the time it takes to generate the embeddings for the 25,000\ntraining documents. Due to repeated high-dimensional matrix operations required for encoding long\nparagraphs, it takes fairly long time to generate the representations for these documents. Similarly\nfor testing. The experiments were conducted on a desktop with Intel i7 2.2Ghz cpu.\nData dependent regularization. As explained in Section 3.1, the corruption introduced in\nDoc2VecC acts as a data-dependent regularization that suppresses the embeddings of frequent but\nuninformative words. Here we conduct an experiment to exam the effect. We used a cutoff of 100\nin this experiment. Table 3 lists the words having the smallest l2 norm of embeddings found by\ndifferent algorithms. The number inside the parenthesis after each word is the number of times this\nword appears in the learning set. In word2Vec or Paragraph Vectors, the least frequent words have\nembeddings that are close to zero, despite some of them being indicative of sentiment such as deba-\ncle, bliss and shabby. In contrast, Doc2VecC manages to clamp down the representation of words\nfrequently appear in the training set, but are uninformative, such as symbols and stop words.\nSubsampling frequent words. Note that for all the numbers reported, we applied the trick of\nsubsampling of frequent words introduced in (Mikolov and Dean, 2013) to counter the imbalance\nbetween frequent and rare words. It is critical to the performance of simple Word2Vec+A VG as the\nsole remedy to diminish the contribution of common words in the \ufb01nal document representation. If\nwe were to remove this step, the error rate of Word2Vec+A VG will increases from12.1% to 13.2%.\nDoc2VecC on the other hand naturally exerts a stronger regularization toward embeddings of words\nthat are frequent but uninformative, therefore does not rely on this trick.\n4.3 W ORD ANALOGY\nIn table 3, we demonstrated that the corruption model introduced in Doc2VecC dampens the embed-\ndings of words which are common and non-discriminative (stop words). In this experiment, we are\ngoing to quantatively compare the word embeddings generated by Doc2VecC to the ones generated\nby Word2Vec, or Paragraph Vectors on the word analogy task introduced by Mikolov et al. (2013a).\nThe dataset contains \ufb01ve types of semantic questions, and nine types of syntactic questions, with a\ntotal of 8,869 semantic and 10,675 syntactic questions. The questions are answered through simple\nlinear algebraic operations on the word embeddings generated by different methods. Please refer to\nthe original paper for more details on the evaluation protocol.\n2As reported in the original paper, training of the skip-thought vector model on the book corpus dataset\ntakes around 2 weeks on GPU.\n7\n\nPublished as a conference paper at ICLR 2017\nTable 6: Test set results on the SICK semantic relatedness task. The \ufb01rst group of results are from the\nsubmission to the 2014 SemEval competition; the second group includes several baseline methods\nreported in (Tai et al., 2015); the third group are methods based on LSTM reported in (Tai et al.,\n2015) as well as the skip-thought vectors (Kiros et al., 2015).\nMethod Pearson\u2019s\u03b3 Spearman\u2019s\u03c1 MSE\nIllinois-LH 0.7993 0.7538 0.3692\nUNAL-NLP 0.8070 0.7489 0.3550\nMeaning Factory 0.8268 0.7721 0.3224\nECNU 0.8279 0.7689 0.3250\nMean vectors (Word2Vec + avg) 0.7577 0.6738 0.4557\nDT-RNN (Socher et al., 2014) 0.7923 0.7319 0.3822\nSDT-RNN (Socher et al., 2014) 0.7900 0.7304 0.3848\nLSTM (Tai et al., 2015) 0.8528 0.7911 0.2831\nBidirectional LSTM (Tai et al., 2015) 0.8567 0.7966 0.2736\nDependency Tree-LSTM (Tai et al., 2015) 0.8676 0.8083 0.2532\ncombine-skip (Kiros et al., 2015) 0.8584 0.7916 0.2687\nDoc2VecC 0.8381 0.7621 0.3053\nREFERENCES\nBlei, D. M., Ng, A. Y ., and Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine\nLearning research, 3(Jan):993\u20131022.\nChen, M., Weinberger, K. Q., Sha, F., and Bengio, Y . (2014). Marginalized denoising auto-encoders\nfor nonlinear representations. In ICML, pages 1476\u20131484.\nChen, M., Xu, Z., Weinberger, K., and Sha, F. (2012). Marginalized denoising autoencoders for\ndomain adaptation. arXiv preprint arXiv:1206.4683.\nCroft, B. and Lafferty, J. (2013).Language modeling for information retrieval, volume 13. Springer\nScience & Business Media.\nDai, A. M. and Le, Q. V . (2015). Semi-supervised sequence learning. In Advances in Neural\nInformation Processing Systems, pages 3079\u20133087.\nDai, A. M., Olah, C., and Le, Q. V . (2015). Document embedding with paragraph vectors. arXiv\npreprint arXiv:1507.07998.\nDeerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. (1990). Indexing\nby latent semantic analysis. Journal of the American society for information science, 41(6):391.\nFan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and Lin, C.-J. (2008). Liblinear: A library for\nlarge linear classi\ufb01cation. JMLR, 9(Aug):1871\u20131874.\nGlorot, X., Bordes, A., and Bengio, Y . (2011). Domain adaptation for large-scale sentiment classi-\n\ufb01cation: A deep learning approach. In ICML, pages 513\u2013520.\nGrefenstette, E., Dinu, G., Zhang, Y .-Z., Sadrzadeh, M., and Baroni, M. (2013). Multi-step regres-\nsion learning for compositional distributional semantics. arXiv preprint arXiv:1301.6939.\nHuang, E. H., Socher, R., Manning, C. D., and Ng, A. Y . (2012). Improving word representations\nvia global context and multiple word prototypes. In ACL, pages 873\u2013882.\nKim, Y ., Jernite, Y ., Sontag, D., and Rush, A. M. (2015). Character-aware neural language models.\narXiv preprint arXiv:1508.06615.\nKiros, R., Zhu, Y ., Salakhutdinov, R. R., Zemel, R., Urtasun, R., Torralba, A., and Fidler, S. (2015).\nSkip-thought vectors. In Advances in neural information processing systems, pages 3294\u20133302.\nKusner, M. J., Sun, Y ., Kolkin, N. I., and Weinberger, K. Q. (2015). From word embeddings to\ndocument distances. In Proceedings of the 32nd International Conference on Machine Learning\n(ICML 2015), pages 957\u2013966.\n11\n\nPublished as a conference paper at ICLR 2017\n1M 2M 4M 8M 15M\n0\n20\n40\n60\n3.8 6.1 8.3 9.1\n13.3\n18.7\n26.4\n32.7\n36.1 38.9\n20.3\n28.1\n36.4\n42.5\n46.7\nNumber of paragraphs used for learning\nAccuracy (%)\nParagraphVectorsWord2VecDoc2VecC\n(a) h=50\n1M 2M 4M 8M 15M\n0\n20\n40\n60\n5.1 7.5\n10.9 10.2 10.2\n23.6\n34.7\n42.4\n48.2 50.7\n24.3\n34.1\n44.1\n52.6\n58.2\nNumber of paragraphs used for learning\nParagraphVectorsWord2VecDoc2VecC\n(b) h=100\nFigure 2: Accuracy on subset of the Semantic-Syntactic Word Relationship test set. Only questions\ncontaining words from the most frequent 30k words are included in the test.\nSemantic questions Word2Vec Doc2VecC Syntactic questions Word2Vec Doc2VecC\ncapital-common-countries 73.59 81.82 gram1-adjective-to-adverb 19.25 20.32\ncapital-world 67.94 77.96 gram2-opposite 14.07 25.54\ncurrency 17.14 12.86 gram3-comparative 60.21 74.47\ncity-in-state 34.49 42.86 gram4-superlative 52.87 55.40\nfamily 68.71 64.62 gram5-present-participle 56.34 65.81\ngram6-nationality-adjective 88.71 91.03\ngram7-past-tense 47.05 51.86\ngram8-plural 50.28 61.27\ngram9-plural-verbs 25.38 39.69\nTable 4: Top 1 accuracy on the 5 type of semantics and 9 types of syntactic questions.\nWe trained the word embeddings of different methods using the English news dataset released under\nthe ACL workshop on statistical machine translation. The training set includes close to 15M para-\ngraphs with 355M tokens. We compare the performance of word embeddings trained by different\nmethods with increasing embedding dimensionality as well as increasing training data.\nWe observe similar trends as in Mikolov et al. (2013a). Increasing embedding dimensionality as\nwell as training data size improves performance of the word embeddings on this task. However, the\nimprovement is diminishing. Doc2VecC produces word embeddings which performs signi\ufb01cantly\nbetter than the ones generated by Word2Vec. We observe close to 20% uplift when we train on the\nfull training corpus. Paragraph vectors on the other hand performs surprisingly bad on this dataset.\nOur hypothesis is that due to the large capacity of the model architecture, Paragraph Vectors relies\nmostly on the unique document vectors to capture the information in a text document instead of\nlearning the word semantic or syntactic similarities. This also explains why the PV-DBOW Le and\nMikolov (2014) model architecture proposed in the original work, which completely removes word\nembedding layers, performs comparable to the distributed memory version.\nIn table 5, we list a detailed comparison of the performance of word embeddings generated by\nWord2Vec and Doc2VecC on the 14 subtasks, when trained on the full dataset with embedding of\nsize 100. We can see that Doc2VecC signi\ufb01cantly outperforms the word embeddings produced by\nWord2Vec across almost all the subtasks.\n4.4 D OCUMENT CLASSIFICATION\nFor the document classi\ufb01cation task, we use a subset of the wikipedia dump, which contains over\n300,000 wikipedia pages in 100 categories. The 100 categories includes categories under sports,\n8",
            1.0444624159999982
          ]
        }
      ],
      "reference_answers": [
        "RNNLM BIBREF11",
        "Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained.",
        "Informative are those that will not be suppressed by regularization performed."
      ]
    },
    "1801.02243.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.07389994230814327,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.10797694874127994,
          "recall": 0.4761904761904762,
          "f1": 0.17153996101364521
        },
        "rouge-2": {
          "precision": 0.0903720462543992,
          "recall": 0.358974358974359,
          "f1": 0.14347965689647266
        },
        "rouge-L": {
          "precision": 0.10585380649074916,
          "recall": 0.4523809523809524,
          "f1": 0.1676413255360624
        }
      },
      "per_question_metrics": [
        {
          "question": "Do the authors give any examples of major events which draw the public's attention and the impact they have on stock price?",
          "generated_answer": "No, the authors do not provide specific examples of major events that draw the public's attention and their impact on stock price in the context provided.",
          "reference_answer": "Yes",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            }
          },
          "bleu_score": 0,
          "exact_match": 0.0
        },
        {
          "question": "Which tweets are used to output the daily sentiment signal?",
          "generated_answer": "The tweets used to output the daily sentiment signal are those that have been filtered according to specific criteria mentioned in the context: removing hashtags (#), user mentions (@), tabs, and extra spaces, filtering out tweets with consecutive question marks (??) because their coding is usually not recognizable, and filtering out non-English tweets using Google's Langdetect package. Additionally, the sentiment analysis considers tweets that are directly related to the stock ticker or the company's products and news, based on keywords defined according to the top twenty related keywords of the stock ticker according to Google Trend. These tweets are then processed to assign a sentiment score using Stanford CoreNLP software, which computes a sentiment score for each sentence ranging from 0 to 4, with 0 indicating negative sentiment and 4 indicating very positive sentiment. For tweets with multiple sentences, the average of the sentiment scores of all sentences is used as the sentiment score of the tweets.",
          "reference_answer": "Tesla and Ford are investigated on how Twitter sentiment could impact the stock price",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.03821656050955414,
              "recall": 0.42857142857142855,
              "f1": 0.07017543859649122
            },
            "rouge-2": {
              "precision": 0.00641025641025641,
              "recall": 0.07692307692307693,
              "f1": 0.011834319526627219
            },
            "rouge-L": {
              "precision": 0.03184713375796178,
              "recall": 0.35714285714285715,
              "f1": 0.05847953216374269
            }
          },
          "bleu_score": 0.002757285800409953,
          "exact_match": 0.0
        },
        {
          "question": "What is the baseline machine learning prediction approach?",
          "generated_answer": "The baseline machine learning prediction approach applies linear logistic regression to a set of stock technical signals to predict the following day's stock return sign (+/-). No sentiment features are provided to the baseline model.",
          "reference_answer": "linear logistic regression to a set of stock technical signals",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.2857142857142857,
              "recall": 1.0,
              "f1": 0.4444444444444445
            },
            "rouge-2": {
              "precision": 0.2647058823529412,
              "recall": 1.0,
              "f1": 0.4186046511627907
            },
            "rouge-L": {
              "precision": 0.2857142857142857,
              "recall": 1.0,
              "f1": 0.4444444444444445
            }
          },
          "bleu_score": 0.21894254112401984,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "Do the authors give any examples of major events which draw the public's attention and the impact they have on stock price?",
        "Which tweets are used to output the daily sentiment signal?",
        "What is the baseline machine learning prediction approach?"
      ],
      "generated_answers": [
        {
          "answer": "No, the authors do not provide specific examples of major events that draw the public's attention and their impact on stock price in the context provided.",
          "relevant_chunks": [
            "Trading the Twitter Sentiment with Reinforcement Learning\nCatherine Xiao\ncatherine.xiao1@gmail.com\nWanfeng Chen\nwanfengc@gmail.com\nAbstract\u2014 This paper is to explore the possibility to use\nalternative data and arti\ufb01cial intelligence techniques to trade\nstocks. The ef\ufb01cacy of the daily Twitter sentiment on predicting\nthe stock return is examined using machine learning methods.\nReinforcement learning(Q-learning) is applied to generate the\noptimal trading policy based on the sentiment signal. The\npredicting power of the sentiment signal is more signi\ufb01cant\nif the stock price is driven by the expectation on the company\ngrowth and when the company has a major event that draws\nthe public attention. The optimal trading strategy based on\nreinforcement learning outperforms the trading strategy based\non the machine learning prediction.\nI. INTRODUCTION\nIn a world where traditional \ufb01nancial information is ubiq-\nuitous and the \ufb01nancial models are largely homogeneous,\n\ufb01nding hidden information that has not been priced in from\nalternative data is critical. The recent development in Natural\nLanguage Processing provides such opportunities to look\ninto text data in addition to numerical data. When the\nmarket sets the stock price, it is not uncommon that the\nexpectation of the company growth outweighs the company\nfundamentals. Twitter, a online news and social network\nwhere the users post and interact with messages to express\nviews about certain topics, contains valuable information on\nthe public mood and sentiment. A collection of research [1]\n[6] have shown that there is a positive correlation between\nthe \u201dpublic mood\u201d and the \u201dmarket mood\u201d. Other research[2]\nalso shows that signi\ufb01cant correlation exists between the\nTwitter sentiment and the abnormal return during the peaks\nof the Twitter volume during a major event.\nOnce a signal that has predicting power on the stock\nmarket return is constructed, a trading strategy to express the\nview of the signal is needed. Traditionally, the quantitative \ufb01-\nnance industry relies on backtest, a process where the trading\nstrategies are tuned during the simulations or optimizations.\nReinforcement learning provides a way to \ufb01nd the optimal\npolicy by maximizing the expected future utility. There are\nrecent attempts from the Arti\ufb01cial Intelligence community\nto apply reinforcement learning to asset allocation [5], algo-\nrithmic trading[3][7], and portfolio management[4].\nThe contribution of this paper is two-fold: First, the pre-\ndicting power of Twitter sentiment is evaluated. Our results\nshow sentiment is more suitable to construct alpha signals\nrather than total return signals and shows predicting power\nespecially when the Twitter volume is high. Second, we\nproposed a trading strategy based on reinforcement learning\n(Q-learning) that takes the sentiment features as part of its\nstates.\nThe paper is constructed as follows: In the second section,\nscraping Tweets from Twitter website and preprocessing the\ndata are described in details. In the third section, assigning\nsentiment scores to the text data is discussed. In the fourth\nsection, feature engineering and prediction based on the\nsentiment score is discussed. In the \ufb01fth section, how the\nreinforcement learning is applied to generate the optimal\ntrading strategy is described.\nII. T WITTER DATA SCRAPING AND PREPROCESSING\nThere are two options of getting the Tweets. First, Twitter\nprovides an API to download the Tweets. However, rate limit\nand history limit make it not an option for this paper. Second,\nscrapping Tweets directly from Twitter website. Using the\nsecond option, the daily Tweets for stocks of interest from\n2015 January to 2017 June were downloaded.\nThe predicting power of Twitter sentiment varies from\nstock to stock. For stocks that are mostly driven by the com-\npany fundamentals and hold by the institutional investors,\nthe predicting power of the Twitter sentiment is limited.\nFor stocks that are priced by the public expectation on the\ncompany\u2019s future growth, Twitter sentiment describes the\ncon\ufb01dence and expectation level of the investors.\nFor this reason, two companies from the same industry,\nTesla and Ford are investigated on how Twitter sentiment\ncould impact the stock price. Tesla is an electronic car\ncompany that shows consecutive negative operating cash \ufb02ow\nand net income but carries very high expectation from the\npublic. Ford, is a traditional auto maker whose stock prices\nhas been stabilized to represent the company fundamentals.\nTo investigate how different key words impact the predict-\ning power of the sentiment score, two Tweet sets, a ticker\nset and a product set, are prepared for each stock. The \ufb01rst\nset of Tweets are searched strictly according to the stock\nticker. The second set of Tweets are searched according to\nthe company\u2019s products and news. The keywords for the\nsecond dataset are de\ufb01ned according to the top twenty related\nkeywords of the stock ticker according to Google Trend, a\nweb facility shows how often a certain word is searched\nrelative to Google\u2019s total search volume. For example, \u201dElon\nMusk\u201d is among the set of keywords that retrieve the second\ntweets set for Tesla.\nTweets contain irregular symbols, url and emoji etc which\nhas to be preprocessed so that the NLP algorithm can extract\nthe relevant information ef\ufb01ciently. Examples of preprocess-\ning are described as below:\n\u2022 Filter out tweets that contains http or .com Motiva-\ntion: They\u2019re usually ads e.g. #Fhotoroom #iPhone\narXiv:1801.02243v1  [cs.AI]  7 Jan 2018\n\nFig. 3. The chart displays the accuracy on predicting the \u201dalpha\u201d, which\nde\ufb01nes as the return of the stock minus the return of its sector ETF.\nproduct dataset is searched using keywords that related to\nthe company\u2019s product and other related topic(see session II\nfor more details). The former dataset represents the investors\u2019\nsentiment, while the latter dataset represents customers sen-\ntiment.\nIn the Tesla case, using product tweets consistently outper-\nforms using the ticker tweets(accuracy 0.6 vs 0.5), it is less\nso in the Ford case(0.58 vs 0.55). The result is displayed\nin Figure 3 First, this is because Tesla\u2019s stock price is\ndriven more by the sentiment on its product instead of the\nstock itself. For Ford, not many people actually express their\nopinion about Ford\u2019s product via Twitter. Secondly, Tesla has\nmany more product tweets than ticker tweets, but Ford is\nopposite.\nD. Predicting using logistic regression and SVM\nIn most cases, SVM performs only slightly better than\nlogistic regression in validation set, although much better\nin testing set. This may be because the dataset is not\nlarge enough to prevent SVM over\ufb01tting. The comparision\nbetween the logistic regression and the SVM is displayed in\nFigure 3\nE. Predicting Total Return vs Alpha\nIt is important to identify which is a better target for the\nprediction. Two targets, predicting \u201dalpha or predicting \u201dtotal\nreturn\u201d are compared. \u201dAlpha\u201d de\ufb01nes as the excess stock\nreturn over its sector ETF. \u201dTotal return\u201d is the absolution\nstock return. Predicting \u201dalpha\u201d achieves better performance\nthan predicting total return. This is because the sentiment\nis more related to stocks idiosyncratic. Good sentiments\ntowards a speci\ufb01c company or its stock wont override the\noverall stock market or sectors impact on the stock return.\nF . Tesla vs Ford\nThe prediction accuracy on Tesla is higher than Ford\naccording to Figure3. The reason is because Tesla\u2019s stock\nprice largely re\ufb02ects the sentiment and con\ufb01dence level of\nthe public. The company has consecutive negative cash \ufb02ow\nand net income, making prediction based on its fundamental\ninformation unrealistic. On the other hand, the stock price\nof Ford, which is a traditional automaker, is not that related\nto the public sentiment.\nG. Feature selection and over\ufb01tting\nTo improve the model accuracy, more features were con-\nstructed. However, more features do not result in better\naccuracy. For example, in Figure 4, adding more features\nimprove the training accuracy but deteriorates out-of-sample\naccuracy due to over\ufb01tting.\nFig. 4. The chart shows an example of over\ufb01tting in the SVM model. The\nover\ufb01tting is caused by adding too many features to the model inputs but\nnot providing enough data for the model to generalize. Different lines shows\nthe SVM performance under different \u03b3 parameter. None of the parameter\nachieves better accuracy than a restricted set of features.\nThe recursive feature elimination and cross validation\n(RFECV) for feature selection is experimented during the\nfeature selection phase. However, only similar or even\nslightly worse performance was achieved by RFECV than\nselecting features according to domain knowledge and intu-\nition. This is because recursive feature elimination is a greedy\nalgorithm and thus doesnt guarantee optimal solution.\nV. Q- LEARNING\nQ-learning is a model-free reinforcement learning tech-\nnique. Speci\ufb01cally, Q-learning can be used to \ufb01nd an optimal\npolicy given a Markov Decision Process(MDP). Instead of\nlearning the transition probability, Q-learning directly learns\nthe expected utility of taking an action from a certain state.\nBy maximizing the expected utility of the certain state, the\noptimal policy is found.\nTraditionally, quants propose trading strategies according\nto backtest, where the optimal parameters are tuned by\nmaximizing the objective function based on historical data.\nHowever, this common practice adopted by the investment\nindustry has drawbacks. First, it over-\ufb01ts the historical data\nand doesn\u2019t generalize to out of sample data. In addition, the\nmodel need to be recalibrated periodically due to the eco-\nnomic regime change. A strategy signi\ufb01cantly outperforms\nin a high volatility environment might suffer signi\ufb01cantly in\na low volatility environment.\n3\n\nlater. Q-learning does better when the state is more common\nbecause it accumulates more experience about the situation\nbut might not take the best action when a outlier state is\npresented. The performance of the q-learning varies during\ndifferent batch due to the random nature of exploitation\nand exploration. In general Q-learning is able to deliver\nbetter performance than using the binary prediction from\nthe machine learning models. Both of the Q-learning and\nmachine learning model outperform the baseline model.\nVI. F UTURE WORK\nThere are many areas that can be improved given more\nresource and data. Below is a list of the improvement that\ncould make this idea more robust.\n\u2022 Use intraday data to test the sentiment signal and Q-\nlearning. By training with more data and trading more\npromptly, we expect both sentiment machine learning\nmodel and Q-learning to do better.\n\u2022 With more data, more features can be considered and\nincorporated into the model.\n\u2022 Apply different function approximators, for example,\nneural net, to better generalize the states and provide\nmore stable behavior\n\u2022 Add another class to the existing binary classi\ufb01er in-\nsigni\ufb01cant price change. The is motivated by preventing\nthe classi\ufb01er to \ufb01t to the noise inherent in stock market\nprice movement, and lumps small, statistically insignif-\nicant upward or downward movements indiscriminately\nwith large ones.\n\u2022 Add crude oil future price as a feature to predict Tesla\nstock alpha return sign\n\u2022 Extend the sentiment analysis to other stocks and Cryp-\ntocurrency, which is an asset class that driven even more\nby the public sentiment.\nVII. CONCLUSIONS\nThe paper explores the possibility to predict stock price\nusing text data and reinforcement learning technique. Pre-\ndicting stock price direction using Twitter sentiment is chal-\nlenging but promising. Which stock and what to predict is\nmore important than how to predict. For example, Tesla,\na company driven by the expectation of the company\u2019s\ngrowth is a better target than Ford, a traditional auto maker.\nReinforcement learning is applied to \ufb01nd the optimal trading\npolicy by learning the feedbacks from the market. The Q-\nlearning is able to adapt automatically if the market regime\nshifts and avoid backtesting, a process applied by investment\nindustry that often over\ufb01t the historical data. Both of the ma-\nchine learning model and the Q-learning model outperforms\nthe baseline model, which is a logistic regression without\nsentiment features.\nACKNOWLEDGMENT\nWe would like to thank Anna Wang, who is the project\nmentor, gives very practical suggestions and guidance. We\nwould like to thank Standford University for the very chal-\nlenging and exciting CS221 course materials and Prof. Percy\nLiang who did such a great job getting us interested in\nsentiment analysis and reinforcement learning\nREFERENCES\n[1] Johan Bollen and Huina Mao. Twitter mood as a stock market predictor.\nComputer, 44(10):91\u201394, 2011.\n[2] Johan Bollen, Huina Mao, and Xiao-Jun Zeng. Twitter mood predicts\nthe stock market. Journal of Computational Science , 2(1):1\u20138, March\n2011. arXiv: 1010.3003.\n[3] James Cumming, Dalal Alrajeh, and Luke Dickens. An Investigation\ninto the Use of Reinforcement Learning Techniques within the Algo-\nrithmic Trading Domain. 2015.\n[4] Zhengyao Jiang, Dixing Xu, and Jinjun Liang. A Deep Reinforcement\nLearning Framework for the Financial Portfolio Management Problem.\narXiv preprint arXiv:1706.10059 , 2017.\n[5] J. Moody and M. Saffell. Learning to trade via direct reinforcement.\nIEEE Transactions on Neural Networks , 12(4):875\u2013889, July 2001.\n[6] Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li, Huayi Li, and Xiaotie\nDeng. Exploiting Topic based Twitter Sentiment for Stock Prediction.\nACL (2), 2013:24\u201329, 2013.\n[7] Jonah Varon and Anthony Soroka. Stock Trading with Reinforcement\nLearning. 2016.\n5",
            0.3839363330000012
          ]
        },
        {
          "answer": "The tweets used to output the daily sentiment signal are those that have been filtered according to specific criteria mentioned in the context: removing hashtags (#), user mentions (@), tabs, and extra spaces, filtering out tweets with consecutive question marks (??) because their coding is usually not recognizable, and filtering out non-English tweets using Google's Langdetect package. Additionally, the sentiment analysis considers tweets that are directly related to the stock ticker or the company's products and news, based on keywords defined according to the top twenty related keywords of the stock ticker according to Google Trend. These tweets are then processed to assign a sentiment score using Stanford CoreNLP software, which computes a sentiment score for each sentence ranging from 0 to 4, with 0 indicating negative sentiment and 4 indicating very positive sentiment. For tweets with multiple sentences, the average of the sentiment scores of all sentences is used as the sentiment score of the tweets.",
          "relevant_chunks": [
            "https://www.fhotoroom.com/fhotos/\n\u2022 Remove #hashtag, @user, tabs and extra spaces\n\u2022 Filter out tweets that contain consecutive two question\nmarks (?). The reason is the coding of these tweets is\nusually not recognizable.\n\u2022 Filtering out none-English tweets using Googles\nLangdetect package.\nIII. S ENTIMENT SCORE\nTo translate each tweet into a sentiment score, the Stanford\ncoreNLP software was used. Stanford CoreNLP is designed\nto make linguistic analysis accessible to the general public. It\nprovides named Entity Recognition, co-reference and basic\ndependencies and many other text understanding applica-\ntions. An example that illustrate the basic functionality of\nStanford coreNLP is shown in Figure.1\nFig. 1. Demo of the functionalities provided by Stanford CoreNLP\ncoreNLP can compute a sentiment score for each sentence\nwith value ranging from 0 to 4 , where 0 stands for negative,\nand 4 stands for very positive. For tweets with multiple\nsentences, the average of the sentiment scores of all sentences\nis used as the sentiment score of the Tweets.\nThe number of Tweets varies everyday from a couple of\nhundreds to over ten thousands, depends on if the company\nhas a major event that attracts the public attention. The\nsentiment scores are normalized between 0 to 1, and features\nbased on the sentiment score is constructed and normalized.\nFigure 2 shows the relationship between Tesla stock return\nand stock sentiment score. According the distribution of the\nsentiment score, the sentiment on Tesla is slightly skewed\ntowards positive during the testing period. The price has\nbeen increased signi\ufb01cantly during the testing period, which\nre\ufb02ected the positive sentiment. The predicting power of\nsentiment score is more signi\ufb01cant when the sentiment is\nmore extreme and less so when the sentiment is neutral.\nIV. T HE SENTIMENT MACHINE LEARNING MODEL\nA. Feature Engineering\nFeature engineering is the process to extract meaningful\ninformation from the raw data in order to improve the\nperformance of machine learning mode. Domain knowledge\nand intuition are often applied to keep the number of the\nFig. 2. Histogram\nfeatures reasonable relative to the training data size. Two\ncategories of features are de\ufb01nes: technical features and\nsentiment features. The technical features include previous\nday\u2019s return and volume, price momentum and volatility. The\nsentiment features include number of Tweets, daily average\nsentiment score, cross-section sentiment volatility, sentiment\nmomentum and reversal.\nB. Machine Learning Prediction Model\nThe logistic regression with L1 regularization and RBF-\nkernel SVM are applied to predict a binary outcome, i.e.\nwhether the stock return will be positive or negative in the\nnext day. Both technical and sentiment-based features carry\nimportant information about the stock price and therefore are\nprovided as the model inputs. Half of the dataset is used for\ntraining and the rest is used for testing.\nThe 3 fold cross validation is applied to learn the model\nhyper-parameters. Speci\ufb01cally, the hyper-parameters C of\nboth models and of RBF-kernel SVM are learned such that\nthe dev set accuracy is maximized. The hyper-parameter C\nin logistic regression determines the degree of regularization.\nSmaller C means more regularization, i.e. high bias and low\nvariance. RBF-kernel SVM has two hyper-parameters, C and\n. C controls the width of soft margin, smaller C allows\nplacing more samples on the wrong side of the margin. is a\nparameter in RBF kernel. A larger means a Gaussian with\nsmaller variance and thus less in\ufb02uence of support vectors.\nTypically, small C and large lead to high bias and low\nvariance.\nTo evaluate if the sentiment feature improves the pre-\ndiction accuracy, a baseline model is de\ufb01ned. The baseline\napplies linear logistic regression to a set of stock technical\nsignals to predict the following days stock return sign (+/).\nNo sentiment features are provided to the baseline model.\nC. Predicting using ticker dataset and product dataset\nThe predicting power for the ticker dataset and product\ndataset are compared. The ticker dataset contains tweets\nthat searched strictly according to the stock ticker. The\n2\n\nTrading the Twitter Sentiment with Reinforcement Learning\nCatherine Xiao\ncatherine.xiao1@gmail.com\nWanfeng Chen\nwanfengc@gmail.com\nAbstract\u2014 This paper is to explore the possibility to use\nalternative data and arti\ufb01cial intelligence techniques to trade\nstocks. The ef\ufb01cacy of the daily Twitter sentiment on predicting\nthe stock return is examined using machine learning methods.\nReinforcement learning(Q-learning) is applied to generate the\noptimal trading policy based on the sentiment signal. The\npredicting power of the sentiment signal is more signi\ufb01cant\nif the stock price is driven by the expectation on the company\ngrowth and when the company has a major event that draws\nthe public attention. The optimal trading strategy based on\nreinforcement learning outperforms the trading strategy based\non the machine learning prediction.\nI. INTRODUCTION\nIn a world where traditional \ufb01nancial information is ubiq-\nuitous and the \ufb01nancial models are largely homogeneous,\n\ufb01nding hidden information that has not been priced in from\nalternative data is critical. The recent development in Natural\nLanguage Processing provides such opportunities to look\ninto text data in addition to numerical data. When the\nmarket sets the stock price, it is not uncommon that the\nexpectation of the company growth outweighs the company\nfundamentals. Twitter, a online news and social network\nwhere the users post and interact with messages to express\nviews about certain topics, contains valuable information on\nthe public mood and sentiment. A collection of research [1]\n[6] have shown that there is a positive correlation between\nthe \u201dpublic mood\u201d and the \u201dmarket mood\u201d. Other research[2]\nalso shows that signi\ufb01cant correlation exists between the\nTwitter sentiment and the abnormal return during the peaks\nof the Twitter volume during a major event.\nOnce a signal that has predicting power on the stock\nmarket return is constructed, a trading strategy to express the\nview of the signal is needed. Traditionally, the quantitative \ufb01-\nnance industry relies on backtest, a process where the trading\nstrategies are tuned during the simulations or optimizations.\nReinforcement learning provides a way to \ufb01nd the optimal\npolicy by maximizing the expected future utility. There are\nrecent attempts from the Arti\ufb01cial Intelligence community\nto apply reinforcement learning to asset allocation [5], algo-\nrithmic trading[3][7], and portfolio management[4].\nThe contribution of this paper is two-fold: First, the pre-\ndicting power of Twitter sentiment is evaluated. Our results\nshow sentiment is more suitable to construct alpha signals\nrather than total return signals and shows predicting power\nespecially when the Twitter volume is high. Second, we\nproposed a trading strategy based on reinforcement learning\n(Q-learning) that takes the sentiment features as part of its\nstates.\nThe paper is constructed as follows: In the second section,\nscraping Tweets from Twitter website and preprocessing the\ndata are described in details. In the third section, assigning\nsentiment scores to the text data is discussed. In the fourth\nsection, feature engineering and prediction based on the\nsentiment score is discussed. In the \ufb01fth section, how the\nreinforcement learning is applied to generate the optimal\ntrading strategy is described.\nII. T WITTER DATA SCRAPING AND PREPROCESSING\nThere are two options of getting the Tweets. First, Twitter\nprovides an API to download the Tweets. However, rate limit\nand history limit make it not an option for this paper. Second,\nscrapping Tweets directly from Twitter website. Using the\nsecond option, the daily Tweets for stocks of interest from\n2015 January to 2017 June were downloaded.\nThe predicting power of Twitter sentiment varies from\nstock to stock. For stocks that are mostly driven by the com-\npany fundamentals and hold by the institutional investors,\nthe predicting power of the Twitter sentiment is limited.\nFor stocks that are priced by the public expectation on the\ncompany\u2019s future growth, Twitter sentiment describes the\ncon\ufb01dence and expectation level of the investors.\nFor this reason, two companies from the same industry,\nTesla and Ford are investigated on how Twitter sentiment\ncould impact the stock price. Tesla is an electronic car\ncompany that shows consecutive negative operating cash \ufb02ow\nand net income but carries very high expectation from the\npublic. Ford, is a traditional auto maker whose stock prices\nhas been stabilized to represent the company fundamentals.\nTo investigate how different key words impact the predict-\ning power of the sentiment score, two Tweet sets, a ticker\nset and a product set, are prepared for each stock. The \ufb01rst\nset of Tweets are searched strictly according to the stock\nticker. The second set of Tweets are searched according to\nthe company\u2019s products and news. The keywords for the\nsecond dataset are de\ufb01ned according to the top twenty related\nkeywords of the stock ticker according to Google Trend, a\nweb facility shows how often a certain word is searched\nrelative to Google\u2019s total search volume. For example, \u201dElon\nMusk\u201d is among the set of keywords that retrieve the second\ntweets set for Tesla.\nTweets contain irregular symbols, url and emoji etc which\nhas to be preprocessed so that the NLP algorithm can extract\nthe relevant information ef\ufb01ciently. Examples of preprocess-\ning are described as below:\n\u2022 Filter out tweets that contains http or .com Motiva-\ntion: They\u2019re usually ads e.g. #Fhotoroom #iPhone\narXiv:1801.02243v1  [cs.AI]  7 Jan 2018\n\nlater. Q-learning does better when the state is more common\nbecause it accumulates more experience about the situation\nbut might not take the best action when a outlier state is\npresented. The performance of the q-learning varies during\ndifferent batch due to the random nature of exploitation\nand exploration. In general Q-learning is able to deliver\nbetter performance than using the binary prediction from\nthe machine learning models. Both of the Q-learning and\nmachine learning model outperform the baseline model.\nVI. F UTURE WORK\nThere are many areas that can be improved given more\nresource and data. Below is a list of the improvement that\ncould make this idea more robust.\n\u2022 Use intraday data to test the sentiment signal and Q-\nlearning. By training with more data and trading more\npromptly, we expect both sentiment machine learning\nmodel and Q-learning to do better.\n\u2022 With more data, more features can be considered and\nincorporated into the model.\n\u2022 Apply different function approximators, for example,\nneural net, to better generalize the states and provide\nmore stable behavior\n\u2022 Add another class to the existing binary classi\ufb01er in-\nsigni\ufb01cant price change. The is motivated by preventing\nthe classi\ufb01er to \ufb01t to the noise inherent in stock market\nprice movement, and lumps small, statistically insignif-\nicant upward or downward movements indiscriminately\nwith large ones.\n\u2022 Add crude oil future price as a feature to predict Tesla\nstock alpha return sign\n\u2022 Extend the sentiment analysis to other stocks and Cryp-\ntocurrency, which is an asset class that driven even more\nby the public sentiment.\nVII. CONCLUSIONS\nThe paper explores the possibility to predict stock price\nusing text data and reinforcement learning technique. Pre-\ndicting stock price direction using Twitter sentiment is chal-\nlenging but promising. Which stock and what to predict is\nmore important than how to predict. For example, Tesla,\na company driven by the expectation of the company\u2019s\ngrowth is a better target than Ford, a traditional auto maker.\nReinforcement learning is applied to \ufb01nd the optimal trading\npolicy by learning the feedbacks from the market. The Q-\nlearning is able to adapt automatically if the market regime\nshifts and avoid backtesting, a process applied by investment\nindustry that often over\ufb01t the historical data. Both of the ma-\nchine learning model and the Q-learning model outperforms\nthe baseline model, which is a logistic regression without\nsentiment features.\nACKNOWLEDGMENT\nWe would like to thank Anna Wang, who is the project\nmentor, gives very practical suggestions and guidance. We\nwould like to thank Standford University for the very chal-\nlenging and exciting CS221 course materials and Prof. Percy\nLiang who did such a great job getting us interested in\nsentiment analysis and reinforcement learning\nREFERENCES\n[1] Johan Bollen and Huina Mao. Twitter mood as a stock market predictor.\nComputer, 44(10):91\u201394, 2011.\n[2] Johan Bollen, Huina Mao, and Xiao-Jun Zeng. Twitter mood predicts\nthe stock market. Journal of Computational Science , 2(1):1\u20138, March\n2011. arXiv: 1010.3003.\n[3] James Cumming, Dalal Alrajeh, and Luke Dickens. An Investigation\ninto the Use of Reinforcement Learning Techniques within the Algo-\nrithmic Trading Domain. 2015.\n[4] Zhengyao Jiang, Dixing Xu, and Jinjun Liang. A Deep Reinforcement\nLearning Framework for the Financial Portfolio Management Problem.\narXiv preprint arXiv:1706.10059 , 2017.\n[5] J. Moody and M. Saffell. Learning to trade via direct reinforcement.\nIEEE Transactions on Neural Networks , 12(4):875\u2013889, July 2001.\n[6] Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li, Huayi Li, and Xiaotie\nDeng. Exploiting Topic based Twitter Sentiment for Stock Prediction.\nACL (2), 2013:24\u201329, 2013.\n[7] Jonah Varon and Anthony Soroka. Stock Trading with Reinforcement\nLearning. 2016.\n5",
            0.8699840409999986
          ]
        },
        {
          "answer": "The baseline machine learning prediction approach applies linear logistic regression to a set of stock technical signals to predict the following day's stock return sign (+/-). No sentiment features are provided to the baseline model.",
          "relevant_chunks": [
            "https://www.fhotoroom.com/fhotos/\n\u2022 Remove #hashtag, @user, tabs and extra spaces\n\u2022 Filter out tweets that contain consecutive two question\nmarks (?). The reason is the coding of these tweets is\nusually not recognizable.\n\u2022 Filtering out none-English tweets using Googles\nLangdetect package.\nIII. S ENTIMENT SCORE\nTo translate each tweet into a sentiment score, the Stanford\ncoreNLP software was used. Stanford CoreNLP is designed\nto make linguistic analysis accessible to the general public. It\nprovides named Entity Recognition, co-reference and basic\ndependencies and many other text understanding applica-\ntions. An example that illustrate the basic functionality of\nStanford coreNLP is shown in Figure.1\nFig. 1. Demo of the functionalities provided by Stanford CoreNLP\ncoreNLP can compute a sentiment score for each sentence\nwith value ranging from 0 to 4 , where 0 stands for negative,\nand 4 stands for very positive. For tweets with multiple\nsentences, the average of the sentiment scores of all sentences\nis used as the sentiment score of the Tweets.\nThe number of Tweets varies everyday from a couple of\nhundreds to over ten thousands, depends on if the company\nhas a major event that attracts the public attention. The\nsentiment scores are normalized between 0 to 1, and features\nbased on the sentiment score is constructed and normalized.\nFigure 2 shows the relationship between Tesla stock return\nand stock sentiment score. According the distribution of the\nsentiment score, the sentiment on Tesla is slightly skewed\ntowards positive during the testing period. The price has\nbeen increased signi\ufb01cantly during the testing period, which\nre\ufb02ected the positive sentiment. The predicting power of\nsentiment score is more signi\ufb01cant when the sentiment is\nmore extreme and less so when the sentiment is neutral.\nIV. T HE SENTIMENT MACHINE LEARNING MODEL\nA. Feature Engineering\nFeature engineering is the process to extract meaningful\ninformation from the raw data in order to improve the\nperformance of machine learning mode. Domain knowledge\nand intuition are often applied to keep the number of the\nFig. 2. Histogram\nfeatures reasonable relative to the training data size. Two\ncategories of features are de\ufb01nes: technical features and\nsentiment features. The technical features include previous\nday\u2019s return and volume, price momentum and volatility. The\nsentiment features include number of Tweets, daily average\nsentiment score, cross-section sentiment volatility, sentiment\nmomentum and reversal.\nB. Machine Learning Prediction Model\nThe logistic regression with L1 regularization and RBF-\nkernel SVM are applied to predict a binary outcome, i.e.\nwhether the stock return will be positive or negative in the\nnext day. Both technical and sentiment-based features carry\nimportant information about the stock price and therefore are\nprovided as the model inputs. Half of the dataset is used for\ntraining and the rest is used for testing.\nThe 3 fold cross validation is applied to learn the model\nhyper-parameters. Speci\ufb01cally, the hyper-parameters C of\nboth models and of RBF-kernel SVM are learned such that\nthe dev set accuracy is maximized. The hyper-parameter C\nin logistic regression determines the degree of regularization.\nSmaller C means more regularization, i.e. high bias and low\nvariance. RBF-kernel SVM has two hyper-parameters, C and\n. C controls the width of soft margin, smaller C allows\nplacing more samples on the wrong side of the margin. is a\nparameter in RBF kernel. A larger means a Gaussian with\nsmaller variance and thus less in\ufb02uence of support vectors.\nTypically, small C and large lead to high bias and low\nvariance.\nTo evaluate if the sentiment feature improves the pre-\ndiction accuracy, a baseline model is de\ufb01ned. The baseline\napplies linear logistic regression to a set of stock technical\nsignals to predict the following days stock return sign (+/).\nNo sentiment features are provided to the baseline model.\nC. Predicting using ticker dataset and product dataset\nThe predicting power for the ticker dataset and product\ndataset are compared. The ticker dataset contains tweets\nthat searched strictly according to the stock ticker. The\n2\n\nThe Q-learning, in the opposite, learns from the feedback\nfrom the market, and generates the optimal trading strategy\naccording to past experience, and automatically adapts to the\nnew market regime.\nIn this paper, the Q-learning algorithm is applied to gen-\nerate the optimal trading strategy. The market is modeled as\na Markov Decision Process where the outcomes are random\nand not under the control of the decision maker. The states\ncontain information of three categories: technical indicators,\nsentiment features and portfolio information. The actions\ncontains buy, sell and hold. The reward is the next day market\nreturn. The limit of leverage and the loss-cutting threshold\nare implemented in the relation ship of successor state and\naction. For example, if the leverage constrain has been met,\nthe actions that valid for this state are only \u201dhold\u201d or \u201dsell\u201d. If\nthe loss cutting threshold has been triggered, say the portfolio\nlost half of the capital and this is the maximum tolerance of\nloss, only the action that exit current position is valid.\nA. Learning\nFormally, the learning process de\ufb01nes as below. In Q-\nlearning the optimal expected utility of a (state, action) pair\n\u02c6Qopt(s,a) is updated with the rewards r and the expected\nutility of the subsequent state \u02c6Vopt(s\u2032) after taking the action\na.\n\u02c6Qopt(s,a) \u2190(1 \u2212\u03b7) \u02c6Qopt(s,a) +\u03b7(r+ \u03b3\u02c6Vopt(s\u2032) (1)\nVopt(s\u2032) = max\na\u2032\u2208Actions(s\u2032)\n\u02c6Qopt(s\u2032,a\u2032) (2)\nThe optimal policy is proposed by Q-learning as\n\u03c0opt(s) = arg max\na\u2208act(s)\nQopt(s,a) (3)\nB. Function Approximation\nFunction approximation refers to the method to generalize\nunseen states by applying machine learning methods. The Q-\ntable stores the expected utility for each (state,action) pair\nthat has been explored. When predicting the expected utility\nfor a certain (state, action) pair, we will look up the Q-table.\nWhen the MDP has many states and actions, it is very likely\nthat a (state, action) pair has not been explored yet so the\nestimate is not accurate. It is too slow to look up a gigantic\ntable and most likely there is not enough training data to\nlearn each of the state individually. Function approximation\nuses features to capture the characteristics of the states and\napplies stochastic gradient descent to update the weights on\neach feature. More speci\ufb01cally, below equation is applied\nto generalize the unseen state in this paper. De\ufb01ne features\n\u03c6(s,a) and weights w, then\n\u02c6Qopt(s,a; w) =w\u00b7\u03c6(s,a) (4)\nFor each (s,a,r,s \u2032), apply stochastic gradient descent to\nupdate the weights.\nw\u2190w\u2212\u03b7[ \u02c6Qopt(s,a; w) \u2212(r+ \u03b3\u02c6Vopt(s\u2032))]\u03c6(s,a) (5)\nwhere \u03b7 is the learning rate, r is the reward and \u03b3 is the\ndiscount factor.\nC. Exploration and Exploitation\nIt is necessary to balance the exploration and exploitation.\nOne might suggest naively to take action only according\nto the optimal policy estimated by maximizing \u02c6Qopt(s,a).\nHowever, this greedy strategy is equivalent to stay in the\ncomfortable zone all the time in life, without gaining new\nexperience and unable to give reasonable prediction when\nencounters unseen situations. Another extreme is to always\nexplore by choosing an action randomly. Without apply-\ning the hard lesson learned and obtaining the rewards,\nthe algorithm can lead to unsatis\ufb01able utility at the end.\nTherefore, in this paper the Epsilon-greedy strategy is applied\nfor exploration. For a certain probability, the algorithm\nacts randomly(exploration), for the rest the algorithm acts\noptimally(exploitation).\nD. Result and Discussion\nFigure 5 shows the cumulative return over 1 year period.\nThe strategy trades daily. The Q-learning states include\nportfolio position, sentiment features and technical indicators\nsuch as price momentum. The machine learning strategy\npredicts the binary movement (+ or -) of next trading day\nprice based on sentiment features and technical indicators.\nThe backtest rule based on the machine learning prediction\nis to long the stock if the prediction is +, short the stock if -.\nThe baseline is the same with machine learning except only\nthe technical indicator was used as the feature. The oracle\nmodel of this project is a trader who has insider information\nabout the stock and be able to bet and act correctly on every\nsingle day of the testing period. The oracle model is able\nto achieve 6 times of the initial capital at the end of testing\nperiod.\nFig. 5. The chart shows the trading strategy derived from Q-learning(in\nblue) outperform the backtest result using machine learning features(in red).\nBoth of Q-learning strategy and machine learning strategy outperform the\nbaseline(in green).\nThere are observations that worth a discussion. At the\nbeginning of the testing period, the Q-learning has not learnt\nhow to estimate the expected utility of a certain action yet.\nThe performance of the initial period is more unstable than\n4\n\nFig. 3. The chart displays the accuracy on predicting the \u201dalpha\u201d, which\nde\ufb01nes as the return of the stock minus the return of its sector ETF.\nproduct dataset is searched using keywords that related to\nthe company\u2019s product and other related topic(see session II\nfor more details). The former dataset represents the investors\u2019\nsentiment, while the latter dataset represents customers sen-\ntiment.\nIn the Tesla case, using product tweets consistently outper-\nforms using the ticker tweets(accuracy 0.6 vs 0.5), it is less\nso in the Ford case(0.58 vs 0.55). The result is displayed\nin Figure 3 First, this is because Tesla\u2019s stock price is\ndriven more by the sentiment on its product instead of the\nstock itself. For Ford, not many people actually express their\nopinion about Ford\u2019s product via Twitter. Secondly, Tesla has\nmany more product tweets than ticker tweets, but Ford is\nopposite.\nD. Predicting using logistic regression and SVM\nIn most cases, SVM performs only slightly better than\nlogistic regression in validation set, although much better\nin testing set. This may be because the dataset is not\nlarge enough to prevent SVM over\ufb01tting. The comparision\nbetween the logistic regression and the SVM is displayed in\nFigure 3\nE. Predicting Total Return vs Alpha\nIt is important to identify which is a better target for the\nprediction. Two targets, predicting \u201dalpha or predicting \u201dtotal\nreturn\u201d are compared. \u201dAlpha\u201d de\ufb01nes as the excess stock\nreturn over its sector ETF. \u201dTotal return\u201d is the absolution\nstock return. Predicting \u201dalpha\u201d achieves better performance\nthan predicting total return. This is because the sentiment\nis more related to stocks idiosyncratic. Good sentiments\ntowards a speci\ufb01c company or its stock wont override the\noverall stock market or sectors impact on the stock return.\nF . Tesla vs Ford\nThe prediction accuracy on Tesla is higher than Ford\naccording to Figure3. The reason is because Tesla\u2019s stock\nprice largely re\ufb02ects the sentiment and con\ufb01dence level of\nthe public. The company has consecutive negative cash \ufb02ow\nand net income, making prediction based on its fundamental\ninformation unrealistic. On the other hand, the stock price\nof Ford, which is a traditional automaker, is not that related\nto the public sentiment.\nG. Feature selection and over\ufb01tting\nTo improve the model accuracy, more features were con-\nstructed. However, more features do not result in better\naccuracy. For example, in Figure 4, adding more features\nimprove the training accuracy but deteriorates out-of-sample\naccuracy due to over\ufb01tting.\nFig. 4. The chart shows an example of over\ufb01tting in the SVM model. The\nover\ufb01tting is caused by adding too many features to the model inputs but\nnot providing enough data for the model to generalize. Different lines shows\nthe SVM performance under different \u03b3 parameter. None of the parameter\nachieves better accuracy than a restricted set of features.\nThe recursive feature elimination and cross validation\n(RFECV) for feature selection is experimented during the\nfeature selection phase. However, only similar or even\nslightly worse performance was achieved by RFECV than\nselecting features according to domain knowledge and intu-\nition. This is because recursive feature elimination is a greedy\nalgorithm and thus doesnt guarantee optimal solution.\nV. Q- LEARNING\nQ-learning is a model-free reinforcement learning tech-\nnique. Speci\ufb01cally, Q-learning can be used to \ufb01nd an optimal\npolicy given a Markov Decision Process(MDP). Instead of\nlearning the transition probability, Q-learning directly learns\nthe expected utility of taking an action from a certain state.\nBy maximizing the expected utility of the certain state, the\noptimal policy is found.\nTraditionally, quants propose trading strategies according\nto backtest, where the optimal parameters are tuned by\nmaximizing the objective function based on historical data.\nHowever, this common practice adopted by the investment\nindustry has drawbacks. First, it over-\ufb01ts the historical data\nand doesn\u2019t generalize to out of sample data. In addition, the\nmodel need to be recalibrated periodically due to the eco-\nnomic regime change. A strategy signi\ufb01cantly outperforms\nin a high volatility environment might suffer signi\ufb01cantly in\na low volatility environment.\n3",
            0.42995587500000454
          ]
        }
      ],
      "reference_answers": [
        "Yes",
        "Tesla and Ford are investigated on how Twitter sentiment could impact the stock price",
        "linear logistic regression to a set of stock technical signals"
      ]
    },
    "1706.01875.pdf": {
      "average_exact_match": 0.25,
      "average_bleu_score": 0.1294392441137679,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.36694860117111255,
          "recall": 0.6051136363636364,
          "f1": 0.4245945945945946
        },
        "rouge-2": {
          "precision": 0.3282967032967033,
          "recall": 0.5119047619047619,
          "f1": 0.3693737769080235
        },
        "rouge-L": {
          "precision": 0.3402732595966168,
          "recall": 0.5198863636363636,
          "f1": 0.38423423423423425
        }
      },
      "per_question_metrics": [
        {
          "question": "What was the baseline?",
          "generated_answer": "The provided context does not specify a baseline for the classifier or evaluation metrics discussed. It focuses on the performance of a selected classifier, its accuracy, precision, recall, and the impact of dataset size and quality on these metrics, but does not mention a specific baseline against which these results were measured.",
          "reference_answer": "stochastic gradient descent, naive bayes, decision tree",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            }
          },
          "bleu_score": 0.0036785086103888684,
          "exact_match": 0.0
        },
        {
          "question": "What was their system's performance?",
          "generated_answer": "The system's performance, as evaluated on the holdout set, demonstrated an accuracy of 89.6% and an F1-score of 89.2%. This indicates that the classifier was highly effective in distinguishing between offensive and non-offensive comments, showcasing its robustness against over-fitting and its reliability in real-world application scenarios.",
          "reference_answer": "accuracy and F1-score of 89.6% and 89.2%, respectively accuracy and F1-score of 89.6% and 89.2%, respectively",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.22641509433962265,
              "recall": 0.5454545454545454,
              "f1": 0.32
            },
            "rouge-2": {
              "precision": 0.1346153846153846,
              "recall": 0.3333333333333333,
              "f1": 0.19178082191780818
            },
            "rouge-L": {
              "precision": 0.18867924528301888,
              "recall": 0.45454545454545453,
              "f1": 0.2666666666666667
            }
          },
          "bleu_score": 0.06433170337272145,
          "exact_match": 0.0
        },
        {
          "question": "What other political events are included in the database?",
          "generated_answer": "The database includes political events such as the Democratic and Republican National Conventions and the US presidential elections held on November 8, during the period covered by the study.",
          "reference_answer": "US presidential primaries, Democratic and Republican National Conventions",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.2413793103448276,
              "recall": 0.875,
              "f1": 0.3783783783783784
            },
            "rouge-2": {
              "precision": 0.17857142857142858,
              "recall": 0.7142857142857143,
              "f1": 0.2857142857142857
            },
            "rouge-L": {
              "precision": 0.1724137931034483,
              "recall": 0.625,
              "f1": 0.2702702702702703
            }
          },
          "bleu_score": 0.13351899845512327,
          "exact_match": 0.0
        },
        {
          "question": "What classifier did they use?",
          "generated_answer": "Random Forest",
          "reference_answer": "Random Forest",
          "rouge_scores": {
            "rouge-1": {
              "precision": 1.0,
              "recall": 1.0,
              "f1": 1.0
            },
            "rouge-2": {
              "precision": 1.0,
              "recall": 1.0,
              "f1": 1.0
            },
            "rouge-L": {
              "precision": 1.0,
              "recall": 1.0,
              "f1": 1.0
            }
          },
          "bleu_score": 0.316227766016838,
          "exact_match": 1.0
        }
      ],
      "questions": [
        "What was the baseline?",
        "What was their system's performance?",
        "What other political events are included in the database?",
        "What classifier did they use?"
      ],
      "generated_answers": [
        {
          "answer": "The provided context does not specify a baseline for the classifier or evaluation metrics discussed. It focuses on the performance of a selected classifier, its accuracy, precision, recall, and the impact of dataset size and quality on these metrics, but does not mention a specific baseline against which these results were measured.",
          "relevant_chunks": [
            "(a) Classi\ufb01er accuracy.\n(b) Classi\ufb01er precision.\n(c) Classi\ufb01er recall.\nFigure 3: Classi\ufb01er performance on holdout sets while\nvarying holdout set sizes and minimum con\ufb01dence\nthresholds.\nparameters (using a grid search). Based on the results of\nthis evaluation, we select a 100-estimator entropy-based\nsplitting random forest model as our classi\ufb01er. Table 1\nshows the mean accuracy and F1-score for each evalu-\nated classi\ufb01er during the 10-fold cross-validation.\nReal-world classi\ufb01er performance. To evaluate real-\nworld performance of our selected classi\ufb01er ( i.e., per-\nformance in the absence of model and parameter bias),\nwe perform classi\ufb01cation of the holdout set. On this set,\nour classi\ufb01er had an accuracy and F1-score of 89.6% and\n89.2%, respectively. These results show that in addition\nto superior accuracy during training and validation, our\nchosen classi\ufb01er is also robust against over-\ufb01tting.\nImpact of dataset quality and size. To understand\nhow the performance of our chosen classi\ufb01er model and\nparameters are impacted by: (1) the quality and consis-\ntency of manually assigned classes in the CrowdFlower\ndataset and (2) the size of the dataset, we re-evaluate the\nclassi\ufb01er while only considering tweets having a mini-\nmum con\ufb01dence score and varying the size of the holdout\nset. Speci\ufb01cally, our experiments considered con\ufb01dence\nthresholds of 0 (all tweets considered), .35 (only tweets\nwhere at least 35% of contributors agreed on a class were\nconsidered), and .70 (only tweets where at least 70% of\nthe contributors agreed on a class were considered) and\nvaried the holdout set sizes between 5% and 95% of all\ntweets meeting the con\ufb01dence threshold set for the ex-\nperiment.\nThe results illustrated in Figure 3 show the perfor-\nmance of the classi\ufb01er while evaluating the correspond-\ning holdout set. We make several conclusions from these\nresults:\n\u2022Beyond a (fairly low) threshold, the size of the train-\ning and validation set has little impact on classi-\n\ufb01er performance. We see that the accuracy, pre-\ncision, and recall have, at best, marginal improve-\nments with holdout set sizes smaller than 60%. This\nimplies that the CrowdFlower dataset is suf\ufb01cient\nfor building an offensive speech classi\ufb01er.\n\u2022Quality of manual labeling has a signi\ufb01cant impact\non the accuracy and precision of the classi\ufb01er. Us-\ning only tweets which had at least 70% of contrib-\nutors agreeing on a class resulted in between 5-7%\nhigher accuracy and up to 5% higher precision.\n\u2022Our classi\ufb01er achieves precision of over 95% and\nrecall of over 85% when considering only high con-\n\ufb01dence samples. This implies that the classi\ufb01er is\nmore likely to underestimate the presence of offen-\nsive speech \u2013 i.e., our results likely provide a lower-\nbound on the quantity of observed offensive speech.\n4 Measurements\nIn this section we quantify and characterize offensive-\nness in the political and general contexts using our offen-\nsive speech classi\ufb01er and the Reddit comments dataset\nwhich considers a random sample of comments made be-\ntween January 2015 and January 2017.\nOffensiveness over time. We \ufb01nd that on average\n8.4% of all political comments are offensive compared to\n7.8% of all apolitical comments. Figure 4 illustrates the\nfraction of offensive political and apolitical comments\nmade during each week in our study. We see that while\nthe fraction of apolitical offensive comments has stayed\nsteady, there has been an increase in the fraction of of-\nfensive political comments starting in July 2016. No-\ntably, this increase is observed after the conclusion of the\nUS presidential primaries and during the period of the\nDemocratic and Republican National Conventions and\ndoes not reduce even after the conclusion of the US presi-\ndential elections held on November 8. Participants in po-\nlitical subreddits were 2.6% more likely to observe offen-\n\nIn future work, we will extend our analysis of Reddit\nback to 2007 with the aim of formulating a more com-\nplete understanding of the dynamics of political incivil-\nity. For example, we seek to understand whether the\nhigh incidence of offensive speech we \ufb01nd in 2016 is\nunique to this particular campaign or if previous presi-\ndential campaigns witnessed similar spikes in incivility.\nWe will also examine whether there is a more general\nlong-term trend toward offensive online political speech,\nwhich would be consistent with what scholars have found\nwhen studying political elites [6, 16].\nReferences\n[1] CrowdFlower Blog. Hate speech identi\ufb01cation.\nURL: https://www.crowdflower.com/data/h\nate-speech-identification/ (Accessed May\n24, 2017).\n[2] Justin H Gross and Kaylee T Johnson. Twitter\ntaunts and tirades: Negative campaigning in the age\nof trump. PS: Political Science &amp; Politics,\n49(4):748\u2013754, 2016.\n[3] Hatebase. Meet the Hatebase API. URL: https:\n//www.hatebase.org/connect_api (Accessed\nMay 24, 2017).\n[4] Susan Herbst. Rude democracy: Civility and in-\ncivility in American politics. Temple University\nPress, 2010.\n[5] Matthew Honnibal and Mark Johnson. An Im-\nproved Non-monotonic Transition System for De-\npendency Parsing. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1373\u20131378, Lisbon, Portugal,\n2015. Association for Computational Linguistics.\n[6] Kathleen Hall Jamieson and Erika Falk. Continu-\nity and change in civility in the house. In Polar-\nized politics: Congress and the president in a parti-\nsan era, pages 96\u2013108. Washington, DC: CQ Press,\n2000.\n[7] Jamiew. All the dirty words from Google\u2019s \u201cwhat\ndo you love\u201d project. URL: https://gist.g\nithub.com/jamiew/1112488 (Accessed May 24,\n2017).\n[8] K Hazel Kwon and Anatoliy Gruzd. Is aggression\ncontagious online? a case of swearing on donald\ntrump\u2019s campaign videos on youtube. In Proceed-\nings of the 50th Hawaii International Conference\non System Sciences, 2017.\n[9] Laurens van der Maaten and Geoffrey Hinton. Vi-\nsualizing Data Using t-SNE. Journal of Machine\nLearning Research, 9(Nov):2579\u20132605, 2008.\n[10] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S\nCorrado, and Jeff Dean. Distributed Representa-\ntions of Words and Phrases and their Composition-\nality. In Advances in neural information processing\nsystems, pages 3111\u20133119, 2013.\n[11] Diana C Mutz. Hearing the other side: Deliber-\native versus participatory democracy. Cambridge\nUniversity Press, 2006.\n[12] Diana C Mutz. In-your-face politics: The con-\nsequences of uncivil media. Princeton University\nPress, 2015.\n[13] Jeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. GloVe: Global Vectors for Word\nRepresentation. In Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1532\u20131543,\n2014.\n[14] Reddit CEO Speaks Out On Violentacrez In\nLeaked Memo: \u2018We Stand for Free Speech\u2019.\nHate speech identi\ufb01cation. URL: https://we\nb.archive.org/web/20170710074932/http:\n//gawker.com/5952349/reddit-ceo-speak\ns-out-on-violentacrez-in-leaked-memo-w\ne-stand-for-free-speech (Accessed May 24,\n2017).\n[15] Radim \u02c7Reh\u02dau\u02c7rek and Petr Sojka. Software Frame-\nwork for Topic Modelling with Large Corpora. In\nProceedings of the LREC 2010 Workshop on New\nChallenges for NLP Frameworks, pages 45\u201350,\nValletta, Malta, 2010. ELRA. http://is.muni.\ncz/publication/884893/en.\n[16] Daniel M Shea and Alex Sproveri. The rise and fall\nof nasty politics in america. PS: Political Science\n&amp; Politics, 45(03):416\u2013421, 2012.\n[17] J Cherie Strachan and Michael R Wolf. Politi-\ncal civility. PS: Political Science &amp; Politics,\n45(03):401\u2013404, 2012.\n[18] Michael R Wolf, J Cherie Strachan, and Daniel M\nShea. Incivility and standing \ufb01rm: A second layer\nof partisan division. PS: Political Science &amp;\nPolitics, 45(03):428\u2013434, 2012.\n\nMeasuring Offensive Speech in Online Political Discourse\nRishab Nithyanand1, Brian Schaffner2, Phillipa Gill1\n1{rishab, phillipa}@cs.umass.edu, 2schaffne@polsci.umass.edu\nUniversity of Massachusetts, Amherst\nAbstract\nThe Internet and online forums such as Reddit have be-\ncome an increasingly popular medium for citizens to\nengage in political conversations. However, the on-\nline disinhibition effect resulting from the ability to use\npseudonymous identities may manifest in the form of\noffensive speech, consequently making political discus-\nsions more aggressive and polarizing than they already\nare. Such environments may result in harassment and\nself-censorship from its targets. In this paper, we present\npreliminary results from a large-scale temporal measure-\nment aimed at quantifying offensiveness in online politi-\ncal discussions.\nTo enable our measurements, we develop and evalu-\nate an offensive speech classi\ufb01er. We then use this clas-\nsi\ufb01er to quantify and compare offensiveness in the po-\nlitical and general contexts. We perform our study us-\ning a database of over 168M Reddit comments made by\nover 7M pseudonyms between January 2015 and January\n2017 \u2013 a period covering several divisive political events\nincluding the 2016 US presidential elections.\n1 Introduction\nThe apparent rise in political incivility has attracted sub-\nstantial attention from scholars in recent years. These\nstudies have largely focused on the extent to which politi-\ncians and elected of\ufb01cials are increasingly employing\nrhetoric that appears to violate norms of civility [4, 12].\nFor the purposes of our work, we use the incidence of of-\nfensive rhetoric as a stand in for incivility. The 2016 US\npresidential election was an especially noteworthy case\nin this regard, particularly in terms of Donald Trump\u2019s\ncampaign which frequently violated norms of civility\nboth in how he spoke about broad groups in the pub-\nlic (such as Muslims, Mexicans, and African Americans)\nand the attacks he leveled at his opponents [2]. The con-\nsequences of incivility are thought to be crucial to the\nfunctioning of democracy since \u201cpublic civility and in-\nterpersonal politeness sustain social harmony and allow\npeople who disagree with one another to maintain ongo-\ning relationships\u201d [17].\nWhile political incivility appears to be on the rise\namong elites, it is less clear whether this is true among\nthe mass public as well. Is political discourse particularly\nlacking in civility compared to discourse more generally?\nDoes the incivility of mass political discourse respond to\nthe dynamics of political campaigns? Addressing these\nquestions has been dif\ufb01cult for political scientists be-\ncause traditional tools for studying mass behavior, such\nas public opinion surveys, are ill-equipped to measure\nhow citizens discuss politics with one another. Survey\ndata does reveal that the public tends to perceive politics\nas becoming increasingly less civil during the course of a\npolitical campaign [18]. Yet, it is not clear whether these\nperceptions match the reality, particularly in terms of the\ntypes of discussions that citizens have with each other.\nAn additional question is how incivility is received\nby others. On one hand, violations of norms regard-\ning offensive discourse may be policed by members of\na community, rendering such speech ineffectual. On the\nother hand, offensive speech may be effective as a means\nfor drawing attention to a particular argument. Indeed,\nthere is evidence that increasing incivility in political\nspeech results in higher levels of attention from the pub-\nlic [12]. During the 2016 campaign, the use of swearing\nin comments posted on Donald Trump\u2019s YouTube chan-\nnel tended to result in additional responses that mimicked\nsuch swearing [8]. Thus, offensive speech in online fora\nmay attract more attention from the community and lead\nto the spread of even more offensive speech in subse-\nquent posts.\nTo address these questions regarding political incivil-\nity, we examine the use of offensive speech in politi-\ncal discussions housed on Reddit. Scholars tend to de-\n\ufb01ne uncivil discourse as \u201ccommunication that violates\nthe norms of politeness\u201d [12] a de\ufb01nition that clearly in-\narXiv:1706.01875v2  [cs.CL]  19 Jul 2017",
            1.0094715840000035
          ]
        },
        {
          "answer": "The system's performance, as evaluated on the holdout set, demonstrated an accuracy of 89.6% and an F1-score of 89.2%. This indicates that the classifier was highly effective in distinguishing between offensive and non-offensive comments, showcasing its robustness against over-fitting and its reliability in real-world application scenarios.",
          "relevant_chunks": [
            "Figure 2: Proximity of offensive and non-offensive com-\nments to the hate vector. Dimension reduction is per-\nformed using t-SNE.\nby exploiting the distributional properties of words and\ntheir occurrences in the input corpus.\nRather than using an off-the-shelf word embedding\n(e.g., the GloVe embeddings [13] trained using public do-\nmain data sources such as Wikipedia and news articles),\nwe construct a 100-dimensional embedding using the\ncomplete Reddit dataset (2B comments) as the input cor-\npus. The constructed embedding consists of over 400M\nunique words (words occurring less than 25 times in the\nentire corpus are excluded) using the Word2Vec [10] im-\nplementation provided by the Gensim library [15]. Prior\nto constructing the embedding, we perform stop-word\nremoval and lemmatize each word in the input corpus\nusing the SpaCy NLP framework [5]. The main reason\nfor building a custom embedding is to ensure that our\nembeddings capture semantics speci\ufb01c to the data be-\ning measured (Reddit) \u2013 e.g., while the word \u201ckarma\u201d in\nthe non-Reddit context may be associated with spiritual-\nity, it is associated with points (comment and submission\nscores) on Reddit.\nConstructing a hate vector. We use two lists of words\nassociated with hate [3] and offensive [7] speech to con-\nstruct a hate vector in our word embedding. This is\ndone by mapping each word in the list into the 100-\ndimensional embedding and computing the mean vector.\nThis vector represents the average of all known offen-\nsive words. The main idea behind creating a hate vector\nis to capture the point (in our embedding) to which the\nmost offensive observed comments are likely to be near.\nAlthough clustering our offensive word lists into similar\ngroups and constructing multiple hate vectors \u2013 one for\neach cluster \u2013 results in marginally better accuracy for\nour classi\ufb01er, we use this approach due to the fact that\nour classi\ufb01cation cost grows linearly with the number of\nhate vectors \u2013 i.e., we need to perform O(|S|) distance\ncomputations per hate vector to classify string S.\nTransforming and classifying text. We \ufb01rst remove\nstop-words and perform lemmatization of each word in\nthe text to be classi\ufb01ed. We then obtain the vector repre-\nsenting each word in the text and compute its similarity\nto the constructed hate vector using the cosine similar-\nity metric. A 0-vector is used to represent words in the\ntext that are not present in the embedding. Finally, the\nmaximum cosine similarity score is used to represent the\ncomment. Equation 1 shows the transformation function\non a string S = {s1,..., sn}where si is the vector rep-\nresenting the ith lemmatized non-stop-word, cos is the\ncosine-similarity function, and H is the hate vector.\nT(S) =max\n1\u2264i\u2264n\n[cos(si,H)] (1)\nIn words, the numerical value assigned to a text is the\ncosine similarity between the hate vector and the vec-\ntor representing the word (in the text) closest to the hate\nvector. This approach allows us to transform a string of\ntext into a single numerical value that captures its se-\nmantic similarity to the most offensive comment. We\nuse these scalars as input to a random forest classi\ufb01er to\nperform classi\ufb01cation into Offensive and Not Offensive\nclasses. Figure 2 shows the proximity of Offensive and\nNon Offensive comments to our constructed hate vector\nafter using t-distributed Stochastic Neighbor Embedding\n(t-SNE) [9] to reduce our 100-dimension vector space\ninto 2 dimensions.\n3.2 Classi\ufb01er evaluation\nWe now present results to (1) validate our choice\nof classi\ufb01er and (2) demonstrate the impact of train-\ning/validation sample count on our classi\ufb01ers perfor-\nmance.\nClassi\ufb01er Accuracy (%) F1-Score (%)\nStochastic Gradient Descent 80.7 80.0\nNaive Bayes 81.5 81.2\nDecision Tree 91.8 91.4\nRandom Forest 92.0 91.9\nTable 1: Average classi\ufb01er performance during 10-fold\ncross-validation on the training/validation set. Results\nshown are for the best performing parameters obtained\nusing a grid search.\nClassi\ufb01er selection methodology. To identify the\nmost suitable classi\ufb01er for classifying the scalars as-\nsociated with each text, we perform evaluations us-\ning the stochastic gradient descent, naive bayes, deci-\nsion tree, and random forest classi\ufb01ers. For each clas-\nsi\ufb01er, we split the CrowdFlower hate speech dataset\ninto a training/validation set (75%), and a holdout set\n(25%). We perform 10-fold cross-validation on the train-\ning/validation set to identify the best classi\ufb01er model and\n\n(a) Classi\ufb01er accuracy.\n(b) Classi\ufb01er precision.\n(c) Classi\ufb01er recall.\nFigure 3: Classi\ufb01er performance on holdout sets while\nvarying holdout set sizes and minimum con\ufb01dence\nthresholds.\nparameters (using a grid search). Based on the results of\nthis evaluation, we select a 100-estimator entropy-based\nsplitting random forest model as our classi\ufb01er. Table 1\nshows the mean accuracy and F1-score for each evalu-\nated classi\ufb01er during the 10-fold cross-validation.\nReal-world classi\ufb01er performance. To evaluate real-\nworld performance of our selected classi\ufb01er ( i.e., per-\nformance in the absence of model and parameter bias),\nwe perform classi\ufb01cation of the holdout set. On this set,\nour classi\ufb01er had an accuracy and F1-score of 89.6% and\n89.2%, respectively. These results show that in addition\nto superior accuracy during training and validation, our\nchosen classi\ufb01er is also robust against over-\ufb01tting.\nImpact of dataset quality and size. To understand\nhow the performance of our chosen classi\ufb01er model and\nparameters are impacted by: (1) the quality and consis-\ntency of manually assigned classes in the CrowdFlower\ndataset and (2) the size of the dataset, we re-evaluate the\nclassi\ufb01er while only considering tweets having a mini-\nmum con\ufb01dence score and varying the size of the holdout\nset. Speci\ufb01cally, our experiments considered con\ufb01dence\nthresholds of 0 (all tweets considered), .35 (only tweets\nwhere at least 35% of contributors agreed on a class were\nconsidered), and .70 (only tweets where at least 70% of\nthe contributors agreed on a class were considered) and\nvaried the holdout set sizes between 5% and 95% of all\ntweets meeting the con\ufb01dence threshold set for the ex-\nperiment.\nThe results illustrated in Figure 3 show the perfor-\nmance of the classi\ufb01er while evaluating the correspond-\ning holdout set. We make several conclusions from these\nresults:\n\u2022Beyond a (fairly low) threshold, the size of the train-\ning and validation set has little impact on classi-\n\ufb01er performance. We see that the accuracy, pre-\ncision, and recall have, at best, marginal improve-\nments with holdout set sizes smaller than 60%. This\nimplies that the CrowdFlower dataset is suf\ufb01cient\nfor building an offensive speech classi\ufb01er.\n\u2022Quality of manual labeling has a signi\ufb01cant impact\non the accuracy and precision of the classi\ufb01er. Us-\ning only tweets which had at least 70% of contrib-\nutors agreeing on a class resulted in between 5-7%\nhigher accuracy and up to 5% higher precision.\n\u2022Our classi\ufb01er achieves precision of over 95% and\nrecall of over 85% when considering only high con-\n\ufb01dence samples. This implies that the classi\ufb01er is\nmore likely to underestimate the presence of offen-\nsive speech \u2013 i.e., our results likely provide a lower-\nbound on the quantity of observed offensive speech.\n4 Measurements\nIn this section we quantify and characterize offensive-\nness in the political and general contexts using our offen-\nsive speech classi\ufb01er and the Reddit comments dataset\nwhich considers a random sample of comments made be-\ntween January 2015 and January 2017.\nOffensiveness over time. We \ufb01nd that on average\n8.4% of all political comments are offensive compared to\n7.8% of all apolitical comments. Figure 4 illustrates the\nfraction of offensive political and apolitical comments\nmade during each week in our study. We see that while\nthe fraction of apolitical offensive comments has stayed\nsteady, there has been an increase in the fraction of of-\nfensive political comments starting in July 2016. No-\ntably, this increase is observed after the conclusion of the\nUS presidential primaries and during the period of the\nDemocratic and Republican National Conventions and\ndoes not reduce even after the conclusion of the US presi-\ndential elections held on November 8. Participants in po-\nlitical subreddits were 2.6% more likely to observe offen-\n\nsive comments prior to July 2016 but 14.9% more likely\nto observe offensive comments from July 2016 onwards.\nFigure 4: Fraction of offensive comments identi\ufb01ed in\npolitical and all subreddits.\nReactions to offensive comments. We use the com-\nment score, roughly the difference between up-votes and\ndown-votes received, as a proxy for understanding how\nusers reacted to offensive comments. We \ufb01nd that com-\nments that were offensive: (1) on average, had a higher\nscore than non-offensive comments (average scores: 8.9\nvs. 6.7) and (2) were better received when they were\nposted in the general context than in the political context\n(average scores: 8.6 vs. 9.0). To understand how peo-\nples reactions to offensive comments evolved over time,\nFigure 5 shows the average scores received by offensive\ncomments over time. Again, we observe an increasing\ntrend in average scores received by offensive and politi-\ncal comments after July 2016.\nFigure 5: Average scores of offensive comments identi-\n\ufb01ed in political and all subreddits.\nCharacteristics of offensive authors. We now focus\non understanding characteristics of authors of offensive\ncomments. Speci\ufb01cally, we are interested in identify-\ning the use of throwaway and troll accounts. For the\npurposes of this study, we characterize throwaway ac-\ncounts as those with less than \ufb01ve total comments \u2013 i.e.,\naccounts that are used to make a small number of com-\nments. Similarly, we de\ufb01ne troll accounts as those with\nover 15 comments of which over 75% are classi\ufb01ed as\noffensive \u2013 i.e., accounts that are used to make a larger\nnumber of comments, of which a signi\ufb01cant majority are\noffensive. We \ufb01nd that 93.7% of the accounts which\nhave over 75% of their comments tagged as offensive are\nthrowaways and 1.3% are trolls. Complete results are\nillustrated in Figure 6.\nFigure 6: CDF of the fraction of each authors comments\nthat were identi\ufb01ed as offensive. Green, orange, and red\ndots are used to represent authors with <5, 5-15, and\n>15 total comments, respectively. The legend provides\na breakdown per quartile.\nCharacteristics of offensive communities. We break-\ndown subreddits by their category (default, political, and\nother) and identify the most and least offensive commu-\nnities in each. Figure 7 shows the distribution of the frac-\ntion of offensive comments in each category and Table 2\nshows the most and least offensive subreddits in the po-\nlitical and default categories (we exclude the \u201cother\u201d cat-\negory due to the inappropriateness of their names). We\n\ufb01nd that less than 19% of all subreddits (that account\nfor over 23% of all comments) have over 10% offensive\ncomments. Further, several default and political subred-\ndits fall in this category, including r/the donald \u2013 the\nmost offensive political subreddit and the subreddit ded-\nicated to the US President.\nFigure 7: Distribution of the fraction of offensive com-\nments observed in each subreddit category. Only subred-\ndits with over 1000 comments are considered.\nFlow of offensive authors. Finally, we uncover\npatterns in the movement of offensive authors between\ncommunities. In Figure 8 we show the communities",
            0.5602163750000031
          ]
        },
        {
          "answer": "The database includes political events such as the Democratic and Republican National Conventions and the US presidential elections held on November 8, during the period covered by the study.",
          "relevant_chunks": [
            "In future work, we will extend our analysis of Reddit\nback to 2007 with the aim of formulating a more com-\nplete understanding of the dynamics of political incivil-\nity. For example, we seek to understand whether the\nhigh incidence of offensive speech we \ufb01nd in 2016 is\nunique to this particular campaign or if previous presi-\ndential campaigns witnessed similar spikes in incivility.\nWe will also examine whether there is a more general\nlong-term trend toward offensive online political speech,\nwhich would be consistent with what scholars have found\nwhen studying political elites [6, 16].\nReferences\n[1] CrowdFlower Blog. Hate speech identi\ufb01cation.\nURL: https://www.crowdflower.com/data/h\nate-speech-identification/ (Accessed May\n24, 2017).\n[2] Justin H Gross and Kaylee T Johnson. Twitter\ntaunts and tirades: Negative campaigning in the age\nof trump. PS: Political Science &amp; Politics,\n49(4):748\u2013754, 2016.\n[3] Hatebase. Meet the Hatebase API. URL: https:\n//www.hatebase.org/connect_api (Accessed\nMay 24, 2017).\n[4] Susan Herbst. Rude democracy: Civility and in-\ncivility in American politics. Temple University\nPress, 2010.\n[5] Matthew Honnibal and Mark Johnson. An Im-\nproved Non-monotonic Transition System for De-\npendency Parsing. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1373\u20131378, Lisbon, Portugal,\n2015. Association for Computational Linguistics.\n[6] Kathleen Hall Jamieson and Erika Falk. Continu-\nity and change in civility in the house. In Polar-\nized politics: Congress and the president in a parti-\nsan era, pages 96\u2013108. Washington, DC: CQ Press,\n2000.\n[7] Jamiew. All the dirty words from Google\u2019s \u201cwhat\ndo you love\u201d project. URL: https://gist.g\nithub.com/jamiew/1112488 (Accessed May 24,\n2017).\n[8] K Hazel Kwon and Anatoliy Gruzd. Is aggression\ncontagious online? a case of swearing on donald\ntrump\u2019s campaign videos on youtube. In Proceed-\nings of the 50th Hawaii International Conference\non System Sciences, 2017.\n[9] Laurens van der Maaten and Geoffrey Hinton. Vi-\nsualizing Data Using t-SNE. Journal of Machine\nLearning Research, 9(Nov):2579\u20132605, 2008.\n[10] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S\nCorrado, and Jeff Dean. Distributed Representa-\ntions of Words and Phrases and their Composition-\nality. In Advances in neural information processing\nsystems, pages 3111\u20133119, 2013.\n[11] Diana C Mutz. Hearing the other side: Deliber-\native versus participatory democracy. Cambridge\nUniversity Press, 2006.\n[12] Diana C Mutz. In-your-face politics: The con-\nsequences of uncivil media. Princeton University\nPress, 2015.\n[13] Jeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. GloVe: Global Vectors for Word\nRepresentation. In Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1532\u20131543,\n2014.\n[14] Reddit CEO Speaks Out On Violentacrez In\nLeaked Memo: \u2018We Stand for Free Speech\u2019.\nHate speech identi\ufb01cation. URL: https://we\nb.archive.org/web/20170710074932/http:\n//gawker.com/5952349/reddit-ceo-speak\ns-out-on-violentacrez-in-leaked-memo-w\ne-stand-for-free-speech (Accessed May 24,\n2017).\n[15] Radim \u02c7Reh\u02dau\u02c7rek and Petr Sojka. Software Frame-\nwork for Topic Modelling with Large Corpora. In\nProceedings of the LREC 2010 Workshop on New\nChallenges for NLP Frameworks, pages 45\u201350,\nValletta, Malta, 2010. ELRA. http://is.muni.\ncz/publication/884893/en.\n[16] Daniel M Shea and Alex Sproveri. The rise and fall\nof nasty politics in america. PS: Political Science\n&amp; Politics, 45(03):416\u2013421, 2012.\n[17] J Cherie Strachan and Michael R Wolf. Politi-\ncal civility. PS: Political Science &amp; Politics,\n45(03):401\u2013404, 2012.\n[18] Michael R Wolf, J Cherie Strachan, and Daniel M\nShea. Incivility and standing \ufb01rm: A second layer\nof partisan division. PS: Political Science &amp;\nPolitics, 45(03):428\u2013434, 2012.\n\nCategory Most offensive (%) Least offensive (%)\nDefault r/tifu(15.1%) r/askscience(2.4%)\nr/announcements(13.2%)r/personalfinance(3.4%)\nr/askreddit(11.0%) r/science(3.8%)\nPolitical r/thedonald(11.4%) r/republican(4.4%)\nr/elections(10.2%) r/sandersforpresident(4.9%)\nr/worldpolitics(9.8%) r/tedcruz(5.1%)\nTable 2: Subreddits in the default and political categories\nwith the highest and lowest fraction of offensive com-\nments.\nin which large number of authors of offensive content\non the r/politics subreddit had previously made\noffensive comments (we refer to these communities\nas sources). Unsurprisingly, the most popular sources\nbelonged to the default subreddits ( e.g., r/worldnews,\nr/wtf, r/videos, r/askreddit, and r/news). We\n\ufb01nd that several other political subreddits also serve as\nlarge sources of offensive authors. In fact, the subreddits\ndedicated to the three most popular US presidential can-\ndidates \u2013 r/the donald, r/sandersforpresident,\nand r/hillaryclinton rank in the top three. Fi-\nnally, outside of the default and political subreddits,\nwe \ufb01nd that r/nfl, r/conspiracy, r/dota2,\nr/reactiongifs, r/blackpeopletwitter, and\nr/imgoingtohellforthis were the largest sources of\noffensive political authors.\nFigure 8: Flow of offensive authors. An edge between\ntwo subreddits indicates that authors made offensive\ncomments in the source subreddit before the \ufb01rst time\nthey made offensive comments in the destination subred-\ndit. Darker and thicker edges indicate larger \ufb02ow sizes\n(only \ufb02ows \u2265200 authors are shown).\n5 Conclusions and Future Work\nWe develop and validate an offensive speech classi\ufb01er\nto quantify the presence of offensive online comments\nfrom January 2015 through January 2017. We \ufb01nd that\npolitical discussions on Reddit became increasingly less\ncivil \u2013 as measured by the incidence of offensive com-\nments \u2013 during the 2016 general election campaign. In\nfact, during the height of the campaign, nearly one of ev-\nery 10 comments posted on a political subreddit were\nclassi\ufb01ed as offensive. Offensive comments also re-\nceived more positive feedback from the community, even\nthough most of the accounts responsible for such com-\nments appear to be throwaway accounts. While offensive\nposts were increasingly common on political subreddits\nas the campaign wore on, there was no such increase in\nnon-political fora. This contrast provides additional ev-\nidence that the increasing use of offensive speech was\ndirectly related to the ramping up of the general election\ncampaign for president.\nEven though our study relies on just a single source\nof online political discussions \u2013 Reddit, we believe that\nour \ufb01ndings generally present an upper-bound on the in-\ncidence of offensiveness in online political discussions\nfor the following reasons: First, Reddit allows the use\nof pseudonymous identities that enables the online dis-\ninhibition effect (unlike social-media platforms such as\nFacebook). Second, Reddit enables users to engage in\ncomplex discussions that are unrestricted in length (un-\nlike Twitter). Finally, Reddit is known for enabling a\ngeneral culture of free speech and delegating content reg-\nulation to moderators of individual subreddits. This pro-\nvides users holding fringe views a variety of subreddits\nin which their content is welcome.\nOur \ufb01ndings provide a unique and important map-\nping of the increasing incivility of online political dis-\ncourse during the 2016 campaign. Such an investiga-\ntion is important because scholars have outlined many\nconsequences for incivility in political discourse. Inci-\nvility tends to \u201cturn off\u201d political moderates, leading to\nincreasing polarization among those who are actively en-\ngaged in politics [18]. More importantly, a lack of ci-\nvility in political discussions generally reduces the de-\ngree to which people view opponents as holding legit-\nimate viewpoints. This dynamic makes it dif\ufb01cult for\npeople to \ufb01nd common ground with those who disagree\nwith them [11] and it may ultimately lead citizens to\nview electoral victories by opponents as lacking legiti-\nmacy [12]. Thus, from a normative standpoint, the fact\nthat the 2016 campaign sparked a marked increase in the\noffensiveness of political comments posted to Reddit is\nof concern in its own right; that the incidence of offensive\npolitical comments has remained high even three months\nafter the election is all the more troubling.\n\n(a) Classi\ufb01er accuracy.\n(b) Classi\ufb01er precision.\n(c) Classi\ufb01er recall.\nFigure 3: Classi\ufb01er performance on holdout sets while\nvarying holdout set sizes and minimum con\ufb01dence\nthresholds.\nparameters (using a grid search). Based on the results of\nthis evaluation, we select a 100-estimator entropy-based\nsplitting random forest model as our classi\ufb01er. Table 1\nshows the mean accuracy and F1-score for each evalu-\nated classi\ufb01er during the 10-fold cross-validation.\nReal-world classi\ufb01er performance. To evaluate real-\nworld performance of our selected classi\ufb01er ( i.e., per-\nformance in the absence of model and parameter bias),\nwe perform classi\ufb01cation of the holdout set. On this set,\nour classi\ufb01er had an accuracy and F1-score of 89.6% and\n89.2%, respectively. These results show that in addition\nto superior accuracy during training and validation, our\nchosen classi\ufb01er is also robust against over-\ufb01tting.\nImpact of dataset quality and size. To understand\nhow the performance of our chosen classi\ufb01er model and\nparameters are impacted by: (1) the quality and consis-\ntency of manually assigned classes in the CrowdFlower\ndataset and (2) the size of the dataset, we re-evaluate the\nclassi\ufb01er while only considering tweets having a mini-\nmum con\ufb01dence score and varying the size of the holdout\nset. Speci\ufb01cally, our experiments considered con\ufb01dence\nthresholds of 0 (all tweets considered), .35 (only tweets\nwhere at least 35% of contributors agreed on a class were\nconsidered), and .70 (only tweets where at least 70% of\nthe contributors agreed on a class were considered) and\nvaried the holdout set sizes between 5% and 95% of all\ntweets meeting the con\ufb01dence threshold set for the ex-\nperiment.\nThe results illustrated in Figure 3 show the perfor-\nmance of the classi\ufb01er while evaluating the correspond-\ning holdout set. We make several conclusions from these\nresults:\n\u2022Beyond a (fairly low) threshold, the size of the train-\ning and validation set has little impact on classi-\n\ufb01er performance. We see that the accuracy, pre-\ncision, and recall have, at best, marginal improve-\nments with holdout set sizes smaller than 60%. This\nimplies that the CrowdFlower dataset is suf\ufb01cient\nfor building an offensive speech classi\ufb01er.\n\u2022Quality of manual labeling has a signi\ufb01cant impact\non the accuracy and precision of the classi\ufb01er. Us-\ning only tweets which had at least 70% of contrib-\nutors agreeing on a class resulted in between 5-7%\nhigher accuracy and up to 5% higher precision.\n\u2022Our classi\ufb01er achieves precision of over 95% and\nrecall of over 85% when considering only high con-\n\ufb01dence samples. This implies that the classi\ufb01er is\nmore likely to underestimate the presence of offen-\nsive speech \u2013 i.e., our results likely provide a lower-\nbound on the quantity of observed offensive speech.\n4 Measurements\nIn this section we quantify and characterize offensive-\nness in the political and general contexts using our offen-\nsive speech classi\ufb01er and the Reddit comments dataset\nwhich considers a random sample of comments made be-\ntween January 2015 and January 2017.\nOffensiveness over time. We \ufb01nd that on average\n8.4% of all political comments are offensive compared to\n7.8% of all apolitical comments. Figure 4 illustrates the\nfraction of offensive political and apolitical comments\nmade during each week in our study. We see that while\nthe fraction of apolitical offensive comments has stayed\nsteady, there has been an increase in the fraction of of-\nfensive political comments starting in July 2016. No-\ntably, this increase is observed after the conclusion of the\nUS presidential primaries and during the period of the\nDemocratic and Republican National Conventions and\ndoes not reduce even after the conclusion of the US presi-\ndential elections held on November 8. Participants in po-\nlitical subreddits were 2.6% more likely to observe offen-",
            1.101781041999999
          ]
        },
        {
          "answer": "Random Forest",
          "relevant_chunks": [
            "Figure 2: Proximity of offensive and non-offensive com-\nments to the hate vector. Dimension reduction is per-\nformed using t-SNE.\nby exploiting the distributional properties of words and\ntheir occurrences in the input corpus.\nRather than using an off-the-shelf word embedding\n(e.g., the GloVe embeddings [13] trained using public do-\nmain data sources such as Wikipedia and news articles),\nwe construct a 100-dimensional embedding using the\ncomplete Reddit dataset (2B comments) as the input cor-\npus. The constructed embedding consists of over 400M\nunique words (words occurring less than 25 times in the\nentire corpus are excluded) using the Word2Vec [10] im-\nplementation provided by the Gensim library [15]. Prior\nto constructing the embedding, we perform stop-word\nremoval and lemmatize each word in the input corpus\nusing the SpaCy NLP framework [5]. The main reason\nfor building a custom embedding is to ensure that our\nembeddings capture semantics speci\ufb01c to the data be-\ning measured (Reddit) \u2013 e.g., while the word \u201ckarma\u201d in\nthe non-Reddit context may be associated with spiritual-\nity, it is associated with points (comment and submission\nscores) on Reddit.\nConstructing a hate vector. We use two lists of words\nassociated with hate [3] and offensive [7] speech to con-\nstruct a hate vector in our word embedding. This is\ndone by mapping each word in the list into the 100-\ndimensional embedding and computing the mean vector.\nThis vector represents the average of all known offen-\nsive words. The main idea behind creating a hate vector\nis to capture the point (in our embedding) to which the\nmost offensive observed comments are likely to be near.\nAlthough clustering our offensive word lists into similar\ngroups and constructing multiple hate vectors \u2013 one for\neach cluster \u2013 results in marginally better accuracy for\nour classi\ufb01er, we use this approach due to the fact that\nour classi\ufb01cation cost grows linearly with the number of\nhate vectors \u2013 i.e., we need to perform O(|S|) distance\ncomputations per hate vector to classify string S.\nTransforming and classifying text. We \ufb01rst remove\nstop-words and perform lemmatization of each word in\nthe text to be classi\ufb01ed. We then obtain the vector repre-\nsenting each word in the text and compute its similarity\nto the constructed hate vector using the cosine similar-\nity metric. A 0-vector is used to represent words in the\ntext that are not present in the embedding. Finally, the\nmaximum cosine similarity score is used to represent the\ncomment. Equation 1 shows the transformation function\non a string S = {s1,..., sn}where si is the vector rep-\nresenting the ith lemmatized non-stop-word, cos is the\ncosine-similarity function, and H is the hate vector.\nT(S) =max\n1\u2264i\u2264n\n[cos(si,H)] (1)\nIn words, the numerical value assigned to a text is the\ncosine similarity between the hate vector and the vec-\ntor representing the word (in the text) closest to the hate\nvector. This approach allows us to transform a string of\ntext into a single numerical value that captures its se-\nmantic similarity to the most offensive comment. We\nuse these scalars as input to a random forest classi\ufb01er to\nperform classi\ufb01cation into Offensive and Not Offensive\nclasses. Figure 2 shows the proximity of Offensive and\nNon Offensive comments to our constructed hate vector\nafter using t-distributed Stochastic Neighbor Embedding\n(t-SNE) [9] to reduce our 100-dimension vector space\ninto 2 dimensions.\n3.2 Classi\ufb01er evaluation\nWe now present results to (1) validate our choice\nof classi\ufb01er and (2) demonstrate the impact of train-\ning/validation sample count on our classi\ufb01ers perfor-\nmance.\nClassi\ufb01er Accuracy (%) F1-Score (%)\nStochastic Gradient Descent 80.7 80.0\nNaive Bayes 81.5 81.2\nDecision Tree 91.8 91.4\nRandom Forest 92.0 91.9\nTable 1: Average classi\ufb01er performance during 10-fold\ncross-validation on the training/validation set. Results\nshown are for the best performing parameters obtained\nusing a grid search.\nClassi\ufb01er selection methodology. To identify the\nmost suitable classi\ufb01er for classifying the scalars as-\nsociated with each text, we perform evaluations us-\ning the stochastic gradient descent, naive bayes, deci-\nsion tree, and random forest classi\ufb01ers. For each clas-\nsi\ufb01er, we split the CrowdFlower hate speech dataset\ninto a training/validation set (75%), and a holdout set\n(25%). We perform 10-fold cross-validation on the train-\ning/validation set to identify the best classi\ufb01er model and\n\ncludes offensive remarks. Reddit fora represent a \u201cmost\nlikely\u201d case for the study of offensive political speech\ndue its strong free speech culture [14] and the ability of\nparticipants to use pseudonymous identities. That is, if\npolitical incivility in the public did increase during the\n2016 campaign, this should be especially evident on fora\nsuch as Reddit. Tracking Reddit discussions through-\nout all of 2015 and 2016, we \ufb01nd that online political\ndiscussions became increasingly more offensive as the\ngeneral election campaign intensi\ufb01ed. By comparison,\ndiscussions on non-political subreddits did not become\nincreasingly offensive during this period. Additionally,\nwe \ufb01nd that the presence of offensive comments did not\nsubside even three months after the election.\n2 Datasets\nOur study makes use of multiple datasets in order to iden-\ntify and characterize trends in offensive speech.\nThe CrowdFlower hate speech dataset. The Crowd-\nFlower hate speech dataset [1] contains 14.5K tweets,\neach receiving labels from at least three contributors.\nContributors were allowed to classify each tweet into\none of three classes: Not Offensive (NO), Offensive but\nnot hateful (O), and Offensive and hateful (OH). Of the\n14.5K tweets, only 37.6% had a decisive class \u2013 i.e., the\nsame class was assigned by all contributors. For inde-\ncisive cases, the majority class was selected and a class\ncon\ufb01dence score (fraction of contributors that selected\nthe majority class) was made available. Using this ap-\nproach, 50.4%, 33.1%, and 16.5% of the tweets were cat-\negorized as NO, O, and OH, respectively. Since our goal\nis to identify any offensive speech (not just hate speech),\nwe consolidate assigned classes into Offensive and Not\nOffensive by relabeling OH tweets as Offensive. We use\nthis modi\ufb01ed dataset to train, validate, and test our offen-\nsive speech classi\ufb01er. To the best of our knowledge, this\nis the only dataset that provides offensive and not offen-\nsive annotations to a large dataset.\nOffensive word lists. We also use two offensive\nword lists as auxiliary input to our classi\ufb01er: (1) The\nHatebase hate speech vocabulary [3] consisting of 1122\nhateful words and (2) 422 offensive words banned from\nGoogle\u2019s What Do You Love project [7].\nReddit comments dataset. Finally, after building our\noffensive speech classi\ufb01er using the above datasets, we\nuse it to classify comments made on Reddit. While the\ncomplete Reddit dataset contains 2B comments made be-\ntween the period of January 2015 and January 2017, we\nonly analyze only 168M. We select comments to be ana-\nlyzed using the following process: (1) we exclude com-\nments shorter than 10 characters in length, (2) we ex-\nclude comments made by [deleted] authors, and (3)\nFigure 1: Number of analyzed political and apoliti-\ncal comments belonging to each week between January\n2015 and January 2017.\nwe randomly sample and include 10% of all remaining\ncomments. We categorize comments made in any of 21\npopular political subreddits as political and the remain-\nder as apolitical. Our \ufb01nal dataset contains 129M apo-\nlitical and 39M political comments. Figure 1 shows the\nnumber of comments in our dataset that were made dur-\ning each week included in our study. We see an increas-\ning number of political comments per week starting in\nFebruary 2016 \u2013 the start of the 2016 US presidential\nprimaries.\n3 Offensive Speech Classi\ufb01cation\nIn order to identify offensive speech, we propose a fully\nautomated technique that classi\ufb01es comments into two\nclasses: Offensive and Not Offensive.\n3.1 Classi\ufb01cation approach\nAt a high-level, our approach works as follows:\n\u2022Build a word embedding. We construct a 100-\ndimensional word embedding using all comments\nfrom our complete Reddit dataset (2B comments).\n\u2022Construct a hate vector. We construct a list of of-\nfensive and hateful words identi\ufb01ed from external\ndata and map them into a single vector within the\nhigh-dimensional word embedding.\n\u2022Text transformation and classi\ufb01cation. Finally,\nwe transform text to be classi\ufb01ed into scalars rep-\nresenting their distance from the constructed hate\nvector and use these as input to a Random Forest\nclassi\ufb01er.\nBuilding a word embedding. At a high-level, a word\nembedding maps words into a high-dimensional continu-\nous vector space in such a way that semantic similarities\nbetween words are maintained. This mapping is achieved\n\n(a) Classi\ufb01er accuracy.\n(b) Classi\ufb01er precision.\n(c) Classi\ufb01er recall.\nFigure 3: Classi\ufb01er performance on holdout sets while\nvarying holdout set sizes and minimum con\ufb01dence\nthresholds.\nparameters (using a grid search). Based on the results of\nthis evaluation, we select a 100-estimator entropy-based\nsplitting random forest model as our classi\ufb01er. Table 1\nshows the mean accuracy and F1-score for each evalu-\nated classi\ufb01er during the 10-fold cross-validation.\nReal-world classi\ufb01er performance. To evaluate real-\nworld performance of our selected classi\ufb01er ( i.e., per-\nformance in the absence of model and parameter bias),\nwe perform classi\ufb01cation of the holdout set. On this set,\nour classi\ufb01er had an accuracy and F1-score of 89.6% and\n89.2%, respectively. These results show that in addition\nto superior accuracy during training and validation, our\nchosen classi\ufb01er is also robust against over-\ufb01tting.\nImpact of dataset quality and size. To understand\nhow the performance of our chosen classi\ufb01er model and\nparameters are impacted by: (1) the quality and consis-\ntency of manually assigned classes in the CrowdFlower\ndataset and (2) the size of the dataset, we re-evaluate the\nclassi\ufb01er while only considering tweets having a mini-\nmum con\ufb01dence score and varying the size of the holdout\nset. Speci\ufb01cally, our experiments considered con\ufb01dence\nthresholds of 0 (all tweets considered), .35 (only tweets\nwhere at least 35% of contributors agreed on a class were\nconsidered), and .70 (only tweets where at least 70% of\nthe contributors agreed on a class were considered) and\nvaried the holdout set sizes between 5% and 95% of all\ntweets meeting the con\ufb01dence threshold set for the ex-\nperiment.\nThe results illustrated in Figure 3 show the perfor-\nmance of the classi\ufb01er while evaluating the correspond-\ning holdout set. We make several conclusions from these\nresults:\n\u2022Beyond a (fairly low) threshold, the size of the train-\ning and validation set has little impact on classi-\n\ufb01er performance. We see that the accuracy, pre-\ncision, and recall have, at best, marginal improve-\nments with holdout set sizes smaller than 60%. This\nimplies that the CrowdFlower dataset is suf\ufb01cient\nfor building an offensive speech classi\ufb01er.\n\u2022Quality of manual labeling has a signi\ufb01cant impact\non the accuracy and precision of the classi\ufb01er. Us-\ning only tweets which had at least 70% of contrib-\nutors agreeing on a class resulted in between 5-7%\nhigher accuracy and up to 5% higher precision.\n\u2022Our classi\ufb01er achieves precision of over 95% and\nrecall of over 85% when considering only high con-\n\ufb01dence samples. This implies that the classi\ufb01er is\nmore likely to underestimate the presence of offen-\nsive speech \u2013 i.e., our results likely provide a lower-\nbound on the quantity of observed offensive speech.\n4 Measurements\nIn this section we quantify and characterize offensive-\nness in the political and general contexts using our offen-\nsive speech classi\ufb01er and the Reddit comments dataset\nwhich considers a random sample of comments made be-\ntween January 2015 and January 2017.\nOffensiveness over time. We \ufb01nd that on average\n8.4% of all political comments are offensive compared to\n7.8% of all apolitical comments. Figure 4 illustrates the\nfraction of offensive political and apolitical comments\nmade during each week in our study. We see that while\nthe fraction of apolitical offensive comments has stayed\nsteady, there has been an increase in the fraction of of-\nfensive political comments starting in July 2016. No-\ntably, this increase is observed after the conclusion of the\nUS presidential primaries and during the period of the\nDemocratic and Republican National Conventions and\ndoes not reduce even after the conclusion of the US presi-\ndential elections held on November 8. Participants in po-\nlitical subreddits were 2.6% more likely to observe offen-",
            0.8401997499999965
          ]
        }
      ],
      "reference_answers": [
        "stochastic gradient descent, naive bayes, decision tree",
        "accuracy and F1-score of 89.6% and 89.2%, respectively accuracy and F1-score of 89.6% and 89.2%, respectively",
        "US presidential primaries, Democratic and Republican National Conventions",
        "Random Forest"
      ]
    },
    "1801.02073.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.023578516141812726,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.39215686274509803,
          "recall": 0.19791666666666666,
          "f1": 0.26303854875283444
        },
        "rouge-2": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        },
        "rouge-L": {
          "precision": 0.39215686274509803,
          "recall": 0.19791666666666666,
          "f1": 0.26303854875283444
        }
      },
      "per_question_metrics": [
        {
          "question": "Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?",
          "generated_answer": "Yes",
          "reference_answer": "Yes No",
          "rouge_scores": {
            "rouge-1": {
              "precision": 1.0,
              "recall": 0.5,
              "f1": 0.6666666666666666
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 1.0,
              "recall": 0.5,
              "f1": 0.6666666666666666
            }
          },
          "bleu_score": 0.06541924356118012,
          "exact_match": 0.0
        },
        {
          "question": "How many question types do they find in the datasets analyzed?",
          "generated_answer": "The context does not specify the number of question types found in the datasets analyzed.",
          "reference_answer": "seven  7",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            }
          },
          "bleu_score": 0,
          "exact_match": 0.0
        },
        {
          "question": "How do they analyze contextual similaries across datasets?",
          "generated_answer": "The text does not provide specific details on the method used to analyze contextual similarities across datasets.",
          "reference_answer": "They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap.",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.17647058823529413,
              "recall": 0.09375,
              "f1": 0.12244897959183672
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.17647058823529413,
              "recall": 0.09375,
              "f1": 0.12244897959183672
            }
          },
          "bleu_score": 0.00531630486425806,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?",
        "How many question types do they find in the datasets analyzed?",
        "How do they analyze contextual similaries across datasets?"
      ],
      "generated_answers": [
        {
          "answer": "Yes",
          "relevant_chunks": [
            "WIKI QA SELQA SQUAD\n(\u03c1,\u03b3c,\u03b3p),t \u2265 0.3 ( 92 .00, 1,203, 96.86) (90.00, 7,446, 94.28) (100.00, 93,928, 95.61)\n(\u03c1,\u03b3c,\u03b3p),t \u2265 0.4 ( 94.00, 1,139, 91.71) (94.00, 7,133, 90.31) (100.00, 93,928, 95.61)\n(\u03c1,\u03b3c,\u03b3p),t \u2265 0.5 (100.00, 1,051, 84.62) (98.00, 6,870, 86.98) (100.00, 93,928, 95.61)\nk= (1,5,10,20) (4.39,12.47,16.59,22.39) (20.01,34.07,40.29,46.40) (19.90,35.08,40.96,46.74)\nTable 2: Statistics of the silver-standard dataset (\ufb01rst three rows) and the accuracies of answer retrieval in % (last row).\n\u03c1: robustness of the silver-standard in %, \u03b3c/p: #/% of retrieved silver-standard passages (coverage).\nalthough WIKI QA comes very close.\nFig. 1 shows the distributions of seven question\ntypes grouped deterministically from the lexicons.\nAlthough these corpora have been independently\ndeveloped, a general trend is found, where thewhat\nquestion type dominates, followed byhow and who,\nfollowed by when and where, and so on.\n0\n15\n30\n45\n60\nWhat How Who When Where Which Why\nWikiQA SelQA SQuAD InfoboxQA\nFigure 1: Distributions of question types in %.\nFig. 2 shows the distributions of answer categories\nautomatically classi\ufb01ed by our Convolutional Neu-\nral Network model trained on the data distributed\nby Li and Roth (2002).2 Interestingly, each corpus\nfocuses on different categories, Numeric for WIK-\nIQA and SELQA, Entity for SQUAD, and Person\nfor INFOBOX QA, which gives enough diversities\nfor statistical learning to build robust models.\nNumeric\nPerson\nEntity\nDescription\nLocation\nAbbreviation\n0 10 20 30 40\nWikiQA SelQA SQuAD InfoboxQA\nFigure 2: Distributions of answer categories in %.\n2Our CNN model shows 95.20% accuracy on their test set.\n3 Answer Retrieval\nThis section describes another selection-based QA\ntask, called answer retrieval, that \ufb01nds the answer\ncontext from a larger dataset, the entire Wikipedia.\nSQUAD provides no mapping of the answer con-\ntexts to Wikipedia, whereas WIKI QA and SELQA\nprovide mappings; however, their data do not come\nfrom the same version of Wikipedia. We propose\nan automatic way of mapping the answer contexts\nfrom all corpora to the same version of Wikipeda3\nso they can be coherently used for answer retrieval.\nEach paragraph in Wikipedia is \ufb01rst indexed by\nLucene using {1,2,3}-grams, where the paragraphs\nare separated by WikiExtractor4 and segmented by\nNLP4J5 (28.7M+ paragraphs are indexed). Each\nanswer sentence from the corpora in Table 2 is then\nqueried to Lucene, and the top-5 ranked paragraphs\nare retrieved. The cosine similarity between each\nsentence in these paragraphs and the answer sen-\ntence is measured forn-grams, say n1,2,3. A weight\nis assigned to each n-gram score, say \u03bb1,2,3, and\nthe weighted sum is measured: t = \u22113\ni=1 \u03bbi \u00b7ni.\nThe \ufb01xed weights of \u03bb1,2,3 = (0.25,0.35,0.4) are\nused for our experiments, which can be improved.\nIf there exists a sentence whose t\u2265\u03b8, the para-\ngraph consisting of that sentence is considered the\nsilver-standard answer passage. Table 2 shows how\nrobust these silver-standard passages are based on\nhuman judgement (\u03c1) and how many passages are\ncollected (\u03b3) for \u03b8 = [0.3,0.5], where the human\njudgement is performed on 50 random samples for\neach case. For answer retrieval, a dataset is created\nby \u03b8 = 0.4, which gives \u03c1 \u226594% accuracy and\n\u03b3p > 90% coverage, respectively.6 Finally, each\nquestion is queried to Lucene and the top-kpara-\ngraphs are retrieved from the entire Wikipedia. If\nthe answer sentence exists within those retrieved\nparagraphs according to the silver-standard, it is\nconsidered correct.\n3enwiki-20160820-pages-articles.xml.bz2\n4github.com/attardi/wikiextractor\n5github.com/emorynlp/nlp4j\n6SQUAD mapping was easier than the others because it was\nbased on a more recent version of Wikipedia.\n\nAnalysis of Wikipedia-based Corpora for Question Answering\nTomasz Jurczyk1, Amit Deshmane2, Jinho D. Choi1\n1Mathematics and Computer Science, Emory University\n2Infosys Ltd.\n{tomasz.jurczyk,jinho.choi}@emory.edu\namit deshmane@infosys.com\nAbstract\nThis paper gives comprehensive analyses\nof corpora based on Wikipedia for several\ntasks in question answering. Four recent\ncorpora are collected, WIKI QA, SELQA,\nSQUAD, and INFOBOX QA, and \ufb01rst an-\nalyzed intrinsically by contextual similari-\nties, question types, and answer categories.\nThese corpora are then analyzed extrinsi-\ncally by three question answering tasks, an-\nswer retrieval, selection, and triggering. An\nindexing-based method for the creation of a\nsilver-standard dataset for answer retrieval\nusing the entire Wikipedia is also presented.\nOur analysis shows the uniqueness of these\ncorpora and suggests a better use of them\nfor statistical question answering learning.\n1 Introduction\nQuestion answering (QA) has been a blooming re-\nsearch \ufb01eld for the last decade. Selection-based QA\nimplies a family of tasks that \ufb01nd answer contexts\nfrom large data given questions in natural language.\nThree tasks have been proposed for selection-based\nQA. Given a document, answer extraction (Shen\nand Klakow, 2006; Sultan et al., 2016) \ufb01nds answer\nphrases whereas answer selection (Wang et al.,\n2007; Yih et al., 2013; Yu et al., 2014; Wang et al.,\n2016) and answer triggering (Yang et al., 2015;\nJurczyk et al., 2016) \ufb01nd answer sentences instead,\nalthough the presence of the answer context is not\nassumed within the provided document for answer\ntriggering but it is for the other two tasks. Recently,\nvarious QA tasks that are not selection-based have\nbeen proposed (Reddy and Bandyopadhyay, 2006;\nHosseini et al., 2014; Jauhar et al., 2016; Sachan\net al., 2016); however, selection-based QA remains\nstill important because of its practical value to real\napplications (e.g., IBM Watson, MIT START).\nSeveral datasets have been released for selection-\nbased QA. Wang et al. (2007) created theQAS ENT\ndataset consisting of 277 questions, which has been\nwidely used for benchmarking the answer selection\ntask. Feng et al. (2015) presented INSURANCE QA\ncomprising 16K+ questions on insurance contexts.\nYang et al. (2015) introduced WIKI QA for answer\nselection and triggering. Jurczyk et al. (2016) cre-\nated SELQA for large real-scale answer triggering.\nRajpurkar et al. (2016) presented SQUAD for an-\nswer extraction and selection as well as for reading\ncomprehension. Finally, Morales et al. (2016) pro-\nvided INFOBOX QA for answer selection.\nThese corpora make it possible to evaluate the ro-\nbustness of statistical question answering learning.\nAlthough all of these corpora target on selection-\nbased QA, they are designed for different purposes\nsuch that it is important to understand the nature of\nthese corpora so a better use of them can be made.\nIn this paper, we make both intrinsic and extrinsic\nanalyses of four latest corpora based on Wikipedia,\nWIKI QA, SELQA, SQUAD, and INFOBOX QA.\nWe \ufb01rst give a thorough intrinsic analysis regarding\ncontextual similarities, question types, and answer\ncategories (Section 2). We then map questions in all\ncorpora to the current version of English Wikipedia\nand benchmark another selection-based QA task,\nanswer retrieval (Section 3). Finally, we present\nan extrinsic analysis through a set of experiments\ncross-testing these corpora using a convolutional\nneural network architecture (Section 4).1\n2 Intrinsic Analysis\nFour publicly available corpora are selected for our\nanalysis. These corpora are based on Wikipedia, so\nmore comparable than the others, and have already\nbeen used for the evaluation of several QA systems.\n1All our resources are publicly available\n: anonymous_url\narXiv:1801.02073v2  [cs.CL]  5 Feb 2018\n\nWIKIQA SELQA SQUAD INFOBOX QA\nSource Bing search queries Crowdsourced Crowdsourced Crowdsourced\nYear 2015 2016 2016 2016\n(AE, AS, AT) (O, O, O) (X, O, O) (O, O, X) (X, O, X)\n(q,c,c/q) (1,242, 12,153, 9.79) (7,904, 95,250, 12.05) (98,202, 496,167, 5.05) (15,271, 271,038, 17.75)\n(w,t) (386,440, 30,191) (3,469,015, 44,099) (19,445,863, 115,092) (5,034,625, 8,323)\n(\u00b5q,\u00b5c) (6.44, 25.36) (11.11, 25.31) (11.33, 27.86) (9.35, 9.22)\n(\u2126q,\u2126a,\u2126f) (46.72, 11.05, 16.96) (32.79, 16.98, 20.19) (32.27, 12.15, 16.54) (26.80, 35.70, 28.09)\nTable 1: Comparisons between the four corpora for answer selection. Note that both WIKI QA and SELQA provide separate\nannotation for answer triggering, which is not shown in this table. The SQUAD column shows statistics excluding the evaluation\nset, which is not publicly available. AE/AS/AT: annotation for answer extraction/selection/triggering, q/c: # of questions/answer\ncandidates, w/t: # of tokens/token types, \u00b5q/c: average length of questions/answer candidates, \u2126q/a: macro average in % of\noverlapping words between question-answer pairs normalized by the questions/answers lengths, \u2126f : (2\u00b7\u2126q\u00b7\u2126a)/(\u2126q+\u2126a).\nWIKI QA (Yang et al., 2015) comprises questions\nselected from the Bing search queries, where user\nclick data give the questions and their correspond-\ning Wikipedia articles. The abstracts of these arti-\ncles are then extracted to create answer candidates.\nThe assumption is made that if many queries lead to\nthe same article, it must contain the answer context;\nhowever, this assumption fails for some occasions,\nwhich makes this dataset more challenging. Since\nthe existence of answer contexts is not guaranteed\nin this task, it is called answer triggering instead of\nanswer selection.\nSELQA (Jurczyk et al., 2016) is a product of \ufb01ve\nannotation tasks through crowdsourcing. It consists\nof about 8K questions where a half of the questions\nare paraphrased from the other half, aiming to re-\nduce contextual similarities between questions and\nanswers. Each question is associated with a section\nin Wikipedia where the answer context is guaran-\nteed, and also with \ufb01ve sections selected from the\nentire Wikipedia where the selection is made by\nthe Lucene search engine. This second dataset does\nnot assume the existence of the answer context, so\ncan be used for the evaluation of answer triggering.\nSQUAD (Rajpurkar et al., 2016) presents 107K+\ncrowdsourced questions on 536 Wikipedia articles,\nwhere the answer contexts are guaranteed to exist\nwithin the provided paragraph. It contains annota-\ntion of answer phrases as well as the pointers to the\nsentences including the answer phrases; thus, it can\nbe used for both answer extraction and selection.\nThis corpus also provides human accuracy on those\nquestions, setting up a reasonable upper bound for\nmachines. To avoid over\ufb01tting, the evaluation set\nis not publicly available although system outputs\ncan be evaluated by their provided script.\nINFOBOX QA (Morales et al., 2016) gives 15K+\nquestions based on the infoboxes from 150 articles\nin Wikipedia. Each question is crowdsourced and\nassociated with an infobox, where each line of the\ninfobox is considered an answer candidate. This\ncorpus emphasizes the gravity of infoboxes, which\nsummary arguably the most commonly asked infor-\nmation about those articles. Although the nature of\nthis corpus is different from the others, it can also\nbe used to evaluate answer selection.\nAnalysis\nAll corpora provide datasets/splits for answer se-\nlection, whereas only ( WIKI QA, SQUAD) and\n(WIKI QA, SELQA) provide datasets for answer\nextraction and answer triggering, respectively.\nSQUAD is much larger in size although ques-\ntions in this corpus are often paraphrased multi-\nple times. On the contrary, SQUAD\u2019s average\ncandidates per question ( c/q) is the smallest be-\ncause SQUAD extracts answer candidates from\nparagraphs whereas the others extract them from\nsections or infoboxes that consist of bigger con-\ntexts. Although INFOBOX QA is larger than WIK-\nIQA or SELQA, the number of token types (t) in\nINFOBOX QA is smaller than those two, due to the\nrepetitive nature of infoboxes.\nAll corpora show similar average answer candi-\ndate lengths (\u00b5c), except for INFOBOX QA where\neach line in the infobox is considered a candidate.\nSELQA and SQUAD show similar average ques-\ntion lengths (\u00b5q) because of the similarity between\ntheir annotation schemes. It is not surprising that\nWIKI QA\u2019s average question length is the smallest,\nconsidering their questions are taken from search\nqueries. INFOBOX QA\u2019s average question length is\nrelatively small, due to the restricted information\nthat can be asked from the infoboxes.INFOBOX QA\nand WIKI QA show the least question-answer word\noverlaps over questions and answers (\u2126q and \u2126a in\nTable 1), respectively. In terms of the F1-score for\noverlapping words (\u2126f ), SQUAD gives the least\nportion of overlaps between question-answer pairs",
            0.4927920840000013
          ]
        },
        {
          "answer": "The context does not specify the number of question types found in the datasets analyzed.",
          "relevant_chunks": [
            "WIKIQA SELQA SQUAD INFOBOX QA\nSource Bing search queries Crowdsourced Crowdsourced Crowdsourced\nYear 2015 2016 2016 2016\n(AE, AS, AT) (O, O, O) (X, O, O) (O, O, X) (X, O, X)\n(q,c,c/q) (1,242, 12,153, 9.79) (7,904, 95,250, 12.05) (98,202, 496,167, 5.05) (15,271, 271,038, 17.75)\n(w,t) (386,440, 30,191) (3,469,015, 44,099) (19,445,863, 115,092) (5,034,625, 8,323)\n(\u00b5q,\u00b5c) (6.44, 25.36) (11.11, 25.31) (11.33, 27.86) (9.35, 9.22)\n(\u2126q,\u2126a,\u2126f) (46.72, 11.05, 16.96) (32.79, 16.98, 20.19) (32.27, 12.15, 16.54) (26.80, 35.70, 28.09)\nTable 1: Comparisons between the four corpora for answer selection. Note that both WIKI QA and SELQA provide separate\nannotation for answer triggering, which is not shown in this table. The SQUAD column shows statistics excluding the evaluation\nset, which is not publicly available. AE/AS/AT: annotation for answer extraction/selection/triggering, q/c: # of questions/answer\ncandidates, w/t: # of tokens/token types, \u00b5q/c: average length of questions/answer candidates, \u2126q/a: macro average in % of\noverlapping words between question-answer pairs normalized by the questions/answers lengths, \u2126f : (2\u00b7\u2126q\u00b7\u2126a)/(\u2126q+\u2126a).\nWIKI QA (Yang et al., 2015) comprises questions\nselected from the Bing search queries, where user\nclick data give the questions and their correspond-\ning Wikipedia articles. The abstracts of these arti-\ncles are then extracted to create answer candidates.\nThe assumption is made that if many queries lead to\nthe same article, it must contain the answer context;\nhowever, this assumption fails for some occasions,\nwhich makes this dataset more challenging. Since\nthe existence of answer contexts is not guaranteed\nin this task, it is called answer triggering instead of\nanswer selection.\nSELQA (Jurczyk et al., 2016) is a product of \ufb01ve\nannotation tasks through crowdsourcing. It consists\nof about 8K questions where a half of the questions\nare paraphrased from the other half, aiming to re-\nduce contextual similarities between questions and\nanswers. Each question is associated with a section\nin Wikipedia where the answer context is guaran-\nteed, and also with \ufb01ve sections selected from the\nentire Wikipedia where the selection is made by\nthe Lucene search engine. This second dataset does\nnot assume the existence of the answer context, so\ncan be used for the evaluation of answer triggering.\nSQUAD (Rajpurkar et al., 2016) presents 107K+\ncrowdsourced questions on 536 Wikipedia articles,\nwhere the answer contexts are guaranteed to exist\nwithin the provided paragraph. It contains annota-\ntion of answer phrases as well as the pointers to the\nsentences including the answer phrases; thus, it can\nbe used for both answer extraction and selection.\nThis corpus also provides human accuracy on those\nquestions, setting up a reasonable upper bound for\nmachines. To avoid over\ufb01tting, the evaluation set\nis not publicly available although system outputs\ncan be evaluated by their provided script.\nINFOBOX QA (Morales et al., 2016) gives 15K+\nquestions based on the infoboxes from 150 articles\nin Wikipedia. Each question is crowdsourced and\nassociated with an infobox, where each line of the\ninfobox is considered an answer candidate. This\ncorpus emphasizes the gravity of infoboxes, which\nsummary arguably the most commonly asked infor-\nmation about those articles. Although the nature of\nthis corpus is different from the others, it can also\nbe used to evaluate answer selection.\nAnalysis\nAll corpora provide datasets/splits for answer se-\nlection, whereas only ( WIKI QA, SQUAD) and\n(WIKI QA, SELQA) provide datasets for answer\nextraction and answer triggering, respectively.\nSQUAD is much larger in size although ques-\ntions in this corpus are often paraphrased multi-\nple times. On the contrary, SQUAD\u2019s average\ncandidates per question ( c/q) is the smallest be-\ncause SQUAD extracts answer candidates from\nparagraphs whereas the others extract them from\nsections or infoboxes that consist of bigger con-\ntexts. Although INFOBOX QA is larger than WIK-\nIQA or SELQA, the number of token types (t) in\nINFOBOX QA is smaller than those two, due to the\nrepetitive nature of infoboxes.\nAll corpora show similar average answer candi-\ndate lengths (\u00b5c), except for INFOBOX QA where\neach line in the infobox is considered a candidate.\nSELQA and SQUAD show similar average ques-\ntion lengths (\u00b5q) because of the similarity between\ntheir annotation schemes. It is not surprising that\nWIKI QA\u2019s average question length is the smallest,\nconsidering their questions are taken from search\nqueries. INFOBOX QA\u2019s average question length is\nrelatively small, due to the restricted information\nthat can be asked from the infoboxes.INFOBOX QA\nand WIKI QA show the least question-answer word\noverlaps over questions and answers (\u2126q and \u2126a in\nTable 1), respectively. In terms of the F1-score for\noverlapping words (\u2126f ), SQUAD gives the least\nportion of overlaps between question-answer pairs\n\nTrained on\nEvaluated on\nWIKI QA SELQA SQUAD INFOBOX QA\nMAP MRR F1 MAP MRR F1 MAP MRR F1 MAP MRR F1\nWIKI QA 65.54 67.41 13.33 53.47 54.12 8.68 73.16 73.72 11.26 30.85 30.85 -\nSELQA 49.05 49.64 24.30 82.72 83.70 48.66 77.22 78.04 44.70 63.13 63.13 -\nSQUAD 58.17 58.53 19.35 81.15 82.27 42.88 88.84 89.69 44.93 63.24 63.24 -\nINFOBOX QA 45.17 45.43 - 53.48 54.25 - 65.27 65.90 - 79.44 79.44 -\nW+S+Q 56.40 56.51 - 83.19 84.25 - 88.78 89.65 - 62.53 62.53 -\nW+S+Q+I 60.19 60.68 - 82.88 83.97 - 88.92 89.79 - 70.81 70.81 -\nTable 3: Results for answer selection and triggering in % trained and evaluated across all corpora splits. The \ufb01rst column shows\nthe training source, and the other columns show the evaluation sources. W: WIKI QA, S: SELQA, Q: SQUAD, I: INFOBOX QA.\n4 Extrinsic Analysis\n4.1 Answer Selection\nAnswer selection is evaluated by two metrics, mean\naverage precision (MAP) and mean reciprocal rank\n(MRR). The bigram CNN introduced by Yu et al.\n(2014) is used to generate all the results in Table 3,\nwhere models are trained on either single or com-\nbined datasets. Clearly, the questions in WIKI QA\nare the most challenging, and adding more train-\ning data from the other corpora hurts accuracy due\nto the uniqueness of query-based questions in this\ncorpus. The best model is achieved by training on\nW+S+Q for SELQA; adding INFOBOX QA hurts\naccuracy for SELQA although it gives a marginal\ngain for SQUAD. Just like WIKI QA, INFOBOX QA\nperforms the best when it is trained on only itself.\nFrom our analysis, we suggest that to use models\ntrained on WIKI QA and INFOBOX QA for short\nquery-like questions, whereas to use ones trained\non SELQA and SQUAD for long natural questions.\n4.2 Answer Retrieval\nFinding a paragraph that includes the answer con-\ntext out of the entire Wikipedia is an extremely\ndif\ufb01cult task ( 1/28.7M). The last row of Table 2\nshows results from answer retrieval. Given k= 5,\nSELQA and SQUAD show about 34% and 35% ac-\ncuracy, which are reasonable. However, WIKI QA\nshows a signi\ufb01cantly lower accuracy of 12.47%;\nthis is because the questions in WIKI QA is about\ntwice shorter than the questions in the other cor-\npora such that not enough lexicons can be extracted\nfrom these questions for the Lucene search.\n4.3 Answer Triggering\nThe results of k= 5 from the answer retrieval task\nin Section 4.2 are used to create the datasets for an-\nswer triggering, where about 65% of the questions\nare not expected to \ufb01nd their answer contexts from\nthe provided paragraphs for SELQA and SQUAD\nand 87.5% are not expected for WIKI QA. Answer\ntriggering is evaluated by the F1 scores as presented\nin Table 3, where three corpora are cross validated.\nThe results on WIKI QA are pretty low as expected\nfrom the poor accuracy on the answer retrieval task.\nTraining on SELQA gives the best models for both\nWIKI QA and SELQA. Training on SQUAD gives\nthe best model for SQUAD although the model\ntrained on SELQA is comparable. Since the answer\ntriggering datasets are about 5 times larger than the\nanswer selection datasets, it is computationally too\nexpensive to combine all data for training. We plan\nto \ufb01nd a strong machine to perform this experiment\nin near future.\n5 Related work\nLately, several deep learning approaches have been\nproposed for question answering. Yu et al. (2014)\npresented a CNN model that recognizes the seman-\ntic similarity between two sentences. Wang and\nNyberg (2015) presented a stacked bidirectional\nLSTM approach to read words in sequence, then\noutputs their similarity scores. Feng et al. (2015)\napplied a general deep learning framework to non-\nfactoid question answering. Santos et al. (2016)\nintroduced an attentive pooling mechanism that led\nto further improvements in selection-based QA.\n6 Conclusion\nWe present a comprehensive comparison study of\nthe existing corpora for selection-based question\nanswering. Our intrinsic analysis provides a better\nunderstanding of the uniqueness or similarity be-\ntween these corpora. Our extrinsic analysis shows\nthe strength or weakness of combining these cor-\npora together for statistical learning. Additionally,\nwe create a silver-standard dataset for answer re-\ntrieval and triggering, which will be publicly avail-\nable. In the future, we will explore different ways\nof improving the quality of our silver-standard\ndatasets by \ufb01ne-tuning the hyper-parameters.\n\nAnalysis of Wikipedia-based Corpora for Question Answering\nTomasz Jurczyk1, Amit Deshmane2, Jinho D. Choi1\n1Mathematics and Computer Science, Emory University\n2Infosys Ltd.\n{tomasz.jurczyk,jinho.choi}@emory.edu\namit deshmane@infosys.com\nAbstract\nThis paper gives comprehensive analyses\nof corpora based on Wikipedia for several\ntasks in question answering. Four recent\ncorpora are collected, WIKI QA, SELQA,\nSQUAD, and INFOBOX QA, and \ufb01rst an-\nalyzed intrinsically by contextual similari-\nties, question types, and answer categories.\nThese corpora are then analyzed extrinsi-\ncally by three question answering tasks, an-\nswer retrieval, selection, and triggering. An\nindexing-based method for the creation of a\nsilver-standard dataset for answer retrieval\nusing the entire Wikipedia is also presented.\nOur analysis shows the uniqueness of these\ncorpora and suggests a better use of them\nfor statistical question answering learning.\n1 Introduction\nQuestion answering (QA) has been a blooming re-\nsearch \ufb01eld for the last decade. Selection-based QA\nimplies a family of tasks that \ufb01nd answer contexts\nfrom large data given questions in natural language.\nThree tasks have been proposed for selection-based\nQA. Given a document, answer extraction (Shen\nand Klakow, 2006; Sultan et al., 2016) \ufb01nds answer\nphrases whereas answer selection (Wang et al.,\n2007; Yih et al., 2013; Yu et al., 2014; Wang et al.,\n2016) and answer triggering (Yang et al., 2015;\nJurczyk et al., 2016) \ufb01nd answer sentences instead,\nalthough the presence of the answer context is not\nassumed within the provided document for answer\ntriggering but it is for the other two tasks. Recently,\nvarious QA tasks that are not selection-based have\nbeen proposed (Reddy and Bandyopadhyay, 2006;\nHosseini et al., 2014; Jauhar et al., 2016; Sachan\net al., 2016); however, selection-based QA remains\nstill important because of its practical value to real\napplications (e.g., IBM Watson, MIT START).\nSeveral datasets have been released for selection-\nbased QA. Wang et al. (2007) created theQAS ENT\ndataset consisting of 277 questions, which has been\nwidely used for benchmarking the answer selection\ntask. Feng et al. (2015) presented INSURANCE QA\ncomprising 16K+ questions on insurance contexts.\nYang et al. (2015) introduced WIKI QA for answer\nselection and triggering. Jurczyk et al. (2016) cre-\nated SELQA for large real-scale answer triggering.\nRajpurkar et al. (2016) presented SQUAD for an-\nswer extraction and selection as well as for reading\ncomprehension. Finally, Morales et al. (2016) pro-\nvided INFOBOX QA for answer selection.\nThese corpora make it possible to evaluate the ro-\nbustness of statistical question answering learning.\nAlthough all of these corpora target on selection-\nbased QA, they are designed for different purposes\nsuch that it is important to understand the nature of\nthese corpora so a better use of them can be made.\nIn this paper, we make both intrinsic and extrinsic\nanalyses of four latest corpora based on Wikipedia,\nWIKI QA, SELQA, SQUAD, and INFOBOX QA.\nWe \ufb01rst give a thorough intrinsic analysis regarding\ncontextual similarities, question types, and answer\ncategories (Section 2). We then map questions in all\ncorpora to the current version of English Wikipedia\nand benchmark another selection-based QA task,\nanswer retrieval (Section 3). Finally, we present\nan extrinsic analysis through a set of experiments\ncross-testing these corpora using a convolutional\nneural network architecture (Section 4).1\n2 Intrinsic Analysis\nFour publicly available corpora are selected for our\nanalysis. These corpora are based on Wikipedia, so\nmore comparable than the others, and have already\nbeen used for the evaluation of several QA systems.\n1All our resources are publicly available\n: anonymous_url\narXiv:1801.02073v2  [cs.CL]  5 Feb 2018",
            0.5442206250000012
          ]
        },
        {
          "answer": "The text does not provide specific details on the method used to analyze contextual similarities across datasets.",
          "relevant_chunks": [
            "Trained on\nEvaluated on\nWIKI QA SELQA SQUAD INFOBOX QA\nMAP MRR F1 MAP MRR F1 MAP MRR F1 MAP MRR F1\nWIKI QA 65.54 67.41 13.33 53.47 54.12 8.68 73.16 73.72 11.26 30.85 30.85 -\nSELQA 49.05 49.64 24.30 82.72 83.70 48.66 77.22 78.04 44.70 63.13 63.13 -\nSQUAD 58.17 58.53 19.35 81.15 82.27 42.88 88.84 89.69 44.93 63.24 63.24 -\nINFOBOX QA 45.17 45.43 - 53.48 54.25 - 65.27 65.90 - 79.44 79.44 -\nW+S+Q 56.40 56.51 - 83.19 84.25 - 88.78 89.65 - 62.53 62.53 -\nW+S+Q+I 60.19 60.68 - 82.88 83.97 - 88.92 89.79 - 70.81 70.81 -\nTable 3: Results for answer selection and triggering in % trained and evaluated across all corpora splits. The \ufb01rst column shows\nthe training source, and the other columns show the evaluation sources. W: WIKI QA, S: SELQA, Q: SQUAD, I: INFOBOX QA.\n4 Extrinsic Analysis\n4.1 Answer Selection\nAnswer selection is evaluated by two metrics, mean\naverage precision (MAP) and mean reciprocal rank\n(MRR). The bigram CNN introduced by Yu et al.\n(2014) is used to generate all the results in Table 3,\nwhere models are trained on either single or com-\nbined datasets. Clearly, the questions in WIKI QA\nare the most challenging, and adding more train-\ning data from the other corpora hurts accuracy due\nto the uniqueness of query-based questions in this\ncorpus. The best model is achieved by training on\nW+S+Q for SELQA; adding INFOBOX QA hurts\naccuracy for SELQA although it gives a marginal\ngain for SQUAD. Just like WIKI QA, INFOBOX QA\nperforms the best when it is trained on only itself.\nFrom our analysis, we suggest that to use models\ntrained on WIKI QA and INFOBOX QA for short\nquery-like questions, whereas to use ones trained\non SELQA and SQUAD for long natural questions.\n4.2 Answer Retrieval\nFinding a paragraph that includes the answer con-\ntext out of the entire Wikipedia is an extremely\ndif\ufb01cult task ( 1/28.7M). The last row of Table 2\nshows results from answer retrieval. Given k= 5,\nSELQA and SQUAD show about 34% and 35% ac-\ncuracy, which are reasonable. However, WIKI QA\nshows a signi\ufb01cantly lower accuracy of 12.47%;\nthis is because the questions in WIKI QA is about\ntwice shorter than the questions in the other cor-\npora such that not enough lexicons can be extracted\nfrom these questions for the Lucene search.\n4.3 Answer Triggering\nThe results of k= 5 from the answer retrieval task\nin Section 4.2 are used to create the datasets for an-\nswer triggering, where about 65% of the questions\nare not expected to \ufb01nd their answer contexts from\nthe provided paragraphs for SELQA and SQUAD\nand 87.5% are not expected for WIKI QA. Answer\ntriggering is evaluated by the F1 scores as presented\nin Table 3, where three corpora are cross validated.\nThe results on WIKI QA are pretty low as expected\nfrom the poor accuracy on the answer retrieval task.\nTraining on SELQA gives the best models for both\nWIKI QA and SELQA. Training on SQUAD gives\nthe best model for SQUAD although the model\ntrained on SELQA is comparable. Since the answer\ntriggering datasets are about 5 times larger than the\nanswer selection datasets, it is computationally too\nexpensive to combine all data for training. We plan\nto \ufb01nd a strong machine to perform this experiment\nin near future.\n5 Related work\nLately, several deep learning approaches have been\nproposed for question answering. Yu et al. (2014)\npresented a CNN model that recognizes the seman-\ntic similarity between two sentences. Wang and\nNyberg (2015) presented a stacked bidirectional\nLSTM approach to read words in sequence, then\noutputs their similarity scores. Feng et al. (2015)\napplied a general deep learning framework to non-\nfactoid question answering. Santos et al. (2016)\nintroduced an attentive pooling mechanism that led\nto further improvements in selection-based QA.\n6 Conclusion\nWe present a comprehensive comparison study of\nthe existing corpora for selection-based question\nanswering. Our intrinsic analysis provides a better\nunderstanding of the uniqueness or similarity be-\ntween these corpora. Our extrinsic analysis shows\nthe strength or weakness of combining these cor-\npora together for statistical learning. Additionally,\nwe create a silver-standard dataset for answer re-\ntrieval and triggering, which will be publicly avail-\nable. In the future, we will explore different ways\nof improving the quality of our silver-standard\ndatasets by \ufb01ne-tuning the hyper-parameters.\n\nAnalysis of Wikipedia-based Corpora for Question Answering\nTomasz Jurczyk1, Amit Deshmane2, Jinho D. Choi1\n1Mathematics and Computer Science, Emory University\n2Infosys Ltd.\n{tomasz.jurczyk,jinho.choi}@emory.edu\namit deshmane@infosys.com\nAbstract\nThis paper gives comprehensive analyses\nof corpora based on Wikipedia for several\ntasks in question answering. Four recent\ncorpora are collected, WIKI QA, SELQA,\nSQUAD, and INFOBOX QA, and \ufb01rst an-\nalyzed intrinsically by contextual similari-\nties, question types, and answer categories.\nThese corpora are then analyzed extrinsi-\ncally by three question answering tasks, an-\nswer retrieval, selection, and triggering. An\nindexing-based method for the creation of a\nsilver-standard dataset for answer retrieval\nusing the entire Wikipedia is also presented.\nOur analysis shows the uniqueness of these\ncorpora and suggests a better use of them\nfor statistical question answering learning.\n1 Introduction\nQuestion answering (QA) has been a blooming re-\nsearch \ufb01eld for the last decade. Selection-based QA\nimplies a family of tasks that \ufb01nd answer contexts\nfrom large data given questions in natural language.\nThree tasks have been proposed for selection-based\nQA. Given a document, answer extraction (Shen\nand Klakow, 2006; Sultan et al., 2016) \ufb01nds answer\nphrases whereas answer selection (Wang et al.,\n2007; Yih et al., 2013; Yu et al., 2014; Wang et al.,\n2016) and answer triggering (Yang et al., 2015;\nJurczyk et al., 2016) \ufb01nd answer sentences instead,\nalthough the presence of the answer context is not\nassumed within the provided document for answer\ntriggering but it is for the other two tasks. Recently,\nvarious QA tasks that are not selection-based have\nbeen proposed (Reddy and Bandyopadhyay, 2006;\nHosseini et al., 2014; Jauhar et al., 2016; Sachan\net al., 2016); however, selection-based QA remains\nstill important because of its practical value to real\napplications (e.g., IBM Watson, MIT START).\nSeveral datasets have been released for selection-\nbased QA. Wang et al. (2007) created theQAS ENT\ndataset consisting of 277 questions, which has been\nwidely used for benchmarking the answer selection\ntask. Feng et al. (2015) presented INSURANCE QA\ncomprising 16K+ questions on insurance contexts.\nYang et al. (2015) introduced WIKI QA for answer\nselection and triggering. Jurczyk et al. (2016) cre-\nated SELQA for large real-scale answer triggering.\nRajpurkar et al. (2016) presented SQUAD for an-\nswer extraction and selection as well as for reading\ncomprehension. Finally, Morales et al. (2016) pro-\nvided INFOBOX QA for answer selection.\nThese corpora make it possible to evaluate the ro-\nbustness of statistical question answering learning.\nAlthough all of these corpora target on selection-\nbased QA, they are designed for different purposes\nsuch that it is important to understand the nature of\nthese corpora so a better use of them can be made.\nIn this paper, we make both intrinsic and extrinsic\nanalyses of four latest corpora based on Wikipedia,\nWIKI QA, SELQA, SQUAD, and INFOBOX QA.\nWe \ufb01rst give a thorough intrinsic analysis regarding\ncontextual similarities, question types, and answer\ncategories (Section 2). We then map questions in all\ncorpora to the current version of English Wikipedia\nand benchmark another selection-based QA task,\nanswer retrieval (Section 3). Finally, we present\nan extrinsic analysis through a set of experiments\ncross-testing these corpora using a convolutional\nneural network architecture (Section 4).1\n2 Intrinsic Analysis\nFour publicly available corpora are selected for our\nanalysis. These corpora are based on Wikipedia, so\nmore comparable than the others, and have already\nbeen used for the evaluation of several QA systems.\n1All our resources are publicly available\n: anonymous_url\narXiv:1801.02073v2  [cs.CL]  5 Feb 2018\n\nWIKIQA SELQA SQUAD INFOBOX QA\nSource Bing search queries Crowdsourced Crowdsourced Crowdsourced\nYear 2015 2016 2016 2016\n(AE, AS, AT) (O, O, O) (X, O, O) (O, O, X) (X, O, X)\n(q,c,c/q) (1,242, 12,153, 9.79) (7,904, 95,250, 12.05) (98,202, 496,167, 5.05) (15,271, 271,038, 17.75)\n(w,t) (386,440, 30,191) (3,469,015, 44,099) (19,445,863, 115,092) (5,034,625, 8,323)\n(\u00b5q,\u00b5c) (6.44, 25.36) (11.11, 25.31) (11.33, 27.86) (9.35, 9.22)\n(\u2126q,\u2126a,\u2126f) (46.72, 11.05, 16.96) (32.79, 16.98, 20.19) (32.27, 12.15, 16.54) (26.80, 35.70, 28.09)\nTable 1: Comparisons between the four corpora for answer selection. Note that both WIKI QA and SELQA provide separate\nannotation for answer triggering, which is not shown in this table. The SQUAD column shows statistics excluding the evaluation\nset, which is not publicly available. AE/AS/AT: annotation for answer extraction/selection/triggering, q/c: # of questions/answer\ncandidates, w/t: # of tokens/token types, \u00b5q/c: average length of questions/answer candidates, \u2126q/a: macro average in % of\noverlapping words between question-answer pairs normalized by the questions/answers lengths, \u2126f : (2\u00b7\u2126q\u00b7\u2126a)/(\u2126q+\u2126a).\nWIKI QA (Yang et al., 2015) comprises questions\nselected from the Bing search queries, where user\nclick data give the questions and their correspond-\ning Wikipedia articles. The abstracts of these arti-\ncles are then extracted to create answer candidates.\nThe assumption is made that if many queries lead to\nthe same article, it must contain the answer context;\nhowever, this assumption fails for some occasions,\nwhich makes this dataset more challenging. Since\nthe existence of answer contexts is not guaranteed\nin this task, it is called answer triggering instead of\nanswer selection.\nSELQA (Jurczyk et al., 2016) is a product of \ufb01ve\nannotation tasks through crowdsourcing. It consists\nof about 8K questions where a half of the questions\nare paraphrased from the other half, aiming to re-\nduce contextual similarities between questions and\nanswers. Each question is associated with a section\nin Wikipedia where the answer context is guaran-\nteed, and also with \ufb01ve sections selected from the\nentire Wikipedia where the selection is made by\nthe Lucene search engine. This second dataset does\nnot assume the existence of the answer context, so\ncan be used for the evaluation of answer triggering.\nSQUAD (Rajpurkar et al., 2016) presents 107K+\ncrowdsourced questions on 536 Wikipedia articles,\nwhere the answer contexts are guaranteed to exist\nwithin the provided paragraph. It contains annota-\ntion of answer phrases as well as the pointers to the\nsentences including the answer phrases; thus, it can\nbe used for both answer extraction and selection.\nThis corpus also provides human accuracy on those\nquestions, setting up a reasonable upper bound for\nmachines. To avoid over\ufb01tting, the evaluation set\nis not publicly available although system outputs\ncan be evaluated by their provided script.\nINFOBOX QA (Morales et al., 2016) gives 15K+\nquestions based on the infoboxes from 150 articles\nin Wikipedia. Each question is crowdsourced and\nassociated with an infobox, where each line of the\ninfobox is considered an answer candidate. This\ncorpus emphasizes the gravity of infoboxes, which\nsummary arguably the most commonly asked infor-\nmation about those articles. Although the nature of\nthis corpus is different from the others, it can also\nbe used to evaluate answer selection.\nAnalysis\nAll corpora provide datasets/splits for answer se-\nlection, whereas only ( WIKI QA, SQUAD) and\n(WIKI QA, SELQA) provide datasets for answer\nextraction and answer triggering, respectively.\nSQUAD is much larger in size although ques-\ntions in this corpus are often paraphrased multi-\nple times. On the contrary, SQUAD\u2019s average\ncandidates per question ( c/q) is the smallest be-\ncause SQUAD extracts answer candidates from\nparagraphs whereas the others extract them from\nsections or infoboxes that consist of bigger con-\ntexts. Although INFOBOX QA is larger than WIK-\nIQA or SELQA, the number of token types (t) in\nINFOBOX QA is smaller than those two, due to the\nrepetitive nature of infoboxes.\nAll corpora show similar average answer candi-\ndate lengths (\u00b5c), except for INFOBOX QA where\neach line in the infobox is considered a candidate.\nSELQA and SQUAD show similar average ques-\ntion lengths (\u00b5q) because of the similarity between\ntheir annotation schemes. It is not surprising that\nWIKI QA\u2019s average question length is the smallest,\nconsidering their questions are taken from search\nqueries. INFOBOX QA\u2019s average question length is\nrelatively small, due to the restricted information\nthat can be asked from the infoboxes.INFOBOX QA\nand WIKI QA show the least question-answer word\noverlaps over questions and answers (\u2126q and \u2126a in\nTable 1), respectively. In terms of the F1-score for\noverlapping words (\u2126f ), SQUAD gives the least\nportion of overlaps between question-answer pairs",
            0.4933603340000019
          ]
        }
      ],
      "reference_answers": [
        "Yes No",
        "seven  7",
        "They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap."
      ]
    },
    "1707.00995.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.22955294554082173,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.5146464646464647,
          "recall": 0.75,
          "f1": 0.5349572919962037
        },
        "rouge-2": {
          "precision": 0.40205627705627706,
          "recall": 0.5476190476190476,
          "f1": 0.400484596746279
        },
        "rouge-L": {
          "precision": 0.4580808080808081,
          "recall": 0.5993303571428572,
          "f1": 0.4568174628282189
        }
      },
      "per_question_metrics": [
        {
          "question": "What misbehavior is identified?",
          "generated_answer": "The identified misbehavior is that if the attention mechanism loses track of the objects in the picture and \"gets lost\", the model still takes the image into account and somehow overrides the information brought by the text-based annotations, leading to a totally misled translation.",
          "reference_answer": "if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.7111111111111111,
              "recall": 0.5,
              "f1": 0.5871559633027522
            },
            "rouge-2": {
              "precision": 0.6136363636363636,
              "recall": 0.42857142857142855,
              "f1": 0.5046728971962617
            },
            "rouge-L": {
              "precision": 0.6888888888888889,
              "recall": 0.484375,
              "f1": 0.5688073394495412
            }
          },
          "bleu_score": 0.39885831815788875,
          "exact_match": 0.0
        },
        {
          "question": "Which attention mechanisms do they compare?",
          "generated_answer": "The attention mechanisms compared in the provided context are:\n\n1. Soft attention\n2. Local attention\n3. Stochastic attention\n4. Hard stochastic attention",
          "reference_answer": "Soft attention, Hard Stochastic attention, Local Attention",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.3181818181818182,
              "recall": 1.0,
              "f1": 0.4827586206896552
            },
            "rouge-2": {
              "precision": 0.19047619047619047,
              "recall": 0.6666666666666666,
              "f1": 0.2962962962962963
            },
            "rouge-L": {
              "precision": 0.22727272727272727,
              "recall": 0.7142857142857143,
              "f1": 0.3448275862068965
            }
          },
          "bleu_score": 0.06024757292375468,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "What misbehavior is identified?",
        "Which attention mechanisms do they compare?"
      ],
      "generated_answers": [
        {
          "answer": "The identified misbehavior is that if the attention mechanism loses track of the objects in the picture and \"gets lost\", the model still takes the image into account and somehow overrides the information brought by the text-based annotations, leading to a totally misled translation.",
          "relevant_chunks": [
            "Figure 7: Representative \ufb01gures of the soft-attention behavior discussed in \u00a75.3\ngating scalar happens to be really low in the\nstochastic attention mechanism: a signi\ufb01cant\namount of sentences don\u2019t have a summed gating\nscalar \u22650.10. The model totally discards the\nimage in the translation process.\nIt is also worth to mention that we use a\nResNet trained on 1.28 million images for a\nclassi\ufb01cation tasks. The features used by the\nattention mechanism are strongly object-oriented\nand the machine could miss important information\nfor a multimodal translation task. We believe\nthat the robust architecture of both encoders\n{\u2190 \u2212\u03a8enc,\u2212 \u2192\u03a8enc}combined with a GRU layer and\nword-embeddings took care of the right trans-\nlation for relationships between objects and\ntime-dependencies. Yet, we noticed a common\nmisbehavior for all our multimodal models: if the\nattention loose track of the objects in the picture\nand \u201dgets lost\u201d, the model still takes it into account\nand somehow overrides the information brought\nby the text-based annotations. The translation\nis then totally mislead. We illustrate with an\nexample:\nRef: Ein Kind sitzt auf den Schultern einer\nFrau und klatscht .\nMono: Ein Kind sitzt auf den Schultern einer\nFrau und schl\u00a8aft .\nSoft: Ein Kind , das sich auf der Schultern\neines Frau reitet , f\u00a8ahrt auf den\nSchultern .\nHard: Ein Kind in der Haltung , w \u00a8ahrend er\nauf den Schultern einer Frau f\u00a8ahrt .\nThe monomodal translation has a sentence-level\nBLEU of 82.16 whilst the soft attention and hard\nstochastic attention scores are of 16.82 and 34.45\nrespectively. Figure 8 shows the attention maps\nfor both mechanism. Nevertheless, one has to\nconcede that the use of images indubitably helps\nthe translation as shown in the score tabular.\nFigure 8: Wrong detection for both Soft attention\n(top) and Hard stochastic attention (bottom)\n6 Conclusion and future work\nWe have tried different attention mechanism and\ntweaks for the image modality. We showed im-\nprovements and encouraging results overall on the\nFlickr30K Entities dataset. Even though we iden-\nti\ufb01ed some \ufb02aws of the current attention mecha-\nnisms, we can conclude pretty safely that images\nare an helpful resource for the machine in a trans-\nlation task. We are looking forward to try out\nricher and more suitable features for multimodal\ntranslation (ie. dense captioning features). An-\nother interesting approach would be to use visu-\nally grounded word embeddings to capture visual\nnotions of semantic relatedness.\n\nModel Test Scores\nBLEU\u2191 METEOR\u2191 TER\u2193\nMonomodal (text only)\nCaglayan et al. (2016) 32.50 49.2\nCalixto et al. (2017) 33.70 52.3 46.7\nNMT 34.11 \u2191+0.41 52.4 \u2191+0.1 46.2 \u2193-0.5\nMultimodal\nCaglayan et al. (2016) 27.82 45.0 -\nHuang et al. (2016) 36.50 54.1 -\nCalixto et al. (2017) 36.50 55.0 43.7\nSoft attention 37.10 \u2191+0.60 54.8 \u2193-0.2 42.8 \u2193-0.9\nLocal attention 37.55 \u2191+1.05 54.8 \u2193-0.2 42.4 \u2193-1.3\nStochastic attention 38.01 \u2191+1.51 55.4 \u2191+0.4 41.5 \u2193-2.2\nSoft attention + grounded image 37.62 \u2191+1.12 55.3 \u2191+0.3 41.8 \u2193-1.9\nStochastic attention + grounded image 38.17 \u2191+1.67 55.4 \u2191+0.4 41.5 \u2193-2.2\nTable 1: Results on the 1000 test triples of the Multi30K dataset. We pick Calixto et al. (2017) scores\nas baseline and report our results accordingly (green for improvement and red for deterioration). In each\nof our experiments, Soft attention is used for text. The comparison is hence with respect to the attention\nmechanism used for the image modality.\nare roughly the same across our models which is\nexpected because all attention mechanisms share\nthe same subsequent step at every time-step t,\ni.e. taking into account the attention weights of\nprevious time-step t\u22121 in order to compute the\nnew intermediate hidden state proposal and there-\nfore the new context vector it. Again, the largest\nimprovement is given by the hard stochastic\nattention mechanism (+0.4 METEOR): because it\nis modeled as a decision process according to the\nprevious choices, this may reinforce the idea of\nrecall. We also remark interesting improvements\nwhen using the grounded mechanism, especially\nfor the soft attention. The soft attention may\nbene\ufb01t more of the grounded image because of\nthe wide range of spatial locations it looks at,\nespecially compared to the stochastic attention.\nThis motivates us to dig into more complex\ngrounding techniques in order to give the machine\na deeper understanding of the modalities.\nNote that even though our baseline NMT\nmodel is basically the same as Calixto et al.\n(2017), our experiments results are slightly better.\nThis is probably due to the different use of dropout\nand subwords. We also compared our results to\nCaglayan et al. (2016) because our multimodal\nmodels are nearly identical with the major ex-\nception of the gating scalar (cfr. section 4). This\nmotivated some of our qualitative analysis and\nhesitation towards the current architecture in the\nnext section.\n5.3 Qualitative results\nFor space-saving and ergonomic reasons, we\nonly discuss about the hard stochastic and soft\nattention, the latter being a generalization of the\nlocal attention.\nAs we can see in Figure 7, the soft attention model\nis looking roughly at the same region of the image\nfor every decoding step t. Because the words\n\u201dhund\u201d(dog), \u201dwald\u201d(forest) or \u201dweg\u201d(way) in left\nimage are objects, they bene\ufb01t from a high gating\nscalar. As a matter of fact, the attention mech-\nanism has learned to detect the objects within a\nscene (at every time-step, whichever word we\nare decoding as shown in the right image) and\nthe gating scalar has learned to decide whether\nor not we have to look at the picture (or more\naccurately whether or not we are translating an\nobject). Without this scalar, the translation scores\nundergo a massive drop (as seen in Caglayan\net al. (2016)) which means that the attention\nmechanisms don\u2019t really understand the more\ncomplex relationships between objects, what is\nreally happening in the scene. Surprisingly, the\n\nFigure 2: Ein Junge sitzt auf und blickt aus einem\nMikroskop .\n3.2 Hard Stochastic attention\nThis model is a stochastic and sampling-based\nprocess where, at every timestep t, we are making\na hard choice to attend only one annotation. This\ncorresponds to one spatial location in the image.\nHard attention has previously been used in the\ncontext of object recognition (Mnih et al., 2014;\nBa et al., 2015) and later extended to image\ndescription generation (Xu et al., 2015). In the\ncontext of multimodal NMT, we can follow Xu\net al. (2015) because both our models involve the\nsame process on images.\nThe mechanism fatt is now a function that\nreturns a sampled intermediate latent variables\n\u03b3t,i based upon a multinouilli distribution\nparameterized by \u03b1:\n\u03b3t \u223cMultinoulli({\u03b11,...,L}) (15)\nwhere \u03b3t,i an indicator one-hot variable which is\nset to 1 if the i-th annotation (out of L) is the one\nused to compute the context vector it:\np(\u03b3t,l = 1|\u03b3<t,I ) =\u03b1t,l (16)\nit =\nL\u2211\ni=1\n\u03b3t,iai (17)\nContext vector it is now seen as the random vari-\nable of this distribution. We de\ufb01ne the variational\nlower bound L(\u03b3) on the marginal log evidence\nlog p(y|I) of observing the target sentenceygiven\nmodality annotations I.\nL(\u03b3) =\n\u2211\n\u03b3\np(\u03b3|I) logp(y|\u03b3,I)\n\u2264log\n\u2211\n\u03b3\np(\u03b3|I)p(y|\u03b3,I)\n= logp(y|I) (18)\nThe learning rules can be derived by taking\nderivatives of the above variational free energy\nL(\u03b3) with respect to the model parameter W :\n\u2202L\n\u2202W =\n\u2211\n\u03b3\np(\u03b3|I)\n[\n\u2202log p(y|\u03b3,I)\n\u2202W +\nlog p(y|\u03b3,I)\u2202log p(\u03b3|I)\n\u2202W\n]\n(19)\nIn order to propagate a gradient through this\nprocess, the summation in equation 19 can then be\napproximated using Monte Carlo based sampling\nde\ufb01ned by equation 16:\n\u2202L\n\u2202W \u2248 1\nN\nN\u2211\nn=1\n[\n\u2202log p(y|\u02dc\u03b3n,I)\n\u2202W +\nlog p(y|\u02dc\u03b3n,I)\u2202log p(\u02dc\u03b3n|I)\n\u2202W\n]\n(20)\nTo reduce variance of the estimator in equation\n20, we use a moving average baseline estimated\nas an accumulated sum of the previous log likeli-\nhoods with exponential decay upon seeing thek-th\nmini-batch:\nbk = 0.9 \u00d7bk\u22121 + 0.1 \u00d7log p(y|\u02dc\u03b3k,I) (21)\nFigure 3: Ein Mann sitzt neben einem\nComputerbildschirm .\nFigure 4: Ein Mann in einem orangefarbenen\nHemd und mit Helm .\n3.3 Local Attention\nIn this section, we propose a local attentional\nmechanism that chooses to focus only on a small",
            1.1599695000000025
          ]
        },
        {
          "answer": "The attention mechanisms compared in the provided context are:\n\n1. Soft attention\n2. Local attention\n3. Stochastic attention\n4. Hard stochastic attention",
          "relevant_chunks": [
            "Model Test Scores\nBLEU\u2191 METEOR\u2191 TER\u2193\nMonomodal (text only)\nCaglayan et al. (2016) 32.50 49.2\nCalixto et al. (2017) 33.70 52.3 46.7\nNMT 34.11 \u2191+0.41 52.4 \u2191+0.1 46.2 \u2193-0.5\nMultimodal\nCaglayan et al. (2016) 27.82 45.0 -\nHuang et al. (2016) 36.50 54.1 -\nCalixto et al. (2017) 36.50 55.0 43.7\nSoft attention 37.10 \u2191+0.60 54.8 \u2193-0.2 42.8 \u2193-0.9\nLocal attention 37.55 \u2191+1.05 54.8 \u2193-0.2 42.4 \u2193-1.3\nStochastic attention 38.01 \u2191+1.51 55.4 \u2191+0.4 41.5 \u2193-2.2\nSoft attention + grounded image 37.62 \u2191+1.12 55.3 \u2191+0.3 41.8 \u2193-1.9\nStochastic attention + grounded image 38.17 \u2191+1.67 55.4 \u2191+0.4 41.5 \u2193-2.2\nTable 1: Results on the 1000 test triples of the Multi30K dataset. We pick Calixto et al. (2017) scores\nas baseline and report our results accordingly (green for improvement and red for deterioration). In each\nof our experiments, Soft attention is used for text. The comparison is hence with respect to the attention\nmechanism used for the image modality.\nare roughly the same across our models which is\nexpected because all attention mechanisms share\nthe same subsequent step at every time-step t,\ni.e. taking into account the attention weights of\nprevious time-step t\u22121 in order to compute the\nnew intermediate hidden state proposal and there-\nfore the new context vector it. Again, the largest\nimprovement is given by the hard stochastic\nattention mechanism (+0.4 METEOR): because it\nis modeled as a decision process according to the\nprevious choices, this may reinforce the idea of\nrecall. We also remark interesting improvements\nwhen using the grounded mechanism, especially\nfor the soft attention. The soft attention may\nbene\ufb01t more of the grounded image because of\nthe wide range of spatial locations it looks at,\nespecially compared to the stochastic attention.\nThis motivates us to dig into more complex\ngrounding techniques in order to give the machine\na deeper understanding of the modalities.\nNote that even though our baseline NMT\nmodel is basically the same as Calixto et al.\n(2017), our experiments results are slightly better.\nThis is probably due to the different use of dropout\nand subwords. We also compared our results to\nCaglayan et al. (2016) because our multimodal\nmodels are nearly identical with the major ex-\nception of the gating scalar (cfr. section 4). This\nmotivated some of our qualitative analysis and\nhesitation towards the current architecture in the\nnext section.\n5.3 Qualitative results\nFor space-saving and ergonomic reasons, we\nonly discuss about the hard stochastic and soft\nattention, the latter being a generalization of the\nlocal attention.\nAs we can see in Figure 7, the soft attention model\nis looking roughly at the same region of the image\nfor every decoding step t. Because the words\n\u201dhund\u201d(dog), \u201dwald\u201d(forest) or \u201dweg\u201d(way) in left\nimage are objects, they bene\ufb01t from a high gating\nscalar. As a matter of fact, the attention mech-\nanism has learned to detect the objects within a\nscene (at every time-step, whichever word we\nare decoding as shown in the right image) and\nthe gating scalar has learned to decide whether\nor not we have to look at the picture (or more\naccurately whether or not we are translating an\nobject). Without this scalar, the translation scores\nundergo a massive drop (as seen in Caglayan\net al. (2016)) which means that the attention\nmechanisms don\u2019t really understand the more\ncomplex relationships between objects, what is\nreally happening in the scene. Surprisingly, the\n\nFigure 2: Ein Junge sitzt auf und blickt aus einem\nMikroskop .\n3.2 Hard Stochastic attention\nThis model is a stochastic and sampling-based\nprocess where, at every timestep t, we are making\na hard choice to attend only one annotation. This\ncorresponds to one spatial location in the image.\nHard attention has previously been used in the\ncontext of object recognition (Mnih et al., 2014;\nBa et al., 2015) and later extended to image\ndescription generation (Xu et al., 2015). In the\ncontext of multimodal NMT, we can follow Xu\net al. (2015) because both our models involve the\nsame process on images.\nThe mechanism fatt is now a function that\nreturns a sampled intermediate latent variables\n\u03b3t,i based upon a multinouilli distribution\nparameterized by \u03b1:\n\u03b3t \u223cMultinoulli({\u03b11,...,L}) (15)\nwhere \u03b3t,i an indicator one-hot variable which is\nset to 1 if the i-th annotation (out of L) is the one\nused to compute the context vector it:\np(\u03b3t,l = 1|\u03b3<t,I ) =\u03b1t,l (16)\nit =\nL\u2211\ni=1\n\u03b3t,iai (17)\nContext vector it is now seen as the random vari-\nable of this distribution. We de\ufb01ne the variational\nlower bound L(\u03b3) on the marginal log evidence\nlog p(y|I) of observing the target sentenceygiven\nmodality annotations I.\nL(\u03b3) =\n\u2211\n\u03b3\np(\u03b3|I) logp(y|\u03b3,I)\n\u2264log\n\u2211\n\u03b3\np(\u03b3|I)p(y|\u03b3,I)\n= logp(y|I) (18)\nThe learning rules can be derived by taking\nderivatives of the above variational free energy\nL(\u03b3) with respect to the model parameter W :\n\u2202L\n\u2202W =\n\u2211\n\u03b3\np(\u03b3|I)\n[\n\u2202log p(y|\u03b3,I)\n\u2202W +\nlog p(y|\u03b3,I)\u2202log p(\u03b3|I)\n\u2202W\n]\n(19)\nIn order to propagate a gradient through this\nprocess, the summation in equation 19 can then be\napproximated using Monte Carlo based sampling\nde\ufb01ned by equation 16:\n\u2202L\n\u2202W \u2248 1\nN\nN\u2211\nn=1\n[\n\u2202log p(y|\u02dc\u03b3n,I)\n\u2202W +\nlog p(y|\u02dc\u03b3n,I)\u2202log p(\u02dc\u03b3n|I)\n\u2202W\n]\n(20)\nTo reduce variance of the estimator in equation\n20, we use a moving average baseline estimated\nas an accumulated sum of the previous log likeli-\nhoods with exponential decay upon seeing thek-th\nmini-batch:\nbk = 0.9 \u00d7bk\u22121 + 0.1 \u00d7log p(y|\u02dc\u03b3k,I) (21)\nFigure 3: Ein Mann sitzt neben einem\nComputerbildschirm .\nFigure 4: Ein Mann in einem orangefarbenen\nHemd und mit Helm .\n3.3 Local Attention\nIn this section, we propose a local attentional\nmechanism that chooses to focus only on a small\n\nFigure 7: Representative \ufb01gures of the soft-attention behavior discussed in \u00a75.3\ngating scalar happens to be really low in the\nstochastic attention mechanism: a signi\ufb01cant\namount of sentences don\u2019t have a summed gating\nscalar \u22650.10. The model totally discards the\nimage in the translation process.\nIt is also worth to mention that we use a\nResNet trained on 1.28 million images for a\nclassi\ufb01cation tasks. The features used by the\nattention mechanism are strongly object-oriented\nand the machine could miss important information\nfor a multimodal translation task. We believe\nthat the robust architecture of both encoders\n{\u2190 \u2212\u03a8enc,\u2212 \u2192\u03a8enc}combined with a GRU layer and\nword-embeddings took care of the right trans-\nlation for relationships between objects and\ntime-dependencies. Yet, we noticed a common\nmisbehavior for all our multimodal models: if the\nattention loose track of the objects in the picture\nand \u201dgets lost\u201d, the model still takes it into account\nand somehow overrides the information brought\nby the text-based annotations. The translation\nis then totally mislead. We illustrate with an\nexample:\nRef: Ein Kind sitzt auf den Schultern einer\nFrau und klatscht .\nMono: Ein Kind sitzt auf den Schultern einer\nFrau und schl\u00a8aft .\nSoft: Ein Kind , das sich auf der Schultern\neines Frau reitet , f\u00a8ahrt auf den\nSchultern .\nHard: Ein Kind in der Haltung , w \u00a8ahrend er\nauf den Schultern einer Frau f\u00a8ahrt .\nThe monomodal translation has a sentence-level\nBLEU of 82.16 whilst the soft attention and hard\nstochastic attention scores are of 16.82 and 34.45\nrespectively. Figure 8 shows the attention maps\nfor both mechanism. Nevertheless, one has to\nconcede that the use of images indubitably helps\nthe translation as shown in the score tabular.\nFigure 8: Wrong detection for both Soft attention\n(top) and Hard stochastic attention (bottom)\n6 Conclusion and future work\nWe have tried different attention mechanism and\ntweaks for the image modality. We showed im-\nprovements and encouraging results overall on the\nFlickr30K Entities dataset. Even though we iden-\nti\ufb01ed some \ufb02aws of the current attention mecha-\nnisms, we can conclude pretty safely that images\nare an helpful resource for the machine in a trans-\nlation task. We are looking forward to try out\nricher and more suitable features for multimodal\ntranslation (ie. dense captioning features). An-\nother interesting approach would be to use visu-\nally grounded word embeddings to capture visual\nnotions of semantic relatedness.",
            0.8296129160000021
          ]
        }
      ],
      "reference_answers": [
        "if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations",
        "Soft attention, Hard Stochastic attention, Local Attention"
      ]
    },
    "1707.00110.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.011113685613885628,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.17045454545454547,
          "recall": 0.5,
          "f1": 0.24358974358974358
        },
        "rouge-2": {
          "precision": 0.015625,
          "recall": 0.1,
          "f1": 0.02702702702702703
        },
        "rouge-L": {
          "precision": 0.1240530303030303,
          "recall": 0.35416666666666663,
          "f1": 0.17628205128205127
        }
      },
      "per_question_metrics": [
        {
          "question": "Which baseline methods are used?",
          "generated_answer": "The baseline methods used for comparison in the experiments are standard models with and without attention.",
          "reference_answer": "standard parametrized attention and a non-attention baseline",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.25,
              "recall": 0.5,
              "f1": 0.3333333333333333
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.1875,
              "recall": 0.375,
              "f1": 0.25
            }
          },
          "bleu_score": 0.01626739260030573,
          "exact_match": 0.0
        },
        {
          "question": "Which datasets are used in experiments?",
          "generated_answer": "The experiments used 4 large machine translation datasets from WMT'17, specifically for the language pairs English-Czech (en-cs), English-German (en-de), English-Finish (en-fi), and English-Turkish (en-tr).",
          "reference_answer": "Sequence Copy Task and WMT'17",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.09090909090909091,
              "recall": 0.5,
              "f1": 0.15384615384615385
            },
            "rouge-2": {
              "precision": 0.03125,
              "recall": 0.2,
              "f1": 0.05405405405405406
            },
            "rouge-L": {
              "precision": 0.06060606060606061,
              "recall": 0.3333333333333333,
              "f1": 0.10256410256410256
            }
          },
          "bleu_score": 0.005959978627465526,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "Which baseline methods are used?",
        "Which datasets are used in experiments?"
      ],
      "generated_answers": [
        {
          "answer": "The baseline methods used for comparison in the experiments are standard models with and without attention.",
          "relevant_chunks": [
            "Figure 2: Surface for the position encodings.\n4 Experiments\n4.1 Toy Copying Experiment\nDue to the reduction of computational time complex-\nity we expect our method to yield performance gains\nespecially for longer sequences and tasks where the\nsource can be compactly represented in a fixed-size\nmemory matrix. To investigate the trade-off between\nspeed and performance, we compare our technique\nto standard models with and without attention on a\nSequence Copy Task of varying length like in Graves\net al. (2014). We generated 4 training datasets of\n100,000 examples and a validation dataset of 1,000 ex-\namples. The vocabulary size was 20. For each dataset,\nthe sequences had lengths randomly chosen between 0\nto L, forL\u2208{10,50,100,200}unique to each dataset.\n4.1.1 Training Setup\nAll models are implemented using TensorFlow\nbased on the seq2seq implementation of Britz et al.\n(2017)3 and trained on a single machine with a\nNvidia K40m GPU. We use a 2-layer 256-unit, a\nbidirectional LSTM (Hochreiter and Schmidhuber,\n1997) encoder, a 2-layer 256-unit LSTM decoder,\nand 256-dimensional embeddings. For the attention\nbaseline, we use the standard parametrized attention\n(Bahdanau et al., 2014). Dropout of 0.2 (0.8 keep\nprobability) is applied to the input of each cell and\nwe optimize using Adam (Kingma and Ba, 2014) at\na learning rate of 0.0001 and batch size 128. We train\nfor at most 200,000 steps (see Figure 3 for sample\nlearning curves). BLEU scores are calculated on\ntokenized data using themulti-bleu.perlscript in\nMoses.4 We decode using beam search with a beam\n3http://github.com/google/seq2seq\n4http://github.com/moses-smt/mosesdecoder\nLength Model BLEU Time (s)\n20 No Att 99.93 2.03\nK=1 99.52 2.12\nK=4 99.56 2.25\nK=16 99.56 2.21\nK=32 99.57 2.59\nK=64 99.75 2.86\nAtt 99.98 2.86\n50 No Att 97.37 3.90\nK=1 98.86 4.33\nK=4 99.95 4.48\nK=16 99.96 4.58\nK=32 99.96 5.35\nK=64 99.97 5.84\nAtt 99.94 6.46\n100 No Att 73.99 6.33\nK=1 87.42 7.32\nK=4 99.81 7.47\nK=16 99.97 7.50\nK=32 99.99 7.65\nK=64 100.00 7.77\nAtt 100.00 11.00\n200 No Att 32.64 9.10\nK=1 44.22 9.30\nK=4 98.54 9.49\nK=16 99.98 9.53\nK=32 100.00 9.59\nK=64 100.00 9.78\nAtt 100.00 14.28\nTable 1: BLEU scores and computation times with\nvaryingKand sequence length compared to baseline\nmodels with and without attention.\nsize of 10 (Wiseman and Rush, 2016).\n4.1.2 Results\nTable 1 shows the BLEU scores of our model on differ-\nent sequence lengths while varyingK. This is a study\nof the trade-off between computational time and rep-\nresentational power. A largeKallows us to compute\ncomplex source representations, while aKof 1 limits\nthe source representation to a single vector. We can\nsee that performance consistently increases withKup\nto a point that depends on the data length, with longer\nsequences requiring more complex representations.\nThe results with and without position encodings are\nalmost identical on the toy data. Our technique learns\nto fit the data as well as the standard attention mecha-\nnism despite having less representational power. Both\nbeat the non-attention baseline by a significant margin.\n\nK\nK\n(a) Training curves for en-fi\nK\nK (b) Training curves for en-tr\nFigure 4: Comparing training curves for en-fi and en-tr with sigmoid encoder scoring and softmax decoder\nscoring and position encoding. Note that en-tr curves converged very quickly.\nModel Dataset K en-cs en-de en-fi en-tr\nMemory Attention Test 32 19.37 28.82 15.87 -\n64 19.65 29.53 16.49 -\nV alid 32 19.20 26.20 15.90 12.94\n64 19.63 26.39 16.35 13.06\nMemory Attention + PETest 32 19.45 29.53 15.86 -\n64 20.36 30.61 17.03 -\nV alid 32 19.35 26.22 16.31 12.97\n64 19.73 27.31 16.91 13.25\nAttention Test - 19.19 30.99 17.34 -\nV alid - 18.61 28.13 17.16 13.76\nTable 2: BLEU scores on WMT\u201917 translation datasets from the memory attention models and regular attention\nbaselines. We picked the best out of the four scoring function combinations on the validation set. Note that\nen-tr does not have an official test set. Best test scores on each dataset are highlighted.\nModel Decoding Time (s)\nK=32 26.85\nK=64 27.13\nAttention 33.28\nTable 3: Decoding time, averaged across 10 runs, for\nthe en-de validation set (2169 examples) with average\nsequence length of 35. Results are similar for both\nPE and non-PE models.\nvocabulary of 16k. We measured decoding time over\nthe full validation set, not including time used for\nmodel setup and data loading, averaged across 10 runs.\nThe average sequence length for examples in this data\nwas 35, and we expect more significant speedups\nfor tasks with longer sequences, as suggested by our\nexperiments on toy data. Note that in our NMT ex-\namples/experiments,K\u2248T, but we obtain computa-\ntional savings from the fact thatK\u226aD. We may be\nable to setK\u226aT, as in toy copying, and still get very\ngood performance in other tasks. For instance, in sum-\nmarization the source is complex but the representa-\ntion of the source required to perform the task is \u201dsim-\nple\u201d (i.e. all that is needed to generate the abstract).\nFigure 5 shows the effect of using sigmoid and\nsoftmax function in the encoders and decoders. We\nfound that softmax/softmax consistently performs\nbadly, while all other combinations perform about\nequally well. We report results for the best combi-\nnation only (as chosen on the validation set), but we\nfound this choice to only make a minor difference.\n\n0 50 100 150 200\nsteps (k)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5log perplexity\nK=1\nK=4\nK=16\nK=32\nK=64\nattention\nno attention\n(a) Comparison of varyingK for copying sequences of length\n200 on evaluation data, showing that largeK leads to faster\nconvergence and smallK performs similarly to the non-attentional\nbaseline.\n0 50 100 150 200\nsteps (k)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5log perplexity\nsigmoid/sigmoid\nsigmoid/softmax\nsoftmax/sigmoid\nsoftmax/softmax\n(b) Comparison of sigmoid and softmax functions for choosing the\nencoder and decoder attention scores on evaluation data, showing\nthat choice of gating/normalization matters.\nFigure 3: Training Curves for the Toy Copy task\nThat we are able to represent the source sequence\nwith a fixed size matrix with fewer than|S|rows\nsuggests that traditional attention mechanisms\nmay be representing the source with redundancies\nand wasting computational resources. This makes\nintuitive sense for the toy task, which should require\na relatively simple representation.\nThe last column shows that our technique signif-\nicantly speeds up the inference process. The gap in\ninference speed increases as sequences become longer.\nWe measured inference time on the full validation\nset of 1,000 examples, not including data loading or\nmodel construction times.\nFigure 3a shows the learning curves for sequence\nlength 200. We see thatK=1 is unable to fit the data\ndistribution, whileK\u2208{32,64}fits the data almost as\nquickly as the attention-based model. Figure 3b shows\nthe effect of varying the encoder and decoder scoring\nfunctions between softmax and sigmoid. All combina-\ntions manage to fit the data, but some converge faster\nthan others. In section 5 we show that distinct align-\nments are learned by different function combinations.\n4.2 Machine Translation\nNext, we explore if the memory-based attention\nmechanism is able to fit complex real-world datasets.\nFor this purpose we use 4 large machine translation\ndatasets of WMT\u2019175 on the following language\npairs: English-Czech (en-cs, 52M examples), English-\nGerman (en-de, 5.9M examples), English-Finish\n(en-fi, 2.6M examples), and English-Turkish (en-tr,\n207,373 examples). We used the newly available pre-\n5statmt.org/wmt17/translation-task.html\nprocessed datasets for the WMT\u201917 task.6 Note that\nour scores may not be directly comparable to other\nwork that performs their own data pre-processing. We\nlearn shared vocabularies of 16,000 subword units\nusing the BPE algorithm (Sennrich et al., 2016).\nWe use newstest2015 as a validation set, and report\nBLEU on newstest2016.\n4.2.1 Training Setup\nWe use a similar setup to the Toy Copy task, but\nuse 512 RNN and embedding units, train using 8\ndistributed workers with 1 GPU each, and train for\nat most 1M steps. We save checkpoints every 30\nminutes during training, and choose the best based\non the validation BLEU score.\n4.2.2 Results\nTable 2 compares our approach with and without\nposition encodings, and with varying values for\nhyperparameterK, to baseline models with regular\nattention mechanism. Learning curves are shown in\nFigure 4. We see that our memory attention model\nwith sufficiently highK performs on-par with, or\nslightly better, than the attention-based baseline model\ndespite its simpler nature. Across the board, models\nwith K = 64performed better than corresponding\nmodels withK= 32, suggesting that using a larger\nnumber of attention vectors can capture a richer under-\nstanding of source sequences. Position encodings also\nseem to consistently improve model performance.\nTable 3 shows that our model results in faster de-\ncoding time even on a complex dataset with a large\n6http://data.statmt.org/wmt17/translation-task/preprocessed",
            0.7193967500000014
          ]
        },
        {
          "answer": "The experiments used 4 large machine translation datasets from WMT'17, specifically for the language pairs English-Czech (en-cs), English-German (en-de), English-Finish (en-fi), and English-Turkish (en-tr).",
          "relevant_chunks": [
            "Figure 2: Surface for the position encodings.\n4 Experiments\n4.1 Toy Copying Experiment\nDue to the reduction of computational time complex-\nity we expect our method to yield performance gains\nespecially for longer sequences and tasks where the\nsource can be compactly represented in a fixed-size\nmemory matrix. To investigate the trade-off between\nspeed and performance, we compare our technique\nto standard models with and without attention on a\nSequence Copy Task of varying length like in Graves\net al. (2014). We generated 4 training datasets of\n100,000 examples and a validation dataset of 1,000 ex-\namples. The vocabulary size was 20. For each dataset,\nthe sequences had lengths randomly chosen between 0\nto L, forL\u2208{10,50,100,200}unique to each dataset.\n4.1.1 Training Setup\nAll models are implemented using TensorFlow\nbased on the seq2seq implementation of Britz et al.\n(2017)3 and trained on a single machine with a\nNvidia K40m GPU. We use a 2-layer 256-unit, a\nbidirectional LSTM (Hochreiter and Schmidhuber,\n1997) encoder, a 2-layer 256-unit LSTM decoder,\nand 256-dimensional embeddings. For the attention\nbaseline, we use the standard parametrized attention\n(Bahdanau et al., 2014). Dropout of 0.2 (0.8 keep\nprobability) is applied to the input of each cell and\nwe optimize using Adam (Kingma and Ba, 2014) at\na learning rate of 0.0001 and batch size 128. We train\nfor at most 200,000 steps (see Figure 3 for sample\nlearning curves). BLEU scores are calculated on\ntokenized data using themulti-bleu.perlscript in\nMoses.4 We decode using beam search with a beam\n3http://github.com/google/seq2seq\n4http://github.com/moses-smt/mosesdecoder\nLength Model BLEU Time (s)\n20 No Att 99.93 2.03\nK=1 99.52 2.12\nK=4 99.56 2.25\nK=16 99.56 2.21\nK=32 99.57 2.59\nK=64 99.75 2.86\nAtt 99.98 2.86\n50 No Att 97.37 3.90\nK=1 98.86 4.33\nK=4 99.95 4.48\nK=16 99.96 4.58\nK=32 99.96 5.35\nK=64 99.97 5.84\nAtt 99.94 6.46\n100 No Att 73.99 6.33\nK=1 87.42 7.32\nK=4 99.81 7.47\nK=16 99.97 7.50\nK=32 99.99 7.65\nK=64 100.00 7.77\nAtt 100.00 11.00\n200 No Att 32.64 9.10\nK=1 44.22 9.30\nK=4 98.54 9.49\nK=16 99.98 9.53\nK=32 100.00 9.59\nK=64 100.00 9.78\nAtt 100.00 14.28\nTable 1: BLEU scores and computation times with\nvaryingKand sequence length compared to baseline\nmodels with and without attention.\nsize of 10 (Wiseman and Rush, 2016).\n4.1.2 Results\nTable 1 shows the BLEU scores of our model on differ-\nent sequence lengths while varyingK. This is a study\nof the trade-off between computational time and rep-\nresentational power. A largeKallows us to compute\ncomplex source representations, while aKof 1 limits\nthe source representation to a single vector. We can\nsee that performance consistently increases withKup\nto a point that depends on the data length, with longer\nsequences requiring more complex representations.\nThe results with and without position encodings are\nalmost identical on the toy data. Our technique learns\nto fit the data as well as the standard attention mecha-\nnism despite having less representational power. Both\nbeat the non-attention baseline by a significant margin.\n\nK\nK\n(a) Training curves for en-fi\nK\nK (b) Training curves for en-tr\nFigure 4: Comparing training curves for en-fi and en-tr with sigmoid encoder scoring and softmax decoder\nscoring and position encoding. Note that en-tr curves converged very quickly.\nModel Dataset K en-cs en-de en-fi en-tr\nMemory Attention Test 32 19.37 28.82 15.87 -\n64 19.65 29.53 16.49 -\nV alid 32 19.20 26.20 15.90 12.94\n64 19.63 26.39 16.35 13.06\nMemory Attention + PETest 32 19.45 29.53 15.86 -\n64 20.36 30.61 17.03 -\nV alid 32 19.35 26.22 16.31 12.97\n64 19.73 27.31 16.91 13.25\nAttention Test - 19.19 30.99 17.34 -\nV alid - 18.61 28.13 17.16 13.76\nTable 2: BLEU scores on WMT\u201917 translation datasets from the memory attention models and regular attention\nbaselines. We picked the best out of the four scoring function combinations on the validation set. Note that\nen-tr does not have an official test set. Best test scores on each dataset are highlighted.\nModel Decoding Time (s)\nK=32 26.85\nK=64 27.13\nAttention 33.28\nTable 3: Decoding time, averaged across 10 runs, for\nthe en-de validation set (2169 examples) with average\nsequence length of 35. Results are similar for both\nPE and non-PE models.\nvocabulary of 16k. We measured decoding time over\nthe full validation set, not including time used for\nmodel setup and data loading, averaged across 10 runs.\nThe average sequence length for examples in this data\nwas 35, and we expect more significant speedups\nfor tasks with longer sequences, as suggested by our\nexperiments on toy data. Note that in our NMT ex-\namples/experiments,K\u2248T, but we obtain computa-\ntional savings from the fact thatK\u226aD. We may be\nable to setK\u226aT, as in toy copying, and still get very\ngood performance in other tasks. For instance, in sum-\nmarization the source is complex but the representa-\ntion of the source required to perform the task is \u201dsim-\nple\u201d (i.e. all that is needed to generate the abstract).\nFigure 5 shows the effect of using sigmoid and\nsoftmax function in the encoders and decoders. We\nfound that softmax/softmax consistently performs\nbadly, while all other combinations perform about\nequally well. We report results for the best combi-\nnation only (as chosen on the validation set), but we\nfound this choice to only make a minor difference.\n\n0 50 100 150 200\nsteps (k)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5log perplexity\nK=1\nK=4\nK=16\nK=32\nK=64\nattention\nno attention\n(a) Comparison of varyingK for copying sequences of length\n200 on evaluation data, showing that largeK leads to faster\nconvergence and smallK performs similarly to the non-attentional\nbaseline.\n0 50 100 150 200\nsteps (k)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5log perplexity\nsigmoid/sigmoid\nsigmoid/softmax\nsoftmax/sigmoid\nsoftmax/softmax\n(b) Comparison of sigmoid and softmax functions for choosing the\nencoder and decoder attention scores on evaluation data, showing\nthat choice of gating/normalization matters.\nFigure 3: Training Curves for the Toy Copy task\nThat we are able to represent the source sequence\nwith a fixed size matrix with fewer than|S|rows\nsuggests that traditional attention mechanisms\nmay be representing the source with redundancies\nand wasting computational resources. This makes\nintuitive sense for the toy task, which should require\na relatively simple representation.\nThe last column shows that our technique signif-\nicantly speeds up the inference process. The gap in\ninference speed increases as sequences become longer.\nWe measured inference time on the full validation\nset of 1,000 examples, not including data loading or\nmodel construction times.\nFigure 3a shows the learning curves for sequence\nlength 200. We see thatK=1 is unable to fit the data\ndistribution, whileK\u2208{32,64}fits the data almost as\nquickly as the attention-based model. Figure 3b shows\nthe effect of varying the encoder and decoder scoring\nfunctions between softmax and sigmoid. All combina-\ntions manage to fit the data, but some converge faster\nthan others. In section 5 we show that distinct align-\nments are learned by different function combinations.\n4.2 Machine Translation\nNext, we explore if the memory-based attention\nmechanism is able to fit complex real-world datasets.\nFor this purpose we use 4 large machine translation\ndatasets of WMT\u2019175 on the following language\npairs: English-Czech (en-cs, 52M examples), English-\nGerman (en-de, 5.9M examples), English-Finish\n(en-fi, 2.6M examples), and English-Turkish (en-tr,\n207,373 examples). We used the newly available pre-\n5statmt.org/wmt17/translation-task.html\nprocessed datasets for the WMT\u201917 task.6 Note that\nour scores may not be directly comparable to other\nwork that performs their own data pre-processing. We\nlearn shared vocabularies of 16,000 subword units\nusing the BPE algorithm (Sennrich et al., 2016).\nWe use newstest2015 as a validation set, and report\nBLEU on newstest2016.\n4.2.1 Training Setup\nWe use a similar setup to the Toy Copy task, but\nuse 512 RNN and embedding units, train using 8\ndistributed workers with 1 GPU each, and train for\nat most 1M steps. We save checkpoints every 30\nminutes during training, and choose the best based\non the validation BLEU score.\n4.2.2 Results\nTable 2 compares our approach with and without\nposition encodings, and with varying values for\nhyperparameterK, to baseline models with regular\nattention mechanism. Learning curves are shown in\nFigure 4. We see that our memory attention model\nwith sufficiently highK performs on-par with, or\nslightly better, than the attention-based baseline model\ndespite its simpler nature. Across the board, models\nwith K = 64performed better than corresponding\nmodels withK= 32, suggesting that using a larger\nnumber of attention vectors can capture a richer under-\nstanding of source sequences. Position encodings also\nseem to consistently improve model performance.\nTable 3 shows that our model results in faster de-\ncoding time even on a complex dataset with a large\n6http://data.statmt.org/wmt17/translation-task/preprocessed",
            0.48414691600000026
          ]
        }
      ],
      "reference_answers": [
        "standard parametrized attention and a non-attention baseline",
        "Sequence Copy Task and WMT'17"
      ]
    },
    "1706.09673.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.018680918212239602,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.14460784313725492,
          "recall": 0.2536231884057971,
          "f1": 0.13765182186234817
        },
        "rouge-2": {
          "precision": 0.06666666666666667,
          "recall": 0.04545454545454545,
          "f1": 0.05405405405405405
        },
        "rouge-L": {
          "precision": 0.12377450980392157,
          "recall": 0.23913043478260868,
          "f1": 0.12055780476833107
        }
      },
      "per_question_metrics": [
        {
          "question": "Which dataset do they use?",
          "generated_answer": "The context does not specify a particular dataset used for the studies or models mentioned.",
          "reference_answer": " Paraphrase Database (PPDB) ,  book corpus",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            }
          },
          "bleu_score": 0,
          "exact_match": 0.0
        },
        {
          "question": "Do they evaluate their learned representations on downstream tasks?",
          "generated_answer": "No, the context does not specifically mention that they evaluate their learned tweet representations on downstream tasks.",
          "reference_answer": "No No",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.058823529411764705,
              "recall": 0.5,
              "f1": 0.10526315789473684
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.058823529411764705,
              "recall": 0.5,
              "f1": 0.10526315789473684
            }
          },
          "bleu_score": 0.010182425646195498,
          "exact_match": 0.0
        },
        {
          "question": "How do they encourage understanding of literature as part of their objective function?",
          "generated_answer": "They organize the models based on the objective function which aids the understanding of the literature.",
          "reference_answer": "They group the existing works in terms of the objective function they optimize - within-tweet relationships, inter-tweet relationships, autoencoder, and weak supervision.",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.375,
              "recall": 0.2608695652173913,
              "f1": 0.30769230769230765
            },
            "rouge-2": {
              "precision": 0.2,
              "recall": 0.13636363636363635,
              "f1": 0.16216216216216214
            },
            "rouge-L": {
              "precision": 0.3125,
              "recall": 0.21739130434782608,
              "f1": 0.2564102564102564
            }
          },
          "bleu_score": 0.0458603289905233,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "Which dataset do they use?",
        "Do they evaluate their learned representations on downstream tasks?",
        "How do they encourage understanding of literature as part of their objective function?"
      ],
      "generated_answers": [
        {
          "answer": "The context does not specify a particular dataset used for the studies or models mentioned.",
          "relevant_chunks": [
            "References\nSanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.\nA simple but tough-to-beat baseline for sentence em-\nbeddings. CoRR .\nYoshua Bengio. 2013a. Deep Learning of Represen-\ntations: Looking Forward. In Proc. of the 1st Intl.\nConf. on Statistical Language and Speech Process-\ning. pages 1\u201337.\nYoshua Bengio, Aaron C. Courville, and Pascal Vin-\ncent. 2013b. Representation Learning: A Review\nand New Perspectives. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence 35(8):1798\u2013\n1828.\nDavid M. Blei, Andrew Y . Ng, and Michael I. Jordan.\n2003. Latent Dirichlet Allocation. Journal of Ma-\nchine Learning Research 3:993\u20131022.\nTao Chen, Dongyuan Lu, Min-Yen Kan, and Peng Cui.\n2013. Understanding and classifying image tweets.\nIn ACM Multimedia Conference, MM \u201913 . pages\n781\u2013784.\nBhuwan Dhingra, Zhong Zhou, Dylan Fitzpatrick,\nMichael Muehl, and William W. Cohen. 2016.\nTweet2Vec: Character-Based Distributed Represen-\ntations for Social Media. In Proc. of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics.\nC\u00b4\u0131cero Nogueira dos Santos and Maira Gatti. 2014.\nDeep convolutional neural networks for sentiment\nanalysis of short texts. In 25th Intl. Conf. on Com-\nputational Linguistics. pages 69\u201378.\nJ Ganesh, Manish Gupta, and Vasudeva Varma. 2016.\nInterpreting the syntactic and social elements of the\ntweet representations via elementary property pre-\ndiction tasks. CoRR abs/1611.04887.\nJ Ganesh, Manish Gupta, and Vasudeva Varma. 2017.\nImproving tweet representations using temporal and\nuser context. In Advances in Information Retrieval\n- 39th European Conference on IR Research, ECIR.\npages 575\u2013581.\nJuri Ganitkevitch, Benjamin Van Durme, and Chris\nCallison-Burch. 2013. PPDB: the paraphrase\ndatabase. In Proc. of conf. of the North American\nChapter of the Association of Computational Lin-\nguistics. pages 758\u2013764.\nAnastasia Giachanou and Fabio Crestani. 2016. Like\nit or not: A Survey of Twitter Sentiment Anal-\nysis Methods. ACM Computing Surveys (CSUR)\n49(2):28.\nZellig S Harris. 1954. Distributional Structure. Word\n10(2-3):146\u2013162.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning Distributed Representations of Sentences\nfrom Unlabelled Data. In Proc. of the Conf. of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies. pages 1367\u20131377.\nPo-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\nAlex Acero, and Larry P. Heck. 2013. Learning\nDeep Structured Semantic Models for Web Search\nusing Clickthrough Data. In Proc. of the 22nd ACM\nIntl. Conf. on Information and Knowledge Manage-\nment. pages 2333\u20132338.\nAndrej Karpathy, Justin Johnson, and Fei-Fei Li. 2015.\nVisualizing and understanding recurrent networks.\nCoRR abs/1506.02078.\nTom Kenter, Alexey Borisov, and Maarten de Rijke.\n2016. Siamese CBOW: Optimizing Word Embed-\ndings for Sentence Representations. In Proc. of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics.\nDouwe Kiela, Felix Hill, Anna Korhonen, and Stephen\nClark. 2014. Improving multi-modal representa-\ntions using image dispersion: Why less is some-\ntimes more. In Proc. of the 52nd Annual Meet-\ning of the Association for Computational Linguis-\ntics. pages 835\u2013841.\nYoon Kim. 2014. Convolutional Neural Networks for\nSentence Classi\ufb01cation. In Proc. of the 2014 Conf.\non Empirical Methods in Natural Language Pro-\ncessing. pages 1746\u20131751.\nRyan Kiros, Yukun Zhu, Ruslan Salakhutdinov,\nRichard S. Zemel, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. 2015. Skip-Thought Vec-\ntors. In Proc. of the 2015 Intl. Conf. on Advances\nin Neural Information Processing Systems . pages\n3294\u20133302.\nAngeliki Lazaridou, Nghia The Pham, and Marco Ba-\nroni. 2015. Combining language and vision with a\nmultimodal skip-gram model. In Proc. of conf. of\nthe North American Chapter of the Association of\nComputational Linguistics. pages 153\u2013163.\nQuoc V . Le and Tomas Mikolov. 2014. Distributed\nRepresentations of Sentences and Documents. In\nProc. of the 31st Intl. Conf. on Machine Learning .\npages 1188\u20131196.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013. Distributed Repre-\nsentations of Words and Phrases and Their Compo-\nsitionality. In Proc. of the 26th Intl. Conf. on Neural\nInformation Processing Systems. pages 3111\u20133119.\nTamara Polajnar, Laura Rimell, and Stephen Clark.\n2015. An exploration of discourse-based sentence\nspaces for compositional distributional semantics.\nIn Workshop on Linking Models of Lexical, Sen-\ntential and Discourse-level Semantics (LSDSem) .\npage 1.\n\nYafeng Ren, Yue Zhang, Meishan Zhang, and\nDonghong Ji. 2016. Context-sensitive twitter senti-\nment classi\ufb01cation using neural network. In Proc. of\nthe 13th AAAI Conference on Arti\ufb01cial Intelligence.\npages 215\u2013221.\nYelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,\nand Gr \u00b4egoire Mesnil. 2014. A Latent Semantic\nModel with Convolutional-Pooling Structure for In-\nformation Retrieval. In Proc. of the 23rd ACM Intl.\nConf. on Conference on Information and Knowledge\nManagement. pages 101\u2013110.\nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston,\nand Rob Fergus. 2015. End-to-end memory net-\nworks. In NIPS. pages 2440\u20132448.\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014.\nSequence to sequence learning with neural net-\nworks. In NIPS. pages 3104\u20133112.\nDuyu Tang, Furu Wei, Bing Qin, Nan Yang, Ting\nLiu, and Ming Zhou. 2016. Sentiment Embeddings\nwith Applications to Sentiment Analysis. IEEE\nTransactions on Knowledge and Data Engineering\n28(2):496\u2013509.\nSoroush V osoughi, Prashanth Vijayaraghavan, and Deb\nRoy. 2016. Tweet2Vec: Learning Tweet Embed-\ndings Using Character-level CNN-LSTM Encoder-\nDecoder. In Proc. of the39th Intl. ACM SIGIR Conf.\non Research and Development in Information Re-\ntrieval. pages 1041\u20131044.\nYashen Wang, Heyan Huang, Chong Feng, Qiang\nZhou, Jiahui Gu, and Xiong Gao. 2016. CSE:\nconceptual sentence embeddings based on attention\nmodel. In Proc. of the 54th Annual Meeting of the\nAssociation for Computational Linguistics.\nZhiyu Wang, Peng Cui, Lexing Xie, Wenwu Zhu,\nYong Rui, and Shiqiang Yang. 2014. Bilateral cor-\nrespondence model for words-and-pictures associ-\nation in multimedia-rich microblogs. TOMCCAP\n10(4):34:1\u201334:21.\nJason Weston, Sumit Chopra, and Keith Adams. 2014.\n#tagspace: Semantic embeddings from hashtags. In\nProc. of the 2014 Conf. on Empirical Methods in\nNatural Language Processing. pages 1822\u20131827.\nJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen\nLivescu. 2016a. Charagram: Embedding words and\nsentences via character n-grams. In Proc. of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP. pages 1504\u20131515.\nJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen\nLivescu. 2016b. Towards universal paraphrastic\nsentence embeddings. CoRR abs/1511.08198.\nWei Xu, Alan Ritter, Chris Callison-Burch, William B.\nDolan, and Yangfeng Ji. 2014. Extracting lexically\ndivergent paraphrases from twitter. TACL 2:435\u2013\n448.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In IEEE Intl. Conf. on\nComputer Vision, ICCV. pages 19\u201327.\n\nrecover the original data from the corrupted ver-\nsion. SDAE produces robust representations by\nlearning to represent the data in terms of fea-\ntures that explain its important factors of varia-\ntion. Tweet2Vec (V osoughi et al., 2016) is a recent\nmodel which uses a character-level CNN-LSTM\nencoder-decoder architecture trained to construct\nthe input tweet directly. This model outper-\nforms competitive models that work on word-level\nlike PV-DM, PV-DBOW on semantic similarity\ncomputation and sentiment classi\ufb01cation tasks,\nthereby showing that the character-level nature of\nTweet2Vec is best-suited to deal with the noise and\nidiosyncrasies of tweets. Tweet2Vec controls the\ngeneralization error by using a data augmentation\ntechnique, wherein tweets are replicated and some\nof the words in the replicated tweets are replaced\nwith their synonyms. Both SDAE and Tweet2Vec\nhas the advantage that they don\u2019t need a coherent\ninter-sentence narrative (like STV), which is hard\nto obtain in Twitter.\n2.5 Modeling using weak supervision\nMotivation: In a weakly supervised setup, we\ncreate labels for a tweet automatically and pre-\ndict them to learn potentially sophisticated mod-\nels than those obtained by unsupervised learning\nalone. Examples of labels include sentiment of\nthe overall tweet, words like hashtag present in\nthe tweet and so on. This technique can create a\nhuge labeled dataset especially for building data-\nhungry, sophisticated deep learning models.\nModels: (Tang et al., 2016) learns sentiment-\nspeci\ufb01c word embedding (SSWE), which encodes\nthe polarity information in the word representa-\ntions so that words with contrasting polarities and\nsimilar syntactic context (like good and bad) are\npushed away from each other in the semantic\nspace that it learns. SSWE utilizes the massive\ndistant-supervised tweets collected by positive and\nnegative emoticons to build a powerful tweet rep-\nresentation, which are shown to be useful in tasks\nsuch as sentiment classi\ufb01cation and word simi-\nlarity computation in sentiment lexicon. (Dhin-\ngra et al., 2016) observes that hashtags in tweets\ncan be considered as topics and hence tweets with\nsimilar hashtags must come closer to each other.\nTheir model predicts the hashtags by using a Bi-\nGRU layer to embed the tweets from its charac-\nters. Due to subword modeling, such character-\nlevel models can approximate the representations\nfor rare words and new words (words not seen dur-\ning training) in the test tweets really well. This\nmodel outperforms the word-level baselines for\nhashtag prediction task, thereby concluding that\nexploring character-level models for tweets is a\nworthy research direction to pursue. Both these\nworks fail to study the model\u2019s generality (Weston\net al., 2014), i.e., the ability of the model to trans-\nfer the learned representations to diverse tasks.\n3 Future Directions\nIn this section we present the future research di-\nrections which we believe can be worth pursuing\nto generate high quality tweet embeddings.\n\u2022 (Ren et al., 2016) propose a supervised\nneural network utilizing contextualized fea-\ntures from conversation, author and topic\nbased context about a target tweet to per-\nform well in classi\ufb01cation of tweet. Apart\nfrom (Ganesh et al., 2017)\u2019s work which uti-\nlizes author context, there is no other work\nwhich builds unsupervised tweet representa-\ntion model on Twitter-speci\ufb01c contexts such\nas conversation and topical streams. We be-\nlieve such a solution directly exploits seman-\ntic signals (or nuances) from Twitter, unlike\nSTV or Siamese CBOW which are trained on\nbooks corpus.\n\u2022 (dos Santos and Gatti, 2014) propose a super-\nvised, hybrid model exploiting both the char-\nacter and word level information for Twit-\nter sentiment analysis task. Since the set-\ntings when the character level model beats the\nword level model is not well understood yet,\nwe believe it would be interesting to explore\nsuch a hybrid compositional model to build\nunsupervised tweet representations.\n\u2022 Twitter provides a platform for the users to\ninteract with other users. To the best of our\nknowledge, there is no related work that com-\nputes unsupervised tweet representation by\nexploiting the user pro\ufb01le attributes like pro-\n\ufb01le picture, user biography and set of fol-\nlowers, and social interactions like retweet\ncontext (set of surrounding tweets in a users\nretweet stream) and favorite context (set of\nsurrounding tweets in a users favorite tweet\nstream).",
            0.49577166699999964
          ]
        },
        {
          "answer": "No, the context does not specifically mention that they evaluate their learned tweet representations on downstream tasks.",
          "relevant_chunks": [
            "\u2022 DSSM (Huang et al., 2013; Shen et al., 2014)\npropose a family of deep models that are\ntrained to maximize the relevance of clicked\ndocuments given a query. Such a ranking loss\nfunction helps the model cater to a wide vari-\nety of applications1 such as web search rank-\ning, ad selection/relevance, question answer-\ning, knowledge inference and machine trans-\nlation. We observe such a loss function has\nnot been explored for building unsupervised\ntweet representations. We believe employing\na ranking loss directly on tweets using a large\nscale microblog dataset 2 can result in repre-\nsentations which can be useful to Twitter ap-\nplications beyond those studied in the tweet\nrepresentation learning literature.\n\u2022 Linguists assume that language is best un-\nderstood as a hierarchical tree of phrases,\nrather than a \ufb02at sequence of words or charac-\nters. It\u2019s dif\ufb01cult to get the syntactic trees for\ntweets as most of them are not grammatically\ncorrect. The average of word vectors model\nhas the most simplest compositional archi-\ntecture with no additional parameters, yet\ndisplays a strong performance outperforming\ncomplex architectures such as CNN, LSTM\nand so on for several downstream applica-\ntions (Wieting et al., 2016a,b). We believe\na theoretical understanding of why word av-\neraging models perform well can help in em-\nbracing these models by linguists.\n\u2022 Models in (Wieting et al., 2016a,b) learn\nfrom noisy phrase pairs of PPDB. Note that\nthe source of the underlying texts is com-\npletely different from Twitter. It can be inter-\nesting to see the effectiveness of such mod-\nels when directly trained on structural re-\nsources from Twitter like Twitter Paraphrase\nCorpus (Xu et al., 2014). The main challenge\nwith this approach is the small size of the an-\nnotated Twitter resources, which can encour-\nage models like (Arora et al., 2017) that work\nwell even when the training data is scarce or\nnonexistent.\n\u2022 Tweets mostly have an accompanying im-\nage which sometimes has visual correspon-\ndence with its textual content (Chen et al.,\n1https://www.microsoft.com/en-us/\nresearch/project/dssm/\n2http://trec.nist.gov/\n2013; Wang et al., 2014) (\u2018visual\u2019 tweet).\nTo the best of our knowledge, there is no\nwork which explores the following question:\ncan we build multimodal representations for\ntweets accompanying correlated visual con-\ntent and compare with traditional bench-\nmarks?. We can leverage insights from mul-\ntimodal skip-gram model (Lazaridou et al.,\n2015) which builds multimodally-enhanced\nword vectors that perform well in the tradi-\ntional semantic benchmarks. However, it\u2019s\nhard to detect visual tweets and learning from\na non-visual tweet can degrade its tweet rep-\nresentation. It would be interesting to see if\na dispersion metric (Kiela et al., 2014) for\ntweets can be explored to overcome this prob-\nlem of building a nondegradable, improved\ntweet representation.\n\u2022 Interpreting the tweet representations to un-\nearth the encoded features responsible for\nits performance on a downstream task is an\nimportant, but a less studied research area.\n(Ganesh et al., 2016)\u2019s work is the \ufb01rst to\nopen the blackbox of vector embeddings for\ntweets. They propose elementary property\nprediction tasks which predicts the accuracy\nto which a given tweet representation en-\ncodes the elementary property (like slang\nwords, hashtags, mentions, etc). The main\ndrawback of the work is that they fail to cor-\nrelate their study with downstream applica-\ntions. We believe performing such a correla-\ntion study can clearly highlight the set of el-\nementary features behind the performance of\na particular representation model over other\nfor a given downstream task.\n4 Conclusion\nIn this work we study the problem of learning\nunsupervised tweet representations. We believe\nour survey of the existing works based on the ob-\njective function can give vital perspectives to re-\nsearchers and aid their understanding of the \ufb01eld.\nWe also believe the future research directions stud-\nied in this work can help in breaking the barriers in\nbuilding high quality, general purpose tweet repre-\nsentation models.\n\nImproving Distributed Representations of Tweets - Present and Future\nGanesh J\nInformation Retrieval and Extraction Laboratory\nIIIT Hyderabad\nTelangana, India\nganesh.j@research.iiit.ac.in\nAbstract\nUnsupervised representation learning for\ntweets is an important research \ufb01eld which\nhelps in solving several business applica-\ntions such as sentiment analysis, hashtag\nprediction, paraphrase detection and mi-\ncroblog ranking. A good tweet represen-\ntation learning model must handle the id-\niosyncratic nature of tweets which poses\nseveral challenges such as short length, in-\nformal words, unusual grammar and mis-\nspellings. However, there is a lack of\nprior work which surveys the represen-\ntation learning models with a focus on\ntweets. In this work, we organize the mod-\nels based on its objective function which\naids the understanding of the literature.\nWe also provide interesting future direc-\ntions, which we believe are fruitful in ad-\nvancing this \ufb01eld by building high-quality\ntweet representation learning models.\n1 Introduction\nTwitter is a widely used microblogging platform,\nwhere users post and interact with messages,\n\u201ctweets\u201d. Understanding the semantic represen-\ntation of tweets can bene\ufb01t a plethora of applica-\ntions such as sentiment analysis (Ren et al., 2016;\nGiachanou and Crestani, 2016), hashtag predic-\ntion (Dhingra et al., 2016), paraphrase detec-\ntion (V osoughi et al., 2016) and microblog rank-\ning (Huang et al., 2013; Shen et al., 2014). How-\never, tweets are dif\ufb01cult to model as they pose\nseveral challenges such as short length, informal\nwords, unusual grammar and misspellings. Re-\ncently, researchers are focusing on leveraging un-\nsupervised representation learning methods based\non neural networks to solve this problem. Once\nthese representations are learned, we can use off-\nthe-shelf predictors taking the representation as in-\nput to solve the downstream task (Bengio, 2013a;\nBengio et al., 2013b). These methods enjoy sev-\neral advantages: (1) they are cheaper to train, as\nthey work with unlabelled data, (2) they reduce the\ndependence on domain level experts, and (3) they\nare highly effective across multiple applications,\nin practice.\nDespite this, there is a lack of prior work which\nsurveys the tweet-speci\ufb01c unsupervised represen-\ntation learning models. In this work, we attempt to\n\ufb01ll this gap by investigating the models in an or-\nganized fashion. Speci\ufb01cally, we group the mod-\nels based on the objective function it optimizes.\nWe believe this work can aid the understanding\nof the existing literature. We conclude the pa-\nper by presenting interesting future research direc-\ntions, which we believe are fruitful in advancing\nthis \ufb01eld by building high-quality tweet represen-\ntation learning models.\n2 Unsupervised Tweet Representation\nModels\nThere are various models spanning across differ-\nent model architectures and objective functions in\nthe literature to compute tweet representation in\nan unsupervised fashion. These models work in a\nsemi-supervised way - the representations gener-\nated by the model is fed to an off-the-shelf pre-\ndictor like Support Vector Machines (SVM) to\nsolve a particular downstream task. These mod-\nels span across a wide variety of neural network\nbased architectures including average of word vec-\ntors, convolutional-based, recurrent-based and so\non. We believe that the performance of these mod-\nels is highly dependent on the objective function\nit optimizes \u2013 predicting adjacent word (within-\ntweet relationships), adjacent tweet (inter-tweet\nrelationships), the tweet itself (autoencoder), mod-\narXiv:1706.09673v1  [cs.CL]  29 Jun 2017\n\nrecover the original data from the corrupted ver-\nsion. SDAE produces robust representations by\nlearning to represent the data in terms of fea-\ntures that explain its important factors of varia-\ntion. Tweet2Vec (V osoughi et al., 2016) is a recent\nmodel which uses a character-level CNN-LSTM\nencoder-decoder architecture trained to construct\nthe input tweet directly. This model outper-\nforms competitive models that work on word-level\nlike PV-DM, PV-DBOW on semantic similarity\ncomputation and sentiment classi\ufb01cation tasks,\nthereby showing that the character-level nature of\nTweet2Vec is best-suited to deal with the noise and\nidiosyncrasies of tweets. Tweet2Vec controls the\ngeneralization error by using a data augmentation\ntechnique, wherein tweets are replicated and some\nof the words in the replicated tweets are replaced\nwith their synonyms. Both SDAE and Tweet2Vec\nhas the advantage that they don\u2019t need a coherent\ninter-sentence narrative (like STV), which is hard\nto obtain in Twitter.\n2.5 Modeling using weak supervision\nMotivation: In a weakly supervised setup, we\ncreate labels for a tweet automatically and pre-\ndict them to learn potentially sophisticated mod-\nels than those obtained by unsupervised learning\nalone. Examples of labels include sentiment of\nthe overall tweet, words like hashtag present in\nthe tweet and so on. This technique can create a\nhuge labeled dataset especially for building data-\nhungry, sophisticated deep learning models.\nModels: (Tang et al., 2016) learns sentiment-\nspeci\ufb01c word embedding (SSWE), which encodes\nthe polarity information in the word representa-\ntions so that words with contrasting polarities and\nsimilar syntactic context (like good and bad) are\npushed away from each other in the semantic\nspace that it learns. SSWE utilizes the massive\ndistant-supervised tweets collected by positive and\nnegative emoticons to build a powerful tweet rep-\nresentation, which are shown to be useful in tasks\nsuch as sentiment classi\ufb01cation and word simi-\nlarity computation in sentiment lexicon. (Dhin-\ngra et al., 2016) observes that hashtags in tweets\ncan be considered as topics and hence tweets with\nsimilar hashtags must come closer to each other.\nTheir model predicts the hashtags by using a Bi-\nGRU layer to embed the tweets from its charac-\nters. Due to subword modeling, such character-\nlevel models can approximate the representations\nfor rare words and new words (words not seen dur-\ning training) in the test tweets really well. This\nmodel outperforms the word-level baselines for\nhashtag prediction task, thereby concluding that\nexploring character-level models for tweets is a\nworthy research direction to pursue. Both these\nworks fail to study the model\u2019s generality (Weston\net al., 2014), i.e., the ability of the model to trans-\nfer the learned representations to diverse tasks.\n3 Future Directions\nIn this section we present the future research di-\nrections which we believe can be worth pursuing\nto generate high quality tweet embeddings.\n\u2022 (Ren et al., 2016) propose a supervised\nneural network utilizing contextualized fea-\ntures from conversation, author and topic\nbased context about a target tweet to per-\nform well in classi\ufb01cation of tweet. Apart\nfrom (Ganesh et al., 2017)\u2019s work which uti-\nlizes author context, there is no other work\nwhich builds unsupervised tweet representa-\ntion model on Twitter-speci\ufb01c contexts such\nas conversation and topical streams. We be-\nlieve such a solution directly exploits seman-\ntic signals (or nuances) from Twitter, unlike\nSTV or Siamese CBOW which are trained on\nbooks corpus.\n\u2022 (dos Santos and Gatti, 2014) propose a super-\nvised, hybrid model exploiting both the char-\nacter and word level information for Twit-\nter sentiment analysis task. Since the set-\ntings when the character level model beats the\nword level model is not well understood yet,\nwe believe it would be interesting to explore\nsuch a hybrid compositional model to build\nunsupervised tweet representations.\n\u2022 Twitter provides a platform for the users to\ninteract with other users. To the best of our\nknowledge, there is no related work that com-\nputes unsupervised tweet representation by\nexploiting the user pro\ufb01le attributes like pro-\n\ufb01le picture, user biography and set of fol-\nlowers, and social interactions like retweet\ncontext (set of surrounding tweets in a users\nretweet stream) and favorite context (set of\nsurrounding tweets in a users favorite tweet\nstream).",
            1.11778104199999
          ]
        },
        {
          "answer": "They organize the models based on the objective function which aids the understanding of the literature.",
          "relevant_chunks": [
            "\u2022 DSSM (Huang et al., 2013; Shen et al., 2014)\npropose a family of deep models that are\ntrained to maximize the relevance of clicked\ndocuments given a query. Such a ranking loss\nfunction helps the model cater to a wide vari-\nety of applications1 such as web search rank-\ning, ad selection/relevance, question answer-\ning, knowledge inference and machine trans-\nlation. We observe such a loss function has\nnot been explored for building unsupervised\ntweet representations. We believe employing\na ranking loss directly on tweets using a large\nscale microblog dataset 2 can result in repre-\nsentations which can be useful to Twitter ap-\nplications beyond those studied in the tweet\nrepresentation learning literature.\n\u2022 Linguists assume that language is best un-\nderstood as a hierarchical tree of phrases,\nrather than a \ufb02at sequence of words or charac-\nters. It\u2019s dif\ufb01cult to get the syntactic trees for\ntweets as most of them are not grammatically\ncorrect. The average of word vectors model\nhas the most simplest compositional archi-\ntecture with no additional parameters, yet\ndisplays a strong performance outperforming\ncomplex architectures such as CNN, LSTM\nand so on for several downstream applica-\ntions (Wieting et al., 2016a,b). We believe\na theoretical understanding of why word av-\neraging models perform well can help in em-\nbracing these models by linguists.\n\u2022 Models in (Wieting et al., 2016a,b) learn\nfrom noisy phrase pairs of PPDB. Note that\nthe source of the underlying texts is com-\npletely different from Twitter. It can be inter-\nesting to see the effectiveness of such mod-\nels when directly trained on structural re-\nsources from Twitter like Twitter Paraphrase\nCorpus (Xu et al., 2014). The main challenge\nwith this approach is the small size of the an-\nnotated Twitter resources, which can encour-\nage models like (Arora et al., 2017) that work\nwell even when the training data is scarce or\nnonexistent.\n\u2022 Tweets mostly have an accompanying im-\nage which sometimes has visual correspon-\ndence with its textual content (Chen et al.,\n1https://www.microsoft.com/en-us/\nresearch/project/dssm/\n2http://trec.nist.gov/\n2013; Wang et al., 2014) (\u2018visual\u2019 tweet).\nTo the best of our knowledge, there is no\nwork which explores the following question:\ncan we build multimodal representations for\ntweets accompanying correlated visual con-\ntent and compare with traditional bench-\nmarks?. We can leverage insights from mul-\ntimodal skip-gram model (Lazaridou et al.,\n2015) which builds multimodally-enhanced\nword vectors that perform well in the tradi-\ntional semantic benchmarks. However, it\u2019s\nhard to detect visual tweets and learning from\na non-visual tweet can degrade its tweet rep-\nresentation. It would be interesting to see if\na dispersion metric (Kiela et al., 2014) for\ntweets can be explored to overcome this prob-\nlem of building a nondegradable, improved\ntweet representation.\n\u2022 Interpreting the tweet representations to un-\nearth the encoded features responsible for\nits performance on a downstream task is an\nimportant, but a less studied research area.\n(Ganesh et al., 2016)\u2019s work is the \ufb01rst to\nopen the blackbox of vector embeddings for\ntweets. They propose elementary property\nprediction tasks which predicts the accuracy\nto which a given tweet representation en-\ncodes the elementary property (like slang\nwords, hashtags, mentions, etc). The main\ndrawback of the work is that they fail to cor-\nrelate their study with downstream applica-\ntions. We believe performing such a correla-\ntion study can clearly highlight the set of el-\nementary features behind the performance of\na particular representation model over other\nfor a given downstream task.\n4 Conclusion\nIn this work we study the problem of learning\nunsupervised tweet representations. We believe\nour survey of the existing works based on the ob-\njective function can give vital perspectives to re-\nsearchers and aid their understanding of the \ufb01eld.\nWe also believe the future research directions stud-\nied in this work can help in breaking the barriers in\nbuilding high quality, general purpose tweet repre-\nsentation models.\n\nImproving Distributed Representations of Tweets - Present and Future\nGanesh J\nInformation Retrieval and Extraction Laboratory\nIIIT Hyderabad\nTelangana, India\nganesh.j@research.iiit.ac.in\nAbstract\nUnsupervised representation learning for\ntweets is an important research \ufb01eld which\nhelps in solving several business applica-\ntions such as sentiment analysis, hashtag\nprediction, paraphrase detection and mi-\ncroblog ranking. A good tweet represen-\ntation learning model must handle the id-\niosyncratic nature of tweets which poses\nseveral challenges such as short length, in-\nformal words, unusual grammar and mis-\nspellings. However, there is a lack of\nprior work which surveys the represen-\ntation learning models with a focus on\ntweets. In this work, we organize the mod-\nels based on its objective function which\naids the understanding of the literature.\nWe also provide interesting future direc-\ntions, which we believe are fruitful in ad-\nvancing this \ufb01eld by building high-quality\ntweet representation learning models.\n1 Introduction\nTwitter is a widely used microblogging platform,\nwhere users post and interact with messages,\n\u201ctweets\u201d. Understanding the semantic represen-\ntation of tweets can bene\ufb01t a plethora of applica-\ntions such as sentiment analysis (Ren et al., 2016;\nGiachanou and Crestani, 2016), hashtag predic-\ntion (Dhingra et al., 2016), paraphrase detec-\ntion (V osoughi et al., 2016) and microblog rank-\ning (Huang et al., 2013; Shen et al., 2014). How-\never, tweets are dif\ufb01cult to model as they pose\nseveral challenges such as short length, informal\nwords, unusual grammar and misspellings. Re-\ncently, researchers are focusing on leveraging un-\nsupervised representation learning methods based\non neural networks to solve this problem. Once\nthese representations are learned, we can use off-\nthe-shelf predictors taking the representation as in-\nput to solve the downstream task (Bengio, 2013a;\nBengio et al., 2013b). These methods enjoy sev-\neral advantages: (1) they are cheaper to train, as\nthey work with unlabelled data, (2) they reduce the\ndependence on domain level experts, and (3) they\nare highly effective across multiple applications,\nin practice.\nDespite this, there is a lack of prior work which\nsurveys the tweet-speci\ufb01c unsupervised represen-\ntation learning models. In this work, we attempt to\n\ufb01ll this gap by investigating the models in an or-\nganized fashion. Speci\ufb01cally, we group the mod-\nels based on the objective function it optimizes.\nWe believe this work can aid the understanding\nof the existing literature. We conclude the pa-\nper by presenting interesting future research direc-\ntions, which we believe are fruitful in advancing\nthis \ufb01eld by building high-quality tweet represen-\ntation learning models.\n2 Unsupervised Tweet Representation\nModels\nThere are various models spanning across differ-\nent model architectures and objective functions in\nthe literature to compute tweet representation in\nan unsupervised fashion. These models work in a\nsemi-supervised way - the representations gener-\nated by the model is fed to an off-the-shelf pre-\ndictor like Support Vector Machines (SVM) to\nsolve a particular downstream task. These mod-\nels span across a wide variety of neural network\nbased architectures including average of word vec-\ntors, convolutional-based, recurrent-based and so\non. We believe that the performance of these mod-\nels is highly dependent on the objective function\nit optimizes \u2013 predicting adjacent word (within-\ntweet relationships), adjacent tweet (inter-tweet\nrelationships), the tweet itself (autoencoder), mod-\narXiv:1706.09673v1  [cs.CL]  29 Jun 2017\n\nReferences\nSanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.\nA simple but tough-to-beat baseline for sentence em-\nbeddings. CoRR .\nYoshua Bengio. 2013a. Deep Learning of Represen-\ntations: Looking Forward. In Proc. of the 1st Intl.\nConf. on Statistical Language and Speech Process-\ning. pages 1\u201337.\nYoshua Bengio, Aaron C. Courville, and Pascal Vin-\ncent. 2013b. Representation Learning: A Review\nand New Perspectives. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence 35(8):1798\u2013\n1828.\nDavid M. Blei, Andrew Y . Ng, and Michael I. Jordan.\n2003. Latent Dirichlet Allocation. Journal of Ma-\nchine Learning Research 3:993\u20131022.\nTao Chen, Dongyuan Lu, Min-Yen Kan, and Peng Cui.\n2013. Understanding and classifying image tweets.\nIn ACM Multimedia Conference, MM \u201913 . pages\n781\u2013784.\nBhuwan Dhingra, Zhong Zhou, Dylan Fitzpatrick,\nMichael Muehl, and William W. Cohen. 2016.\nTweet2Vec: Character-Based Distributed Represen-\ntations for Social Media. In Proc. of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics.\nC\u00b4\u0131cero Nogueira dos Santos and Maira Gatti. 2014.\nDeep convolutional neural networks for sentiment\nanalysis of short texts. In 25th Intl. Conf. on Com-\nputational Linguistics. pages 69\u201378.\nJ Ganesh, Manish Gupta, and Vasudeva Varma. 2016.\nInterpreting the syntactic and social elements of the\ntweet representations via elementary property pre-\ndiction tasks. CoRR abs/1611.04887.\nJ Ganesh, Manish Gupta, and Vasudeva Varma. 2017.\nImproving tweet representations using temporal and\nuser context. In Advances in Information Retrieval\n- 39th European Conference on IR Research, ECIR.\npages 575\u2013581.\nJuri Ganitkevitch, Benjamin Van Durme, and Chris\nCallison-Burch. 2013. PPDB: the paraphrase\ndatabase. In Proc. of conf. of the North American\nChapter of the Association of Computational Lin-\nguistics. pages 758\u2013764.\nAnastasia Giachanou and Fabio Crestani. 2016. Like\nit or not: A Survey of Twitter Sentiment Anal-\nysis Methods. ACM Computing Surveys (CSUR)\n49(2):28.\nZellig S Harris. 1954. Distributional Structure. Word\n10(2-3):146\u2013162.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning Distributed Representations of Sentences\nfrom Unlabelled Data. In Proc. of the Conf. of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies. pages 1367\u20131377.\nPo-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\nAlex Acero, and Larry P. Heck. 2013. Learning\nDeep Structured Semantic Models for Web Search\nusing Clickthrough Data. In Proc. of the 22nd ACM\nIntl. Conf. on Information and Knowledge Manage-\nment. pages 2333\u20132338.\nAndrej Karpathy, Justin Johnson, and Fei-Fei Li. 2015.\nVisualizing and understanding recurrent networks.\nCoRR abs/1506.02078.\nTom Kenter, Alexey Borisov, and Maarten de Rijke.\n2016. Siamese CBOW: Optimizing Word Embed-\ndings for Sentence Representations. In Proc. of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics.\nDouwe Kiela, Felix Hill, Anna Korhonen, and Stephen\nClark. 2014. Improving multi-modal representa-\ntions using image dispersion: Why less is some-\ntimes more. In Proc. of the 52nd Annual Meet-\ning of the Association for Computational Linguis-\ntics. pages 835\u2013841.\nYoon Kim. 2014. Convolutional Neural Networks for\nSentence Classi\ufb01cation. In Proc. of the 2014 Conf.\non Empirical Methods in Natural Language Pro-\ncessing. pages 1746\u20131751.\nRyan Kiros, Yukun Zhu, Ruslan Salakhutdinov,\nRichard S. Zemel, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. 2015. Skip-Thought Vec-\ntors. In Proc. of the 2015 Intl. Conf. on Advances\nin Neural Information Processing Systems . pages\n3294\u20133302.\nAngeliki Lazaridou, Nghia The Pham, and Marco Ba-\nroni. 2015. Combining language and vision with a\nmultimodal skip-gram model. In Proc. of conf. of\nthe North American Chapter of the Association of\nComputational Linguistics. pages 153\u2013163.\nQuoc V . Le and Tomas Mikolov. 2014. Distributed\nRepresentations of Sentences and Documents. In\nProc. of the 31st Intl. Conf. on Machine Learning .\npages 1188\u20131196.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013. Distributed Repre-\nsentations of Words and Phrases and Their Compo-\nsitionality. In Proc. of the 26th Intl. Conf. on Neural\nInformation Processing Systems. pages 3111\u20133119.\nTamara Polajnar, Laura Rimell, and Stephen Clark.\n2015. An exploration of discourse-based sentence\nspaces for compositional distributional semantics.\nIn Workshop on Linking Models of Lexical, Sen-\ntential and Discourse-level Semantics (LSDSem) .\npage 1.",
            0.5358595000000008
          ]
        }
      ],
      "reference_answers": [
        " Paraphrase Database (PPDB) ,  book corpus",
        "No No",
        "They group the existing works in terms of the objective function they optimize - within-tweet relationships, inter-tweet relationships, autoencoder, and weak supervision."
      ]
    },
    "1706.08198.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.19941029595532586,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.40625,
          "recall": 0.5119047619047619,
          "f1": 0.39729729729729735
        },
        "rouge-2": {
          "precision": 0.26666666666666666,
          "recall": 0.2,
          "f1": 0.2285714285714286
        },
        "rouge-L": {
          "precision": 0.375,
          "recall": 0.4880952380952381,
          "f1": 0.37027027027027026
        }
      },
      "per_question_metrics": [
        {
          "question": "Is pre-training effective in their evaluation?",
          "generated_answer": "Yes, pre-training is effective in their evaluation.",
          "reference_answer": "Yes Yes",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.125,
              "recall": 0.5,
              "f1": 0.2
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.125,
              "recall": 0.5,
              "f1": 0.2
            }
          },
          "bleu_score": 0.023980296761827107,
          "exact_match": 0.0
        },
        {
          "question": "What parallel corpus did they use?",
          "generated_answer": "They used two parallel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) and NTCIR PatentMT Parallel Corpus.",
          "reference_answer": "Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0, NTCIR PatentMT Parallel Corpus BIBREF1 Asian Scientific Paper Excerpt Corpus, NTCIR PatentMT Parallel Corpus ",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.6875,
              "recall": 0.5238095238095238,
              "f1": 0.5945945945945946
            },
            "rouge-2": {
              "precision": 0.5333333333333333,
              "recall": 0.4,
              "f1": 0.4571428571428572
            },
            "rouge-L": {
              "precision": 0.625,
              "recall": 0.47619047619047616,
              "f1": 0.5405405405405405
            }
          },
          "bleu_score": 0.3748402951488246,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "Is pre-training effective in their evaluation?",
        "What parallel corpus did they use?"
      ],
      "generated_answers": [
        {
          "answer": "Yes, pre-training is effective in their evaluation.",
          "relevant_chunks": [
            "Example 1: Improvement in under-translation.\nInput the conditions under which the effect of turbulent viscosity is cor-\nrectly evaluated were examined on the basis of the relation between turbulent\nviscosity and numerical viscosity in size .\nBaseline-NMT \u4e71\u6d41\u7c98\u6027 \u306e\u5f71\u97ff\u3092\u6b63\u78ba\u306b\u8a55\u4fa1\u3059\u308b\u6761\u4ef6\u3092\u691c\u8a0e\u3057\u305f\u3002\n+Reconstructor \u4e71\u6d41\u7c98\u6027 \u306e\u5f71\u97ff\u3092\u6b63\u78ba\u306b\u8a55\u4fa1\u3059\u308b\u6761\u4ef6\u3092, \u4e71 \u4e71 \u4e71\u6d41 \u6d41 \u6d41\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\u3068 \u3068 \u3068\u6570 \u6570 \u6570\u5024 \u5024 \u5024 \u7684 \u7684 \u7684\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\n\u306e \u306e \u306e\u95a2 \u95a2 \u95a2\u4fc2 \u4fc2 \u4fc2\u3092 \u3092 \u3092\u57fa \u57fa \u57fa\u306b \u306b \u306b\u8abf\u3079\u305f \u3002\n+Reconstructor \u4e71\u6d41\u7c98\u6027 \u306e\u5f71\u97ff\u3092\u8003\u616e\u3057\u305f \u6761\u4ef6\u3092, \u4e71 \u4e71 \u4e71\u6d41 \u6d41 \u6d41\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\u3068 \u3068 \u3068\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\u306e \u306e \u306e\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\u3068 \u3068 \u3068\u306e \u306e \u306e\n(Jointly-Training) \u95a2 \u95a2 \u95a2\u4fc2 \u4fc2 \u4fc2\u3092 \u3092 \u3092\u3082 \u3082 \u3082\u3068 \u3068 \u3068\u306b \u306b \u306b\u691c\u8a0e\u3057\u305f \u3002\nReference \u4e71 \u4e71 \u4e71\u6d41 \u6d41 \u6d41\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\u3068 \u3068 \u3068\u6570 \u6570 \u6570\u5024 \u5024 \u5024\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\u306e \u306e \u306e\u5927 \u5927 \u5927\u5c0f \u5c0f \u5c0f\u95a2 \u95a2 \u95a2\u4fc2 \u4fc2 \u4fc2\u306b \u306b \u306b\u3088 \u3088 \u3088\u308a \u308a \u308a, \u4e71\u6d41\u7c98\u6027\u306e\u52b9\u679c\u304c\u6b63\u3057\u304f\n\u8a55\u4fa1\u3055\u308c\u308b \u6761\u4ef6\u3092\u691c\u8a0e\u3057\u305f \u3002\nExample 2: Improvement in over-translation.\nInput activity was high in cells of the young , especiallynewborn infant , and was very\nslight in cells of 30 - year - old or more .\nBaseline-NMT \u6d3b\u52d5\u6027\u306f\u82e5\u9f62, \u7279\u306b\u65b0 \u65b0 \u65b0\u751f \u751f \u751f\u5150 \u5150 \u5150\u65b0 \u65b0 \u65b0\u751f \u751f \u751f\u5150 \u5150 \u5150\u3067\u306f30 \u6b73 \u6b73 \u6b73\u4ee5 \u4ee5 \u4ee5\u4e0a \u4e0a \u4e0a\u306e \u306e \u306e\u7d30\u80de\u3067\u9ad8\u304f,\n30 \u6b73 \u6b73 \u6b73\u4ee5 \u4ee5 \u4ee5\u4e0a \u4e0a \u4e0a\u306e \u306e \u306e\u7d30\u80de\u3067\u306f\u308f\u305a\u304b\u3067\u3042\u3063\u305f \u3002\n+Reconstructor \u305d\u306e\u6d3b\u6027\u306f\u82e5\u9f62, \u7279\u306b\u65b0 \u65b0 \u65b0\u751f \u751f \u751f\u5150 \u5150 \u5150\u306f\u7d30\u80de\u304c\u9ad8\u304f, 30 \u6b73 \u6b73 \u6b73\u4ee5 \u4ee5 \u4ee5\u4e0a \u4e0a \u4e0a\u306e \u306e \u306e\u7d30\u80de \u3067\u306f\n\u308f\u305a\u304b\u3067\u3042\u3063\u305f \u3002\n+Reconstructor \u82e5\u9f62\u306e\u65b0 \u65b0 \u65b0\u751f \u751f \u751f\u5150 \u5150 \u5150\u3067\u306f\u6d3b\u6027\u306f\u9ad8\u304b\u3063\u305f\u304c , 30 \u6b73 \u6b73 \u6b73\u4ee5 \u4ee5 \u4ee5\u4e0a \u4e0a \u4e0a\u306e \u306e \u306e\u5834\u5408\u306b\u306f \u6975\u3081\u3066\n(Jointly-Training) \u8efd\u5ea6\u3067\u3042\u3063\u305f \u3002\nReference \u6d3b\u6027\u306f\u82e5\u3044\u500b\u4f53, \u7279\u306b\u65b0 \u65b0 \u65b0\u751f \u751f \u751f\u5150 \u5150 \u5150\u306e\u7d30\u80de\u3067\u9ad8\u304f, 30 \u6b73 \u6b73 \u6b73\u4ee5 \u4ee5 \u4ee5\u4e0a \u4e0a \u4e0a\u306e \u306e \u306e\u3082\u306e\u3067\u306f\n\u3054\u304f\u308f\u305a\u304b\u3067\u3042\u3063\u305f \u3002\nTable 4: Examples of outputs of English-Japanese translation.\n5.1 Datasets\nWe used two parallel corpora: Asian Scien-\nti\ufb01c Paper Excerpt Corpus (ASPEC) (Nakazawa\net al., 2016) and NTCIR PatentMT Parallel Corpus\n(Goto et al., 2013). Regarding the training data of\nASPEC, we used only the \ufb01rst 1 million sentences\nsorted by sentence-alignment similarity. Japanese\nsentences were segmented by the morphological\nanalyzer MeCab (version 0.996, IPADIC), and En-\nglish sentences were tokenized by tokenizer.perl\nof Moses. Table 1 shows the numbers of the sen-\ntences in each corpus. Note that sentences with\nmore than 40 words were excluded from the train-\ning data.\n5.2 Models\nWe used the attention-based NMT (Bahdanau\net al., 2015) as a baseline-NMT, the encoder-\ndecoder-reconstructor (Tu et al., 2017) and the\nencoder-decoder-reconstructor that jointly trained\nforward translation and back-translation without\npre-training. The RNN used in the experi-\nments had 512 hidden units, 512 embedding units,\n30,000 vocabulary size and 64 batch size. We\nused Adagrad (initial learning rate 0.01) for op-\ntimizing model parameters. We trained our model\non GeForce GTX TITAN X GPU. Note that we\nset the hyper-parameter \u03bb = 1 on the encoder-\ndecoder-reconstructor same as Tu et al. (2017).\n5.3 Results\nTables 2 and 3 show the translation accuracy\nin BLEU scores, the p-value of the signi\ufb01cance\ntest by bootstrap resampling (Koehn, 2004) and\ntraining time in hours until convergence. The\nencoder-decoder-reconstructor (Tu et al., 2017)\nrequires slightly longer time to train than the\nbaseline NMT, but we emphasize that decoding\ntime remains the same with the encoder-decoder-\nreconstructor and baseline-NMT. The results show\nthat the encoder-decoder-reconstructor (Tu et al.,\n2017) signi\ufb01cantly improves translation accuracy\nby 1.01 points on ASPEC and 1.37 points on NT-\nCIR in English-Japanese translation ( p <0.05).\nHowever, it does not signi\ufb01cantly improve trans-\nlation accuracy in Japanese-English translation. In\naddition, it is proved that the encoder-decoder-\nreconstructor without pre-training worsens rather\n\nCorpus Model English-Japanese Japanese-English\n(i) (ii) (iii) (i) (ii) (iii)\nBaseline-NMT 1,141 378 1,045 951 494 1,085\nASPEC +Reconstructor 988 336 1,042 836 418 1,014\n+Reconstructor (Jointly-Training) 1,292 446 1,147 1,106 525 1,821\nBaseline-NMT 2,122 1,015 1,106 2,521 1,073 1,630\nNTCIR +Reconstructor 1,958 922 963 2,187 987 1,422\n+Reconstructor (Jointly-Training) 1,978 916 1,078 2,475 1,107 1,610\nTable 5: Numbers of redundant and unknown word tokens.\nof encoder-decoder-reconstructor almost correctly\nattends input words.\nTable 5 shows a comparison of the number of\nword occurrences for each corpus and model. The\ncolumns show (i) the number of words that ap-\npear more frequently than the counterparts in the\nreference, and (ii) the number of words that ap-\npear more than once but are not included in the\nreference. Note that these numbers do not in-\nclude unknown words, so (iii) shows the num-\nber of unknown words. In all the cases, the\nnumber of occurrence of redundant words is re-\nduced in encoder-decoder-reconstructor. Thus,\nwe con\ufb01rmed that encoder-decoder-reconstructor\nachieves reduction of repeating and missing words\nwhile maintaining the quality of translation.\n6 Conclusion\nIn this paper, we evaluated the encoder-decoder-\nreconstructor on English-Japanese and Japanese-\nEnglish translation tasks. In addition, we evaluate\nthe effectiveness of pre-training by comparing it\nwith a jointly-trained model of forward translation\nand back-translation. Experimental results show\nthat the encoder-decoder-reconstructor offers sig-\nni\ufb01cant improvement in BLEU scores and allevi-\nates the problem of repeating and missing words\nin the translation on English-Japanese translation\ntask, and the encoder-decoder-reconstructor can\nnot be trained well without pre-training, so it\nproves that we have to train the forward transla-\ntion model in a manner similar to the conventional\nattention-based NMT as pre-training.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural Machine Translation by Jointly\nLearning to Align and Translate. Proceedings of\nthe 3rd International Conference on Learning Rep-\nresentations (ICLR), pages 1\u201315.\nKyunghyun Cho, Bart Van Merri \u00a8enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. 2014. Learn-\ning Phrase Representations using RNN Encoder-\nDecoder for Statistical Machine Translation. Pro-\nceedings of the 2014 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP),\npages 1724\u20131734.\nShi Feng, Shujie Liu, Nan Yang, Mu Li, and Ming\nZhou. 2016. Improving Attention Modeling with\nImplicit Distortion and Fertility for Machine Trans-\nlation. Proceedings of the 26th International Con-\nference on Computational Linguistics (COLING),\npages 3082\u20133092.\nIsao Goto, Ka-Po Chow, Bin Lu, Eiichiro Sumita, and\nBenjamin K Tsou. 2013. Overview of the Patent\nMachine Translation Task at the NTCIR-10 Work-\nshop. Proceedings of the 10th NII Testbeds and\nCommunity for Information access Research Con-\nference (NTCIR), pages 260\u2013286.\nPhilipp Koehn. 2004. Statistical Signi\ufb01cance Tests for\nMachine Translation Evaluation. Proceedings of the\n2004 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 388\u2013395.\nFandong Meng, Zhengdong Lu, Hang Li, and Qun\nLiu. 2016. Interactive Attention for Neural Ma-\nchine Translation. Proceedings of the 26th Inter-\nnational Conference on Computational Linguistics\n(COLING), pages 2174\u20132185.\nHaitao Mi, Baskaran Sankaran, Zhiguo Wang, and Abe\nIttycheriah. 2016. Coverage Embedding Models for\nNeural Machine Translation. Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 955\u2013960.\nToshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchi-\nmoto, Masao Utiyama, Eiichiro Sumita, Sadao\nKurohashi, and Hitoshi Isahara. 2016. ASPEC:\nAsian Scienti\ufb01c Paper Excerpt Corpus. Proceedings\nof the Tenth International Conference on Language\nResources and Evaluation (LREC), pages 2204\u2013\n2208.\nJan Niehues, Eunah Cho, Thanh-Le Ha, and Alex\nWaibel. 2016. Pre-Translation for Neural Machine\nTranslation. Proceedings of the 26th International\n\nEnglish-Japanese\nCorpus Model BLEU p-value Hours\nBaseline-NMT 29.75 - 99\nASPEC +Reconstructor 30.76 0.00 149\n+Reconstructor (Jointly-Training) 26.04 - 174\nBaseline-NMT 30.03 - 116\nNTCIR +Reconstructor 31.40 0.00 166\n+Reconstructor (Jointly-Training) 29.04 - 252\nTable 2: English-Japanese translation results.\nJapanese-English\nCorpus Model BLEU p-value Hours\nBaseline-NMT 21.91 - 87\nASPEC +Reconstructor 22.27 0.10 127\n+Reconstructor (Jointly-Training) 16.29 - 187\nBaseline-NMT 29.48 - 180\nNTCIR +Reconstructor 29.73 0.11 244\n+Reconstructor (Jointly-Training) 28.95 - 300\nTable 3: Japanese-English translation results.\nand its weight \u03b1\u2032\nij is a normalized probability dis-\ntribution. It is computed by\n\u03b1\u2032\nij =\nexp(e\u2032\nij)\n\u2211|y|\nk=1 exp(e\u2032\nik)\n(14)\nand\ne\u2032\nij = v\u2032\na\u22a4tanh(W\u2032\nas\u2032\ni\u22121 + U\u2032\nasj) (15)\nwhere v\u2032\na is a weight vector and W\u2032\na and U\u2032\na are\nweight matrices.\nThe objective function is de\ufb01ned by\nL(\u03b8,\u03b3) = 1\nN\n\u2211N\nn=1\n{ |y|\u2211\ni=1\nlog p(\u02c6y(n)\ni |y(n)\n<i ,x(n),\u03b8)\n+ \u03bb\n|x|\u2211\ni=1\nlog p(\u02c6x(n)\ni |x(n)\n<i ,s(n),\u03b3)\n}\n(16)\nwhere N is the number of data, \u03b8and \u03b3are model\nparameters and \u03bbis a hyper-parameter which can\nconsider the weight between forward translation\nand back-translation.\nThis objective function consists of two parts:\nforward measures translation \ufb02uency, and back-\nward measures translation adequacy. Thus, the\ncombined objective function is more consistent\nwith the goal of enhancing overall translation\nquality, and can more effectively guide the param-\neter training for making better translation.\n4.2 Training\nThe encoder-decoder-reconstructor is trained with\nlikelihood of both the encoder-decoder and the re-\nconstructor on a set of training datasets. Tu et al.\n(2017) trained a back-translation model from the\nhidden state of the decoder into the source se-\nquence by reconstructor to enforce agreement be-\ntween source and target sentences using Equa-\ntion 16 after training the forward translation in a\nmanner similar to the conventional attention-based\nNMT using Equation 10.\nIn addition, we experiment to jointly train a\nmodel of forward translation and back-translation\nwithout pre-training. It may learn a globally opti-\nmal model compared to locally optimal model pre-\ntrained using the forward translation.\n4.3 Testing\nTu et al. (2017) used a beam search to predict tar-\nget sentences that approximately maximizes both\nof forward translation and back-translation on test-\ning. In this paper, however, we do not use a beam\nsearch for simplicity and effectiveness.\n5 Experiments\nWe evaluated the encoder-decoder-reconstructor\nframework for NMT on English-Japanese and\nJapanese-English translation tasks.",
            1.218791207999999
          ]
        },
        {
          "answer": "They used two parallel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) and NTCIR PatentMT Parallel Corpus.",
          "relevant_chunks": [
            "Corpus Model English-Japanese Japanese-English\n(i) (ii) (iii) (i) (ii) (iii)\nBaseline-NMT 1,141 378 1,045 951 494 1,085\nASPEC +Reconstructor 988 336 1,042 836 418 1,014\n+Reconstructor (Jointly-Training) 1,292 446 1,147 1,106 525 1,821\nBaseline-NMT 2,122 1,015 1,106 2,521 1,073 1,630\nNTCIR +Reconstructor 1,958 922 963 2,187 987 1,422\n+Reconstructor (Jointly-Training) 1,978 916 1,078 2,475 1,107 1,610\nTable 5: Numbers of redundant and unknown word tokens.\nof encoder-decoder-reconstructor almost correctly\nattends input words.\nTable 5 shows a comparison of the number of\nword occurrences for each corpus and model. The\ncolumns show (i) the number of words that ap-\npear more frequently than the counterparts in the\nreference, and (ii) the number of words that ap-\npear more than once but are not included in the\nreference. Note that these numbers do not in-\nclude unknown words, so (iii) shows the num-\nber of unknown words. In all the cases, the\nnumber of occurrence of redundant words is re-\nduced in encoder-decoder-reconstructor. Thus,\nwe con\ufb01rmed that encoder-decoder-reconstructor\nachieves reduction of repeating and missing words\nwhile maintaining the quality of translation.\n6 Conclusion\nIn this paper, we evaluated the encoder-decoder-\nreconstructor on English-Japanese and Japanese-\nEnglish translation tasks. In addition, we evaluate\nthe effectiveness of pre-training by comparing it\nwith a jointly-trained model of forward translation\nand back-translation. Experimental results show\nthat the encoder-decoder-reconstructor offers sig-\nni\ufb01cant improvement in BLEU scores and allevi-\nates the problem of repeating and missing words\nin the translation on English-Japanese translation\ntask, and the encoder-decoder-reconstructor can\nnot be trained well without pre-training, so it\nproves that we have to train the forward transla-\ntion model in a manner similar to the conventional\nattention-based NMT as pre-training.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural Machine Translation by Jointly\nLearning to Align and Translate. Proceedings of\nthe 3rd International Conference on Learning Rep-\nresentations (ICLR), pages 1\u201315.\nKyunghyun Cho, Bart Van Merri \u00a8enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. 2014. Learn-\ning Phrase Representations using RNN Encoder-\nDecoder for Statistical Machine Translation. Pro-\nceedings of the 2014 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP),\npages 1724\u20131734.\nShi Feng, Shujie Liu, Nan Yang, Mu Li, and Ming\nZhou. 2016. Improving Attention Modeling with\nImplicit Distortion and Fertility for Machine Trans-\nlation. Proceedings of the 26th International Con-\nference on Computational Linguistics (COLING),\npages 3082\u20133092.\nIsao Goto, Ka-Po Chow, Bin Lu, Eiichiro Sumita, and\nBenjamin K Tsou. 2013. Overview of the Patent\nMachine Translation Task at the NTCIR-10 Work-\nshop. Proceedings of the 10th NII Testbeds and\nCommunity for Information access Research Con-\nference (NTCIR), pages 260\u2013286.\nPhilipp Koehn. 2004. Statistical Signi\ufb01cance Tests for\nMachine Translation Evaluation. Proceedings of the\n2004 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 388\u2013395.\nFandong Meng, Zhengdong Lu, Hang Li, and Qun\nLiu. 2016. Interactive Attention for Neural Ma-\nchine Translation. Proceedings of the 26th Inter-\nnational Conference on Computational Linguistics\n(COLING), pages 2174\u20132185.\nHaitao Mi, Baskaran Sankaran, Zhiguo Wang, and Abe\nIttycheriah. 2016. Coverage Embedding Models for\nNeural Machine Translation. Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 955\u2013960.\nToshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchi-\nmoto, Masao Utiyama, Eiichiro Sumita, Sadao\nKurohashi, and Hitoshi Isahara. 2016. ASPEC:\nAsian Scienti\ufb01c Paper Excerpt Corpus. Proceedings\nof the Tenth International Conference on Language\nResources and Evaluation (LREC), pages 2204\u2013\n2208.\nJan Niehues, Eunah Cho, Thanh-Le Ha, and Alex\nWaibel. 2016. Pre-Translation for Neural Machine\nTranslation. Proceedings of the 26th International\n\nExample 1: Improvement in under-translation.\nInput the conditions under which the effect of turbulent viscosity is cor-\nrectly evaluated were examined on the basis of the relation between turbulent\nviscosity and numerical viscosity in size .\nBaseline-NMT \u4e71\u6d41\u7c98\u6027 \u306e\u5f71\u97ff\u3092\u6b63\u78ba\u306b\u8a55\u4fa1\u3059\u308b\u6761\u4ef6\u3092\u691c\u8a0e\u3057\u305f\u3002\n+Reconstructor \u4e71\u6d41\u7c98\u6027 \u306e\u5f71\u97ff\u3092\u6b63\u78ba\u306b\u8a55\u4fa1\u3059\u308b\u6761\u4ef6\u3092, \u4e71 \u4e71 \u4e71\u6d41 \u6d41 \u6d41\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\u3068 \u3068 \u3068\u6570 \u6570 \u6570\u5024 \u5024 \u5024 \u7684 \u7684 \u7684\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\n\u306e \u306e \u306e\u95a2 \u95a2 \u95a2\u4fc2 \u4fc2 \u4fc2\u3092 \u3092 \u3092\u57fa \u57fa \u57fa\u306b \u306b \u306b\u8abf\u3079\u305f \u3002\n+Reconstructor \u4e71\u6d41\u7c98\u6027 \u306e\u5f71\u97ff\u3092\u8003\u616e\u3057\u305f \u6761\u4ef6\u3092, \u4e71 \u4e71 \u4e71\u6d41 \u6d41 \u6d41\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\u3068 \u3068 \u3068\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\u306e \u306e \u306e\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\u3068 \u3068 \u3068\u306e \u306e \u306e\n(Jointly-Training) \u95a2 \u95a2 \u95a2\u4fc2 \u4fc2 \u4fc2\u3092 \u3092 \u3092\u3082 \u3082 \u3082\u3068 \u3068 \u3068\u306b \u306b \u306b\u691c\u8a0e\u3057\u305f \u3002\nReference \u4e71 \u4e71 \u4e71\u6d41 \u6d41 \u6d41\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\u3068 \u3068 \u3068\u6570 \u6570 \u6570\u5024 \u5024 \u5024\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\u306e \u306e \u306e\u5927 \u5927 \u5927\u5c0f \u5c0f \u5c0f\u95a2 \u95a2 \u95a2\u4fc2 \u4fc2 \u4fc2\u306b \u306b \u306b\u3088 \u3088 \u3088\u308a \u308a \u308a, \u4e71\u6d41\u7c98\u6027\u306e\u52b9\u679c\u304c\u6b63\u3057\u304f\n\u8a55\u4fa1\u3055\u308c\u308b \u6761\u4ef6\u3092\u691c\u8a0e\u3057\u305f \u3002\nExample 2: Improvement in over-translation.\nInput activity was high in cells of the young , especiallynewborn infant , and was very\nslight in cells of 30 - year - old or more .\nBaseline-NMT \u6d3b\u52d5\u6027\u306f\u82e5\u9f62, \u7279\u306b\u65b0 \u65b0 \u65b0\u751f \u751f \u751f\u5150 \u5150 \u5150\u65b0 \u65b0 \u65b0\u751f \u751f \u751f\u5150 \u5150 \u5150\u3067\u306f30 \u6b73 \u6b73 \u6b73\u4ee5 \u4ee5 \u4ee5\u4e0a \u4e0a \u4e0a\u306e \u306e \u306e\u7d30\u80de\u3067\u9ad8\u304f,\n30 \u6b73 \u6b73 \u6b73\u4ee5 \u4ee5 \u4ee5\u4e0a \u4e0a \u4e0a\u306e \u306e \u306e\u7d30\u80de\u3067\u306f\u308f\u305a\u304b\u3067\u3042\u3063\u305f \u3002\n+Reconstructor \u305d\u306e\u6d3b\u6027\u306f\u82e5\u9f62, \u7279\u306b\u65b0 \u65b0 \u65b0\u751f \u751f \u751f\u5150 \u5150 \u5150\u306f\u7d30\u80de\u304c\u9ad8\u304f, 30 \u6b73 \u6b73 \u6b73\u4ee5 \u4ee5 \u4ee5\u4e0a \u4e0a \u4e0a\u306e \u306e \u306e\u7d30\u80de \u3067\u306f\n\u308f\u305a\u304b\u3067\u3042\u3063\u305f \u3002\n+Reconstructor \u82e5\u9f62\u306e\u65b0 \u65b0 \u65b0\u751f \u751f \u751f\u5150 \u5150 \u5150\u3067\u306f\u6d3b\u6027\u306f\u9ad8\u304b\u3063\u305f\u304c , 30 \u6b73 \u6b73 \u6b73\u4ee5 \u4ee5 \u4ee5\u4e0a \u4e0a \u4e0a\u306e \u306e \u306e\u5834\u5408\u306b\u306f \u6975\u3081\u3066\n(Jointly-Training) \u8efd\u5ea6\u3067\u3042\u3063\u305f \u3002\nReference \u6d3b\u6027\u306f\u82e5\u3044\u500b\u4f53, \u7279\u306b\u65b0 \u65b0 \u65b0\u751f \u751f \u751f\u5150 \u5150 \u5150\u306e\u7d30\u80de\u3067\u9ad8\u304f, 30 \u6b73 \u6b73 \u6b73\u4ee5 \u4ee5 \u4ee5\u4e0a \u4e0a \u4e0a\u306e \u306e \u306e\u3082\u306e\u3067\u306f\n\u3054\u304f\u308f\u305a\u304b\u3067\u3042\u3063\u305f \u3002\nTable 4: Examples of outputs of English-Japanese translation.\n5.1 Datasets\nWe used two parallel corpora: Asian Scien-\nti\ufb01c Paper Excerpt Corpus (ASPEC) (Nakazawa\net al., 2016) and NTCIR PatentMT Parallel Corpus\n(Goto et al., 2013). Regarding the training data of\nASPEC, we used only the \ufb01rst 1 million sentences\nsorted by sentence-alignment similarity. Japanese\nsentences were segmented by the morphological\nanalyzer MeCab (version 0.996, IPADIC), and En-\nglish sentences were tokenized by tokenizer.perl\nof Moses. Table 1 shows the numbers of the sen-\ntences in each corpus. Note that sentences with\nmore than 40 words were excluded from the train-\ning data.\n5.2 Models\nWe used the attention-based NMT (Bahdanau\net al., 2015) as a baseline-NMT, the encoder-\ndecoder-reconstructor (Tu et al., 2017) and the\nencoder-decoder-reconstructor that jointly trained\nforward translation and back-translation without\npre-training. The RNN used in the experi-\nments had 512 hidden units, 512 embedding units,\n30,000 vocabulary size and 64 batch size. We\nused Adagrad (initial learning rate 0.01) for op-\ntimizing model parameters. We trained our model\non GeForce GTX TITAN X GPU. Note that we\nset the hyper-parameter \u03bb = 1 on the encoder-\ndecoder-reconstructor same as Tu et al. (2017).\n5.3 Results\nTables 2 and 3 show the translation accuracy\nin BLEU scores, the p-value of the signi\ufb01cance\ntest by bootstrap resampling (Koehn, 2004) and\ntraining time in hours until convergence. The\nencoder-decoder-reconstructor (Tu et al., 2017)\nrequires slightly longer time to train than the\nbaseline NMT, but we emphasize that decoding\ntime remains the same with the encoder-decoder-\nreconstructor and baseline-NMT. The results show\nthat the encoder-decoder-reconstructor (Tu et al.,\n2017) signi\ufb01cantly improves translation accuracy\nby 1.01 points on ASPEC and 1.37 points on NT-\nCIR in English-Japanese translation ( p <0.05).\nHowever, it does not signi\ufb01cantly improve trans-\nlation accuracy in Japanese-English translation. In\naddition, it is proved that the encoder-decoder-\nreconstructor without pre-training worsens rather\n\nEnglish-Japanese\nCorpus Model BLEU p-value Hours\nBaseline-NMT 29.75 - 99\nASPEC +Reconstructor 30.76 0.00 149\n+Reconstructor (Jointly-Training) 26.04 - 174\nBaseline-NMT 30.03 - 116\nNTCIR +Reconstructor 31.40 0.00 166\n+Reconstructor (Jointly-Training) 29.04 - 252\nTable 2: English-Japanese translation results.\nJapanese-English\nCorpus Model BLEU p-value Hours\nBaseline-NMT 21.91 - 87\nASPEC +Reconstructor 22.27 0.10 127\n+Reconstructor (Jointly-Training) 16.29 - 187\nBaseline-NMT 29.48 - 180\nNTCIR +Reconstructor 29.73 0.11 244\n+Reconstructor (Jointly-Training) 28.95 - 300\nTable 3: Japanese-English translation results.\nand its weight \u03b1\u2032\nij is a normalized probability dis-\ntribution. It is computed by\n\u03b1\u2032\nij =\nexp(e\u2032\nij)\n\u2211|y|\nk=1 exp(e\u2032\nik)\n(14)\nand\ne\u2032\nij = v\u2032\na\u22a4tanh(W\u2032\nas\u2032\ni\u22121 + U\u2032\nasj) (15)\nwhere v\u2032\na is a weight vector and W\u2032\na and U\u2032\na are\nweight matrices.\nThe objective function is de\ufb01ned by\nL(\u03b8,\u03b3) = 1\nN\n\u2211N\nn=1\n{ |y|\u2211\ni=1\nlog p(\u02c6y(n)\ni |y(n)\n<i ,x(n),\u03b8)\n+ \u03bb\n|x|\u2211\ni=1\nlog p(\u02c6x(n)\ni |x(n)\n<i ,s(n),\u03b3)\n}\n(16)\nwhere N is the number of data, \u03b8and \u03b3are model\nparameters and \u03bbis a hyper-parameter which can\nconsider the weight between forward translation\nand back-translation.\nThis objective function consists of two parts:\nforward measures translation \ufb02uency, and back-\nward measures translation adequacy. Thus, the\ncombined objective function is more consistent\nwith the goal of enhancing overall translation\nquality, and can more effectively guide the param-\neter training for making better translation.\n4.2 Training\nThe encoder-decoder-reconstructor is trained with\nlikelihood of both the encoder-decoder and the re-\nconstructor on a set of training datasets. Tu et al.\n(2017) trained a back-translation model from the\nhidden state of the decoder into the source se-\nquence by reconstructor to enforce agreement be-\ntween source and target sentences using Equa-\ntion 16 after training the forward translation in a\nmanner similar to the conventional attention-based\nNMT using Equation 10.\nIn addition, we experiment to jointly train a\nmodel of forward translation and back-translation\nwithout pre-training. It may learn a globally opti-\nmal model compared to locally optimal model pre-\ntrained using the forward translation.\n4.3 Testing\nTu et al. (2017) used a beam search to predict tar-\nget sentences that approximately maximizes both\nof forward translation and back-translation on test-\ning. In this paper, however, we do not use a beam\nsearch for simplicity and effectiveness.\n5 Experiments\nWe evaluated the encoder-decoder-reconstructor\nframework for NMT on English-Japanese and\nJapanese-English translation tasks.",
            0.6334784169999921
          ]
        }
      ],
      "reference_answers": [
        "Yes Yes",
        "Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0, NTCIR PatentMT Parallel Corpus BIBREF1 Asian Scientific Paper Excerpt Corpus, NTCIR PatentMT Parallel Corpus "
      ]
    }
  },
  "average_metrics": {
    "collection": "fix_length_paragraph_1",
    "average_exact_match": 0.041666666666666664,
    "average_bleu_score": 0.08184191489410235,
    "average_rouge_scores": {
      "rouge-1": {
        "precision": 0.26918476671812863,
        "recall": 0.48290602605593547,
        "f1": 0.2904429507702328
      },
      "rouge-2": {
        "precision": 0.14509849661504073,
        "recall": 0.24453844836704883,
        "f1": 0.16089445769897762
      },
      "rouge-L": {
        "precision": 0.24111190785055273,
        "recall": 0.4140588102601237,
        "f1": 0.2542766148839083
      }
    }
  }
}