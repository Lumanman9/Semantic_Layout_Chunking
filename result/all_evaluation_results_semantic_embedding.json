{
  "individual_document_results": {
    "Literature_paper.pdf": {
      "error": "No valid question-answer pairs to evaluate",
      "average_exact_match": 0.0,
      "average_bleu_score": 0.0,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        },
        "rouge-2": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        },
        "rouge-L": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        }
      },
      "per_question_metrics": []
    },
    "1706.09147.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.008845332756352567,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.18811188811188811,
          "recall": 0.23148148148148145,
          "f1": 0.1943351691581432
        },
        "rouge-2": {
          "precision": 0.009259259259259259,
          "recall": 0.019230769230769232,
          "f1": 0.0125
        },
        "rouge-L": {
          "precision": 0.12237762237762238,
          "recall": 0.14814814814814814,
          "f1": 0.12549173878835565
        }
      },
      "per_question_metrics": [
        {
          "question": "What is the new initialization method proposed in this paper?",
          "generated_answer": "The new initialization method proposed in this paper involves using pre-initialized embeddings.",
          "reference_answer": "They initialize their word and entity embeddings with vectors pre-trained over a large corpus of unlabeled data.",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.23076923076923078,
              "recall": 0.16666666666666666,
              "f1": 0.1935483870967742
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.15384615384615385,
              "recall": 0.1111111111111111,
              "f1": 0.12903225806451615
            }
          },
          "bleu_score": 0.0125774279703401,
          "exact_match": 0.0
        },
        {
          "question": "How was a quality control performed so that the text is noisy but the annotations are accurate?",
          "generated_answer": "The context provided does not specifically mention the methods used for ensuring the quality control of the text to ensure its noisiness while maintaining accurate annotations. It mainly discusses the analysis of prediction errors, model evaluation, and comparisons with existing datasets and methods, without detailing the process of quality control for text and annotation accuracy.",
          "reference_answer": "The authors believe that the Wikilinks corpus  contains ground truth annotations while being noisy. They discard mentions that cannot have ground-truth verified by comparison with Wikipedia.",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.14545454545454545,
              "recall": 0.2962962962962963,
              "f1": 0.19512195121951217
            },
            "rouge-2": {
              "precision": 0.018518518518518517,
              "recall": 0.038461538461538464,
              "f1": 0.025
            },
            "rouge-L": {
              "precision": 0.09090909090909091,
              "recall": 0.18518518518518517,
              "f1": 0.12195121951219513
            }
          },
          "bleu_score": 0.005113237542365033,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "What is the new initialization method proposed in this paper?",
        "How was a quality control performed so that the text is noisy but the annotations are accurate?"
      ],
      "generated_answers": [
        {
          "answer": "The new initialization method proposed in this paper involves using pre-initialized embeddings.",
          "relevant_chunks": [
            "(2011). Baselines As a baseline we took the standard\nMost Probable Sense (MPS) prediction, which se-\nlects the entity that was seen most with the given\nmention during training. We also compare to the\nfollowing papers - Francis-Landau et al.\n\n67.2\nAll-Entity, with init.\n\n(2016),\nYamada at el. (2016), and Chisholm et al. (2015),\nas they are all strong local approaches and a good\nsource for comparison. Results Table 2 displays the micro and macro\nP@1 scores on CoNLL-Y AGO test-b for the dif-\nferent training steps. We \ufb01nd that when using only\nthe training set of CoNLL-Y AGO our model is\nunder-trained and that the domain adaptation sig-\nni\ufb01cant boosts performance. We \ufb01nd that incorpo-\nrating extra statistical and string features yields a\nsmall extra improvement in performance. The \ufb01nal micro and macro P@1 scores on\nCoNLL-Y AGO test-b are displayed in table 3. On\nthis dataset our model achieves comparable re-\nsults, however it does not outperform the state-\nof-the-art, probably because of the relatively small\ntraining set and our reliance on domain adaptation. 5.3 Effects of initialized embeddings and\ncorrupt-sampling schemes\nWe performed a study of the effects of using\npre-initialized embeddings for our model, and of\nusing either All-Entity or Near-Misses corrupt-\nsampling. The evaluation was done on a10% sam-\nple of the evaluation set of the WikilinksNED cor-\npus and can be seen in Table 4. We have found that using pre-initialized embed-\ndings results in signi\ufb01cant performance gains, due\nto the better starting point. We have also found\nthat using Near-Misses, our model achieves sig-\nni\ufb01cantly improved performance. We attribute this\n6Learning rate of 0.02; maximal tree depth of 4; 10, 000\ntrees. CoNLL-Y AGO test-b - Training Steps Eval\nModel Micro\nP@1\nMacro\nP@1\nPPRforNED\nCoNLL training set 82 82\n+ domain adaptation 86.6 87 .7\n+ GBRT 87.3 88 .6\nYago\nCoNLL training set 74.8 73 .5\n+ domain adaptation 83.6 85 .1\n+ GBRT 83.3 86 .3\nTable 2: Evaluation of training steps on CoNLL-\nY AGO. CoNLL-Y AGO test-b (Local methods)\nModel Micro\nP@1\nMacro\nP@1\nPPRforNED\nOur ARNN + GBRT 87.3 88 .6\nYamada (2016) local 90.9 92 .4\nYamada (2016) global 93.1 92 .6\nYago\nOur ARNN + GBRT 83.3 86 .3\nYamada (2016) local 87.2 89 .6\nFrancis-Landau (2016) 85.5 -\nChisholm (2015) local 86.1 -\nYamada (2016) global 91.5 90 .9\nChisholm (2015) global 88.7 -\nTable 3: Evaluation on CoNLL-Y AGO. difference to the more ef\ufb01cient nature of training\nwith near misses.",
            0.5181356249999851
          ]
        },
        {
          "answer": "The context provided does not specifically mention the methods used for ensuring the quality control of the text to ensure its noisiness while maintaining accurate annotations. It mainly discusses the analysis of prediction errors, model evaluation, and comparisons with existing datasets and methods, without detailing the process of quality control for text and annotation accuracy.",
          "relevant_chunks": [
            "Both these results were found to\nbe statistically signi\ufb01cant. 6 Error Analysis\nWe randomly sampled and manually analyzed200\ncases of prediction errors made by our model. This\nset was obtained from WikilinksNED\u2019s validation\nset that was not used for training. Working with crowd-sourced data, we expected\nsome errors to result from noise in the ground\ntruths themselves. Indeed, we found that 19.5%\n(39/200) of the errors were not false, out of which\n5% (2) where wrong labels, 33% (13) were pre-\ndictions with an equivalent meaning as the correct\nentity, and in 61.5% (24) our model suggested a\nmore convincing solution than the original author\nby using speci\ufb01c hints from the context. In this\nmanner, the mention\u2019Supreme leader\u2019, which was\n\nWikilinks Evaluation-Set\nModel Micro\naccuracy\nNear-misses, with init. 72.5\nNear-misses, random init.\n\n70\nAll-Entity, random init. 67.1\nTable 4: Corrupt-sampling and Initialization\ncontextually associated to the Iranian leader Ali\nKhamenei, was linked by our model with\u2019supreme\nleader of Iran\u2019while the \u201dcorrect\u201d tag was the gen-\neral \u2019supreme leader\u2019entity. In addition, 15.5% (31/200) were cases where\na Wikipedia disambiguation-page was either the\ncorrect or predicted entity (2.5% and 14%, respec-\ntively). We considered the rest of the 130 errors as\ntrue semantic errors, and analyzed them in-depth. Error type Fraction\nFalse errors\nNot errors 19.5% (39 /200)\n- Annotation error 5% (2 /39)\n- Better suggestion 61.5% (24 /39)\n- Equivalent entities 33% (13 /39)\nDisambiguation page 15.5% (31 /200)\nTrue semantic errors\nToo speci\ufb01c/general 31.5% (41 /130)\n\u2019almost correct\u2019 errors 26% (34 /130)\ninsuf\ufb01cient training 21.5% (28 /130)\nTable 5: Error distribution in 200 samples. Cate-\ngories of true errors are not fully distinct. First, we noticed that in31.5% of the true errors\n(41/130) our model selected an entity that can be\nunderstood as a speci\ufb01c ( 6.5%) or general ( 25%)\nrealization of the correct solution. For example,\ninstead of predicting \u2019Aroma of wine\u2019for a text on\nthe scent and \ufb02avor of Turkish wine, the model\nassigned the mention \u2019Aroma\u2019with the general\n\u2019Odor\u2019entity. We observed that in 26% (34/130)\nof the error cases, the predicted entity had a very\nstrong semantic relationship to the correct entity. A closer look discovered two prominent types of\n\u2019almost correct\u2019 errors occurred repeatedly in the\ndata. The \ufb01rst was a \ufb01lm/book/theater type of er-\nror (8.4%), where the actual and the predicted enti-\nties were a different display of the same narrative. Even though having different jargon and produc-\ners, those \ufb01elds share extremely similar content,\nwhich may explain why they tend to be frequently\nconfused by the algorithm. A third (4/14) of those\ncases were tagged as truly ambiguous even for hu-\nman reader. The second prominent type of \u2019almost\ncorrect\u2019 errors where differentiating between ad-\njectives that are used to describe properties of a na-\ntion. Particularity, mentions such as \u2019Germanic\u2019,\n\u2019Chinese\u2019and \u2019Dutch\u2019were falsely assigned to en-\ntities that describe language instead of people, and\nvice versa. We observed this type of mistake in\n8.4% of the errors (11/130). Another interesting type of errors where in\ncases where the correct entity had insuf\ufb01cient\ntraining. We de\ufb01ned insuf\ufb01cient training errors as\nerrors where the correct entity appeared less than\n10 times in the training data. We saw that the\nmodel followed the MPS in 75% of these cases,\nshowing that our model tends to follow the base-\nline in such cases. Further, the amount of gen-\neralization error in insuf\ufb01cient-training conditions\nwas also signi\ufb01cant (35.7%), as our model tended\nto select more general entities. 7 Conclusions\nOur results indicate that the expressibility of\nattention-RNNs indeed allows us to extract use-\nful features from noisy context, when suf\ufb01cient\namounts of training examples are available. This\nallows our model to signi\ufb01cantly out-perform ex-\nisting state-of-the-art models. We \ufb01nd that both\nusing pre-initialized embedding vocabularies, and\nthe corrupt-sampling method employed are very\nimportant for properly training our model. However, the gap between results of all systems\ntested on both CoNLL-Y AGO and WikilinksNED\nindicates that mentions with noisy context are in-\ndeed a challenging test. We believe this to be\nan important real-world scenario, that represents\na distinct test-case that \ufb01lls a gap between existing\nnews-based datasets and the much noisier Twitter\ndata (Ritter et al., 2011) that has received increas-\ning attention. We \ufb01nd recurrent neural models are\na promising direction for this task. Finally, our error analysis shows a number of\npossible improvements that should be addressed. Since we use the training set for candidate genera-\ntion, non-nonsensical candidates (i.e. disambigua-\ntion pages) cause our model to err and should be\nremoved from the candidate set. In addition, we\nobserve that lack of suf\ufb01cient training for long-\n\ntail entities is still a problem, even when a large\ntraining set is available. We believe this, and some\nsubtle semantic cases (book/movie) can be at least\npartially addressed by considering semantic prop-\nerties of entities, such as types and categories. We\nintend to address these issues in future work. References\nMart\u00b4\u0131n Abadi, Ashish Agarwal, Paul Barham, Eugene\nBrevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-\nrado, Andy Davis, Jeffrey Dean, Matthieu Devin,\nSanjay Ghemawat, Ian Goodfellow, Andrew Harp,\nGeoffrey Irving, Michael Isard, Yangqing Jia, Rafal\nJozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh\nLevenberg, Dan Man\u00b4e, Rajat Monga, Sherry Moore,\nDerek Murray, Chris Olah, Mike Schuster, Jonathon\nShlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-\nwar, Paul Tucker, Vincent Vanhoucke, Vijay Va-\nsudevan, Fernanda Vi \u00b4egas, Oriol Vinyals, Pete\nWarden, Martin Wattenberg, Martin Wicke, Yuan\nYu, and Xiaoqiang Zheng. 2015. TensorFlow:\nLarge-scale machine learning on heterogeneous sys-\ntems. Software available from tensor\ufb02ow.org. http://tensor\ufb02ow.org/. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2014. Neural machine translation by\njointly learning to align and translate. CoRR\nabs/1409.0473.\n\n\u201cindoor\ngames\u201d, \u201cplayed\u201d) from the noisy ones. We therefore propose a new model that lever-\nages local contextual information to disambiguate\nentities. Our neural approach (based on RNNs\nwith attention) leverages the vast amount of train-\ning data in WikilinksNED to learn representations\narXiv:1706.09147v2  [cs.CL]  1 Jul 2017\n\nfor entity and context, allowing it to extract signals\nfrom noisy and unexpected context patterns. While convolutional neural networks (Sun\net al., 2015; Francis-Landau et al., 2016) and prob-\nabilistic attention (Lazic et al., 2015) have been\napplied to the task, this is the \ufb01rst model to use\nRNNs and a neural attention model for NED. RNNs account for the sequential nature of textual\ncontext while the attention model is applied to re-\nduce the impact of noise in the text. Our experiments show that our model signi\ufb01-\ncantly outperforms existing state-of-the-art NED\nalgorithms on WikilinksNED, suggesting that\nRNNs with attention are able to model short and\nnoisy context better than current approaches. In\naddition, we evaluate our algorithm on CoNLL-\nY AGO (Hoffart et al., 2011), a dataset of annotated\nnews articles. We use a simple domain adapta-\ntion technique since CoNLL-Y AGO lacks a large\nenough training set for our model, and achieve\ncomparable results to other state-of-the-art meth-\nods. These experiments highlight the difference\nbetween the two datasets, indicating that our NED\nbenchmark is substantially more challenging. Code and data used for our experiments\ncan be found at https://github.com/\nyotam-happy/NEDforNoisyText\n2 Related Work\nLocal vs Global NED Early work on Named\nEntity Disambiguation, such as Bunescu and\nPas \u00b8ca (2006) and Mihalcea and Csomai (2007) fo-\ncused on local approaches where each mention is\ndisambiguated separately using hand-crafted fea-\ntures. While local approaches provide a hard-to-\nbeat baseline (Ratinov et al., 2011), recent work\nhas largely focused on global approaches. These\ndisambiguate all mentions within a document si-\nmultaneously by considering the coherency of en-\ntity assignments within a document. For exam-\nple the local component of the GLOW algorithm\n(Ratinov et al., 2011) was used as part of the re-\nlational inference system suggested by Cheng and\nRoth (2013). Similarly, Globerson et al. (2016)\nachieved state-of-the-art results by extending the\nlocal-based selective-context model of Lazic et al. (2015) with an attention-like coherence model. Global models can tap into highly-\ndiscriminative semantic signals (e.g. coreference\nand entity relatedness) that are unavailable to local\nmethods, and have signi\ufb01cantly outperformed the\nlocal approach on standard datasets (Guo and Bar-\nbosa, 2014; Pershina et al., 2015; Globerson et al.,\n2016). However, global approaches are dif\ufb01cult\nto apply in domains where only short and noisy\ntext is available, as often occurs in social media,\nquestions and answers, and other short web docu-\nments. For example, Huang et al. (2014) collected\nmany tweets from the same author in order to\napply a global disambiguation method. Since this\nwork focuses on disambiguating entities within\nshort fragments of text, our algorithmic approach\ntries to extract as much information from the local\ncontext, without resorting to external signals. Neural Approaches The \ufb01rst neural approach\nfor NED (He et al., 2013) used stacked auto-\nencoders to learn a similarity measure between\nmention-context structures and entity candidates. More recently, convolutional neural networks\n(CNNs) were employed for learning semantic sim-\nilarity between context, mention, and candidate\ninputs (Sun et al., 2015; Francis-Landau et al.,\n2016). Neural embedding techniques have also\ninspired a number of works that measure entity-\ncontext relatedness using their embeddings (Ya-\nmada et al., 2016; Hu et al., 2015). In this paper,\nwe train a recurrent neural network (RNN) model,\nwhich unlike CNNs and embeddings, is designed\nto exploit the sequential nature of text. We also\nutilize an attention mechanism, inspired by results\nfrom Lazic et al. (2015) that successfully used a\nprobabilistic attention-like model for NED. Noisy Data Chisholm and Hachey (2015)\nshowed that despite the noisy nature of web data,\naugmenting Wikipedia-derived data with web-\nlinks from the Wikilinks corpus (Singh et al.,\n2012) can improve performance on standard\ndatasets. In our work, we \ufb01nd noisy web data to\nbe a unique and challenging test case for disam-\nbiguation. We therefore use Wikilinks to construct\na new stand-alone disambiguation benchmark that\nfocuses on noisy text, rather than use it for training\nalone. Moreover, we differ from Chisholm at el. by taking a neural approach that implicitly discov-\ners useful signals from contexts, instead of manu-\nally crafting features. Commonly-used benchmarks for NED sys-\ntems have mostly focused on news-based cor-\npora. CoNLL-Y AGO (Hoffart et al., 2011) is\na dataset based on Reuters, created by hand-\nannotating the CoNLL 2003 Named Entity Recog-\n\nnition task dataset with Y AGO (Suchanek et al.,\n2007) entities. It contains 1, 393 documents split\ninto train, development and test sets.",
            1.098130832999999
          ]
        }
      ],
      "reference_answers": [
        "They initialize their word and entity embeddings with vectors pre-trained over a large corpus of unlabeled data.",
        "The authors believe that the Wikilinks corpus  contains ground truth annotations while being noisy. They discard mentions that cannot have ground-truth verified by comparison with Wikipedia."
      ]
    },
    "Risk_Prediction_of_Digital_Human_Resource_Manageme.pdf": {
      "error": "No valid question-answer pairs to evaluate",
      "average_exact_match": 0.0,
      "average_bleu_score": 0.0,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        },
        "rouge-2": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        },
        "rouge-L": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        }
      },
      "per_question_metrics": []
    },
    "1707.02377.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.007836024554469773,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.11272863008585292,
          "recall": 0.404040404040404,
          "f1": 0.175632183908046
        },
        "rouge-2": {
          "precision": 0.02482269503546099,
          "recall": 0.08045977011494253,
          "f1": 0.03794037940379404
        },
        "rouge-L": {
          "precision": 0.05987308697275103,
          "recall": 0.21313131313131314,
          "f1": 0.09314942528735633
        }
      },
      "per_question_metrics": [
        {
          "question": "Which language models do they compare against?",
          "generated_answer": "The language models they compare against are Word2Vec and Paragraph Vectors.",
          "reference_answer": "RNNLM BIBREF11",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            }
          },
          "bleu_score": 0,
          "exact_match": 0.0
        },
        {
          "question": "Is their approach similar to making an averaged weighted sum of word vectors, where weights reflect word frequencies?",
          "generated_answer": "No, their approach, as described in the context, does not simply make an averaged weighted sum of word vectors where weights reflect word frequencies. Instead, Doc2VecC represents each document as a simple average of the word embeddings of all the words in the document, with a corruption model that randomly removes words from a document during learning. This process introduces a data-dependent regularization, favoring informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. This approach contrasts with merely weighting word vectors by their frequencies.",
          "reference_answer": "Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained.",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.21052631578947367,
              "recall": 0.6666666666666666,
              "f1": 0.32
            },
            "rouge-2": {
              "precision": 0.07446808510638298,
              "recall": 0.2413793103448276,
              "f1": 0.11382113821138212
            },
            "rouge-L": {
              "precision": 0.11578947368421053,
              "recall": 0.36666666666666664,
              "f1": 0.17600000000000002
            }
          },
          "bleu_score": 0.017521057199848773,
          "exact_match": 0.0
        },
        {
          "question": "How do they determine which words are informative?",
          "generated_answer": "They determine which words are informative by using a data-dependent regularization technique in Doc2VecC that suppresses the embeddings of frequent but uninformative words, such as symbols and stop words. This technique favors informative or rare words while dampening the embeddings of common and non-discriminative words.",
          "reference_answer": "Informative are those that will not be suppressed by regularization performed.",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.1276595744680851,
              "recall": 0.5454545454545454,
              "f1": 0.2068965517241379
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.06382978723404255,
              "recall": 0.2727272727272727,
              "f1": 0.10344827586206895
            }
          },
          "bleu_score": 0.005987016463560547,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "Which language models do they compare against?",
        "Is their approach similar to making an averaged weighted sum of word vectors, where weights reflect word frequencies?",
        "How do they determine which words are informative?"
      ],
      "generated_answers": [
        {
          "answer": "The language models they compare against are Word2Vec and Paragraph Vectors.",
          "relevant_chunks": [
            "(2015). Character-aware neural language models. arXiv preprint arXiv:1508.06615. Kiros, R., Zhu, Y ., Salakhutdinov, R. R., Zemel, R., Urtasun, R., Torralba, A., and Fidler, S. (2015). Skip-thought vectors. In Advances in neural information processing systems, pages 3294\u20133302. Kusner, M. J., Sun, Y ., Kolkin, N. I., and Weinberger, K. Q.\n\nd. and Hinton, G. (2008). Visualizing data using t-sne. Journal of Machine Learning\nResearch, 9(Nov):2579\u20132605. Marelli, M., Bentivogli, L., Baroni, M., Bernardi, R., Menini, S., and Zamparelli, R. (2014). Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sen-\ntences through semantic relatedness and textual entailment. SemEval-2014. Mesnil, G., Mikolov, T., Ranzato, M., and Bengio, Y . (2014). Ensemble of generative and discrimi-\nnative techniques for sentiment analysis of movie reviews. arXiv preprint arXiv:1412.5335. Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013a). Ef\ufb01cient estimation of word representa-\ntions in vector space. arXiv preprint arXiv:1301.3781. Mikolov, T. and Dean, J. (2013). Distributed representations of words and phrases and their compo-\nsitionality. Advances in neural information processing systems. Mikolov, T., Kara\ufb01 \u00b4at, M., Burget, L., Cernock `y, J., and Khudanpur, S. (2010). Recurrent neural\nnetwork based language model. In Interspeech, volume 2, page 3. Mikolov, T., Yih, W.-t., and Zweig, G. (2013b). Linguistic regularities in continuous space word\nrepresentations. In HLT-NAACL, volume 13, pages 746\u2013751. Mitchell, J. and Lapata, M. (2010). Composition in distributional models of semantics. Cognitive\nscience, 34(8):1388\u20131429. Salton, G. and Buckley, C.\n\nThe experiments were conducted on a desktop with Intel i7 2.2Ghz cpu. Data dependent regularization. As explained in Section 3.1, the corruption introduced in\nDoc2VecC acts as a data-dependent regularization that suppresses the embeddings of frequent but\nuninformative words. Here we conduct an experiment to exam the effect. We used a cutoff of 100\nin this experiment. Table 3 lists the words having the smallest l2 norm of embeddings found by\ndifferent algorithms. The number inside the parenthesis after each word is the number of times this\nword appears in the learning set. In word2Vec or Paragraph Vectors, the least frequent words have\nembeddings that are close to zero, despite some of them being indicative of sentiment such as deba-\ncle, bliss and shabby. In contrast, Doc2VecC manages to clamp down the representation of words\nfrequently appear in the training set, but are uninformative, such as symbols and stop words. Subsampling frequent words. Note that for all the numbers reported, we applied the trick of\nsubsampling of frequent words introduced in (Mikolov and Dean, 2013) to counter the imbalance\nbetween frequent and rare words. It is critical to the performance of simple Word2Vec+A VG as the\nsole remedy to diminish the contribution of common words in the \ufb01nal document representation. If\nwe were to remove this step, the error rate of Word2Vec+A VG will increases from12.1% to 13.2%. Doc2VecC on the other hand naturally exerts a stronger regularization toward embeddings of words\nthat are frequent but uninformative, therefore does not rely on this trick. 4.3 W ORD ANALOGY\nIn table 3, we demonstrated that the corruption model introduced in Doc2VecC dampens the embed-\ndings of words which are common and non-discriminative (stop words). In this experiment, we are\ngoing to quantatively compare the word embeddings generated by Doc2VecC to the ones generated\nby Word2Vec, or Paragraph Vectors on the word analogy task introduced by Mikolov et al. (2013a). The dataset contains \ufb01ve types of semantic questions, and nine types of syntactic questions, with a\ntotal of 8,869 semantic and 10,675 syntactic questions. The questions are answered through simple\nlinear algebraic operations on the word embeddings generated by different methods. Please refer to\nthe original paper for more details on the evaluation protocol. 2As reported in the original paper, training of the skip-thought vector model on the book corpus dataset\ntakes around 2 weeks on GPU. 7\n\nPublished as a conference paper at ICLR 2017\n1M 2M 4M 8M 15M\n0\n20\n40\n60\n3.8 6.1 8.3 9.1\n13.3\n18.7\n26.4\n32.7\n36.1 38.9\n20.3\n28.1\n36.4\n42.5\n46.7\nNumber of paragraphs used for learning\nAccuracy (%)\nParagraphVectorsWord2VecDoc2VecC\n(a) h=50\n1M 2M 4M 8M 15M\n0\n20\n40\n60\n5.1 7.5\n10.9 10.2 10.2\n23.6\n34.7\n42.4\n48.2 50.7\n24.3\n34.1\n44.1\n52.6\n58.2\nNumber of paragraphs used for learning\nParagraphVectorsWord2VecDoc2VecC\n(b) h=100\nFigure 2: Accuracy on subset of the Semantic-Syntactic Word Relationship test set. Only questions\ncontaining words from the most frequent 30k words are included in the test. Semantic questions Word2Vec Doc2VecC Syntactic questions Word2Vec Doc2VecC\ncapital-common-countries 73.59 81.82 gram1-adjective-to-adverb 19.25 20.32\ncapital-world 67.94 77.96 gram2-opposite 14.07 25.54\ncurrency 17.14 12.86 gram3-comparative 60.21 74.47\ncity-in-state 34.49 42.86 gram4-superlative 52.87 55.40\nfamily 68.71 64.62 gram5-present-participle 56.34 65.81\ngram6-nationality-adjective 88.71 91.03\ngram7-past-tense 47.05 51.86\ngram8-plural 50.28 61.27\ngram9-plural-verbs 25.38 39.69\nTable 4: Top 1 accuracy on the 5 type of semantics and 9 types of syntactic questions. We trained the word embeddings of different methods using the English news dataset released under\nthe ACL workshop on statistical machine translation. The training set includes close to 15M para-\ngraphs with 355M tokens. We compare the performance of word embeddings trained by different\nmethods with increasing embedding dimensionality as well as increasing training data. We observe similar trends as in Mikolov et al. (2013a). Increasing embedding dimensionality as\nwell as training data size improves performance of the word embeddings on this task. However, the\nimprovement is diminishing. Doc2VecC produces word embeddings which performs signi\ufb01cantly\nbetter than the ones generated by Word2Vec. We observe close to 20% uplift when we train on the\nfull training corpus. Paragraph vectors on the other hand performs surprisingly bad on this dataset. Our hypothesis is that due to the large capacity of the model architecture, Paragraph Vectors relies\nmostly on the unique document vectors to capture the information in a text document instead of\nlearning the word semantic or syntactic similarities. This also explains why the PV-DBOW Le and\nMikolov (2014) model architecture proposed in the original work, which completely removes word\nembedding layers, performs comparable to the distributed memory version. In table 5, we list a detailed comparison of the performance of word embeddings generated by\nWord2Vec and Doc2VecC on the 14 subtasks, when trained on the full dataset with embedding of\nsize 100. We can see that Doc2VecC signi\ufb01cantly outperforms the word embeddings produced by\nWord2Vec across almost all the subtasks. 4.4 D OCUMENT CLASSIFICATION\nFor the document classi\ufb01cation task, we use a subset of the wikipedia dump, which contains over\n300,000 wikipedia pages in 100 categories. The 100 categories includes categories under sports,\n8\n\nPublished as a conference paper at ICLR 2017\nTable 5: Classi\ufb01cation error (%) of a linear classi\ufb01er trained on various document representations\non the Wikipedia dataset. Model BOW DEA Word2Vec + A VG Word2Vec + IDF ParagraphVectors Doc2VecC\nh= 100 36.03 32.30 33.2 33.16 35.78 31.92\nh= 200 36.03 31.36 32.46 32.48 34.92 30.84\nh= 500 36.03 31.10 32.02 32.13 33.93 30.43\nh= 1000 36.03 31.13 31.78 32.06 33.02 30.24\n(a) Doc2Vec\n (b) Doc2VecC\nFigure 3: Visualization of document vectors on Wikipedia dataset using t-SNE. entertainment, literature, and politics etc.",
            0.870664583000007
          ]
        },
        {
          "answer": "No, their approach, as described in the context, does not simply make an averaged weighted sum of word vectors where weights reflect word frequencies. Instead, Doc2VecC represents each document as a simple average of the word embeddings of all the words in the document, with a corruption model that randomly removes words from a document during learning. This process introduces a data-dependent regularization, favoring informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. This approach contrasts with merely weighting word vectors by their frequencies.",
          "relevant_chunks": [
            "(1988). Term-weighting approaches in automatic text retrieval.Informa-\ntion processing & management, 24(5):513\u2013523. Socher, R., Karpathy, A., Le, Q.\n\nPublished as a conference paper at ICLR 2017\nEFFICIENT VECTOR REPRESENTATION FOR DOCU -\nMENTS THROUGH CORRUPTION\nMinmin Chen\nCriteo Research\nPalo Alto, CA 94301, USA\nm.chen@criteo.com\nABSTRACT\nWe present an ef\ufb01cient document representation learning framework, Document\nVector through Corruption (Doc2VecC). Doc2VecC represents each document as\na simple average of word embeddings. It ensures a representation generated as\nsuch captures the semantic meanings of the document during learning. A cor-\nruption model is included, which introduces a data-dependent regularization that\nfavors informative or rare words while forcing the embeddings of common and\nnon-discriminative ones to be close to zero. Doc2VecC produces signi\ufb01cantly\nbetter word embeddings than Word2Vec. We compare Doc2VecC with several\nstate-of-the-art document representation learning algorithms. The simple model\narchitecture introduced by Doc2VecC matches or out-performs the state-of-the-art\nin generating high-quality document representations for sentiment analysis, doc-\nument classi\ufb01cation as well as semantic relatedness tasks. The simplicity of the\nmodel enables training on billions of words per hour on a single machine. At\nthe same time, the model is very ef\ufb01cient in generating representations of unseen\ndocuments at test time. 1 I NTRODUCTION\nText understanding starts with the challenge of \ufb01nding machine-understandable representation that\ncaptures the semantics of texts. Bag-of-words (BoW) and its N-gram extensions are arguably the\nmost commonly used document representations. Despite its simplicity, BoW works surprisingly\nwell for many tasks (Wang and Manning, 2012). However, by treating words and phrases as unique\nand discrete symbols, BoW often fails to capture the similarity between words or phrases and also\nsuffers from sparsity and high dimensionality. Recent works on using neural networks to learn distributed vector representations of words have\ngained great popularity. The well celebrated Word2Vec (Mikolov et al., 2013a), by learning to\npredict the target word using its neighboring words, maps words of similar meanings to nearby\npoints in the continuous vector space. The surprisingly simple model has succeeded in generating\nhigh-quality word embeddings for tasks such as language modeling, text understanding and machine\ntranslation. Word2Vec naturally scales to large datasets thanks to its simple model architecture. It\ncan be trained on billions of words per hour on a single machine. Paragraph Vectors (Le and Mikolov, 2014) generalize the idea to learn vector representation for\ndocuments. A target word is predicted by the word embeddings of its neighbors in together with\na unique document vector learned for each document. It outperforms established document repre-\nsentations, such as BoW and Latent Dirichlet Allocation (Blei et al., 2003), on various text under-\nstanding tasks (Dai et al., 2015). However, two caveats come with this approach: 1) the number of\nparameters grows with the size of the training corpus, which can easily go to billions; and 2) it is\nexpensive to generate vector representations for unseen documents at test time. We propose an ef\ufb01cient model architecture, referred to as Document Vector through Corruption\n(Doc2VecC), to learn vector representations for documents. It is motivated by the observation that\nlinear operations on the word embeddings learned by Word2Vec can sustain substantial amount of\nsyntactic and semantic meanings of a phrase or a sentence (Mikolov et al., 2013b). For exam-\nple, vec(\u201cRussia\u201d) + vec(\u201criver\u201d) is close to vec(\u201cV olga River\u201d) (Mikolov and Dean, 2013), and\n1\narXiv:1707.02377v1  [cs.CL]  8 Jul 2017\n\nPublished as a conference paper at ICLR 2017\nvec(\u201cking\u201d) - vec(\u201cman\u201d) + vec(\u201cwomen\u201d) is close to vec(\u201cqueen\u201d) (Mikolov et al., 2013b). In\nDoc2VecC, we represent each document as a simple average of the word embeddings of all the\nwords in the document. In contrast to existing approaches which post-process learned word em-\nbeddings to form document representation (Socher et al., 2013; Mesnil et al., 2014), Doc2VecC\nenforces a meaningful document representation can be formed by averaging the word embeddings\nduring learning. Furthermore, we include a corruption model that randomly remove words from a\ndocument during learning, a mechanism that is critical to the performance and learning speed of our\nalgorithm. Doc2VecC has several desirable properties: 1. The model complexity of Doc2VecC is decoupled\nfrom the size of the training corpus, depending only on the size of the vocabulary; 2. The model\narchitecture of Doc2VecC resembles that of Word2Vec, and can be trained very ef\ufb01ciently; 3. The\nnew framework implicitly introduces a data-dependent regularization, which favors rare or informa-\ntive words and suppresses words that are common but not discriminative; 4. Vector representation\nof a document can be generated by simply averaging the learned word embeddings of all the words\nin the document, which signi\ufb01cantly boost test ef\ufb01ciency; 5. The vector representation generated by\nDoc2VecC matches or beats the state-of-the-art for sentiment analysis, document classi\ufb01cation as\nwell as semantic relatedness tasks. 2 R ELATED WORKS AND NOTATIONS\nText representation learning has been extensively studied. Popular representations range from the\nsimplest BoW and its term-frequency based variants (Salton and Buckley, 1988), language model\nbased methods (Croft and Lafferty, 2013; Mikolov et al., 2010; Kim et al., 2015), topic mod-\nels (Deerwester et al., 1990; Blei et al., 2003), Denoising Autoencoders and its variants (Vincent\net al., 2008; Chen et al., 2012), and distributed vector representations (Mesnil et al., 2014; Le and\nMikolov, 2014; Kiros et al., 2015). Another prominent line of work includes learning task-speci\ufb01c\ndocument representation with deep neural networks, such as CNN (Zhang and LeCun, 2015) or\nLSTM based approaches (Tai et al., 2015; Dai and Le, 2015). In this section, we brie\ufb02y introduce Word2Vec and Paragraph Vectors, the two approaches that are\nmost similar to ours. There are two well-know model architectures used for both methods, referred\nto as Continuous Bag-of-Words (CBoW) and Skipgram models (Mikolov et al., 2013a). In this\nwork, we focus on CBoW. Extending to Skipgram is straightforward. Here are the notations we are\ngoing to use throughout the paper:\nD= {D1,\u00b7\u00b7\u00b7 ,Dn}: a training corpus of size n, in which each document Di contains a variable-\nlength sequence of words w1\ni ,\u00b7\u00b7\u00b7 ,wTi\ni ;\nV: the vocabulary used in the training corpus, of sizes v;\nx \u2208Rv\u00d71: BoW of a document, where xj = 1 iff word jdoes appear in the document. ct \u2208Rv\u00d71: BoW of the local context wt\u2212k,\u00b7\u00b7\u00b7 ,wt\u22121,wt+1,\u00b7\u00b7\u00b7 ,wt+k at the target position t. ct\nj = 1 iff word jappears within the sliding window of the target;\nU \u2208Rh\u00d7v: the projection matrix from the input space to a hidden space of size h. We use uw to\ndenote the column in U for word w, i.e., the \u201cinput\u201c vector of wordw;\nV\u22a4 \u2208Rv\u00d7h: the projection matrix from the hidden space to output. Similarly, we use vw to\ndenote the column in V for word w, i.e., the \u201coutput\u201c vector of wordw. Word2Vec. Word2Vec proposed a neural network architecture of an input layer, a projection layer\nparameterized by the matrix U and an output layer by V\u22a4. It de\ufb01nes the probability of observing\nthe target word wt in a document Dgiven its local context ct as\nP(wt|ct) = exp(v\u22a4\nwt Uct)\u2211\nw\u2032\u2208V exp(v\u22a4\nw\u2032 Uct)\nThe word vectors are then learned to maximize the log likelihood of observing the target word at\neach position of the document. Various techniques (Mitchell and Lapata, 2010; Zanzotto et al.,\n2010; Yessenalina and Cardie, 2011; Grefenstette et al., 2013; Socher et al., 2013; Kusner et al.,\n2\n\nPublished as a conference paper at ICLR 2017\n2015) have been studied to generate vector representations of documents from word embeddings,\namong which the simplest approach is to use weighted average of word embeddings. Similarly,\nour method forms document representation by averaging word embeddings of all the words in the\ndocument. Differently, as our model encodes the compositionality of words in the learned word\nembeddings, heuristic weighting at test time is not required. Paragraph Vectors. Paragraph Vectors, on the other hands, explicitly learns a document vector\nwith the word embeddings. It introduces another projection matrix D \u2208Rh\u00d7n. Each column of D\nacts as a memory of the global topic of the corresponding document. It then de\ufb01nes the probability\nof observing the target word wt in a document Dgiven its local context ct as\nP(wt|ct,d) = exp(v\u22a4\nwt (Uct + d))\u2211\nw\u2032\u2208V exp(v\u22a4\nw\u2032 (Uct + d))\nwhere d \u2208D is the vector representation of the document. As we can see from this formula, the\ncomplexity of Paragraph Vectors grows with not only the size of the vocabulary, but also the size of\nthe training corpus. While we can reasonably limit the size of a vocabulary to be within a million\nfor most datasets, the size of a training corpus can easily go to billions. What is more concerning is\nthat, in order to come up with the vector representations of unseen documents, we need to perform\nan expensive inference by appending more columns to D and gradient descent on D while \ufb01xing\nother parameters of the learned model. 3 M ETHOD\nSeveral works (Mikolov and Dean, 2013; Mikolov et al., 2013b) showcased that syntactic and se-\nmantic regularities of phrases and sentences are reasonably well preserved by adding or subtract-\ning word embeddings learned through Word2Vec. It prompts us to explore the option of simply\nrepresenting a document as an average of word embeddings. Figure 1 illustrates the new model\narchitecture. wt#1 Wt+1 Wt+2 wp wq wr\nwt\nopening for the performance praised brazil\nceremony\nword<vectors\ndocument< vector\nAverage/Concatenate\nFigure 1: A new framework for learning document vectors. Similar to Word2Vec or Paragraph Vectors, Doc2VecC consists of an input layer, a projection layer\nas well as an output layer to predict the target word, \u201cceremony\u201d in this example. The embeddings of\nneighboring words (\u201copening\u201d, \u201cfor\u201d, \u201cthe\u201d) provide local context while the vector representation of\nthe entire document (shown in grey) serves as the global context. In contrast to Paragraph Vectors,\nwhich directly learns a unique vector for each document, Doc2VecC represents each document as\nan average of the embeddings of words randomly sampled from the document (\u201cperformance\u201d at\nposition p, \u201cpraised\u201d at position q, and \u201cbrazil\u201d at position r). Huang et al. (2012) also proposed the idea of using average of word embeddings to represent the\nglobal context of a document. Different from their work, we choose to corrupt the original document\nby randomly removing signi\ufb01cant portion of words, and represent the document using only the\nembeddings of the words remained. This corruption mechanism offers us great speedup during\ntraining as it signi\ufb01cantly reduces the number of parameters to update in back propagation. At the\nsame time, as we are going to detail in the next section, it introduces a special form of regularization,\nwhich brings great performance improvement. 3\n\nPublished as a conference paper at ICLR 2017\nHere we describe the stochastic process we used to generate a global context at each update. The\nglobal context, which we denote as\u02dcx, is generated through a unbiasedmask-out/drop-out corruption,\nin which we randomly overwrites each dimension of the original document x with probability q. To\nmake the corruption unbiased, we set the uncorrupted dimensions to 1/(1 \u2212q) times its original\nvalue. Formally,\n\u02dcxd =\n{\n0, with probability q\nxd\n1\u2212q , otherwise (1)\nDoc2VecC then de\ufb01nes the probability of observing a target word wt given its local context ct as\nwell as the global context \u02dcx as\nP(wt|ct,\u02dcx) =\nexp(v\u22a4\nwt (\nlocal context\ued17\ued1a\ued19\ued18\nUct +\nglobal context\n\ued17\ued1a\ued19\ued18\n1\nTU\u02dcx ))\n\u2211\nw\u2032\u2208V exp(v\u22a4\nw\u2032\n(\nUct + 1\nT U\u02dcx)\n) (2)\nHere T is the length of the document. Exactly computing the probability is impractical, instead we\napproximate it with negative sampling (Mikolov et al., 2013a). f(w,c,\u02dcx) \u2261 log P(wt|ct,\u02dcx)\n\u2248 log \u03c3\n(\nv\u22a4\nw(Uc + 1\nTU\u02dcx)\n)\n+\n\u2211\nw\u2032\u223cPv\nlog \u03c3\n(\n\u2212v\u22a4\nw\u2032 (Uc + 1\nTU\u02dcx)\n)\n(3)\nhere Pv stands for a uniform distribution over the terms in the vocabulary. The two projection\nmatrices U and V are then learned to minimize the loss:\n\u2113= \u2212\nn\u2211\ni=1\nTi\u2211\nt=1\nf(wt\ni,ct\ni,\u02dcxt\ni) (4)\nGiven the learned projection matrix U, we then represent each document simply as an average of\nthe embeddings of the words in the document,\nd = 1\nT\n\u2211\nw\u2208D\nuw. (5)\nWe are going to elaborate next why we choose to corrupt the original document with the corruption\nmodel in eq.(1) during learning, and how it enables us to simply use the average word embeddings\nas the vector representation for documents at test time. 3.1 C ORRUPTION AS DATA -DEPENDENT REGULARIZATION\nWe approximate the log likelihood for each instance f(w,c,\u02dcx) in eq.(4) with its Taylor expansion\nwith respect to \u02dcx up to the second-order (Van Der Maaten et al., 2013; Wager et al., 2013; Chen\net al., 2014). Concretely, we choose to expand at the mean of the corruption \u00b5x = Ep(\u02dcx|x)[\u02dcx]:\nf(w,c,\u02dcx) \u2248f(w,c,\u00b5x) + (\u02dcx \u2212\u00b5x)\u22a4\u2207\u02dcxf + 1\n2(\u02dcx \u2212\u00b5x)\u22a4\u22072\n\u02dcxf(\u02dcx \u2212\u00b5x)\nwhere \u2207\u02dcxf and \u22072\n\u02dcxf are the \ufb01rst-order (i.e., gradient) and second-order (i.e., Hessian) of the log\nlikelihood with respect to \u02dcx. Expansion at the mean \u00b5x is crucial as shown in the following steps. Let us assume that for each instance, we are going to sample the global context \u02dcx in\ufb01nitely many\ntimes, and thus compute the expected log likelihood with respect to the corrupted \u02dcx. Ep(\u02dcx|x)[f(w,c,\u02dcx)] \u2248f(w,c,\u00b5x) + 1\n2tr\n(\nE[(\u02dcx \u2212x)(\u02dcx \u2212x)\u22a4]\u22072\n\u02dcxf\n)\nThe linear term disappears as Ep(\u02dcx|x)[\u02dcx \u2212\u00b5x] = 0 . We substitute in x for the mean \u00b5x of the\ncorrupting distribution (unbiased corruption) and the matrix \u03a3x = E[(\u02dcx \u2212\u00b5x)(\u02dcx \u2212\u00b5x)\u22a4] for the\nvariance, and obtain\nEp(\u02dcx|x)[f(w,c,\u02dcx)] \u2248f(w,c,x) + 1\n2tr\n(\n\u03a3x\u22072\n\u02dcxf\n)\n(6)\n4\n\nPublished as a conference paper at ICLR 2017\nAs each word in a document is corrupted independently of others, the variance matrix \u03a3x is simpli-\n\ufb01ed to a diagonal matrix with jth element equals q\n1\u2212q x2\nj. As a result, we only need to compute the\ndiagonal terms of the Hessian matrix \u22072\n\u02dcxf. The jth dimension of the Hessian\u2019s diagonal evaluated at the meanx is given by\n\u22022f\n\u2202x2\nj\n= \u2212\u03c3w,c,x(1 \u2212\u03c3w,c,x)( 1\nTv\u22a4\nwuj)2 \u2212\n\u2211\nw\u2032\u223cPv\n\u03c3w\u2032,c,x(1 \u2212\u03c3w\u2032,c,x)( 1\nTv\u22a4\nw\u2032 uj)2\nPlug the Hessian matrix and the variance matrix back into eq.(6), and then back to the loss de\ufb01ned\nin eq.(4), we can see that Doc2VecC intrinsically minimizes\n\u2113= \u2212\nn\u2211\ni=1\nTi\u2211\nt=1\nf(wt\ni,ct\ni,xi) + q\n1 \u2212q\nv\u2211\nj=1\nR(uj) (7)\nEach f(wt\ni,ct\ni,xi) in the \ufb01rst term measures the log likelihood of observing the target word wt\ni\ngiven its local context ct\ni and the document vector di = 1\nT Uxi. As such, Doc2VecC enforces that a\ndocument vector generated by averaging word embeddings can capture the global semantics of the\ndocument, and \ufb01ll in information missed in the local context. The second term here is a data-dependent regularization. The regularization on the embedding uj\nof each word jtakes the following form,\nR(uj) \u221d\nn\u2211\ni=1\nTi\u2211\nt=1\nx2\nij\n[\n\u03c3wt\ni,ct\ni,xi (1 \u2212\u03c3wt\ni,ct\ni,xi )( 1\nTv\u22a4\nwt\ni\nuj)2 +\n\u2211\nw\u2032\u223cPv\n\u03c3w\u2032,ct\ni,xi (1 \u2212\u03c3w\u2032,ct\ni,xi )( 1\nTv\u22a4\nw\u2032 uj)2\n]\nwhere \u03c3w,c,x = \u03c3(v\u22a4\nw(Uc+ 1\nT Ux)) prescribes the con\ufb01dence of predicting the target wordwgiven\nits neighboring context c as well as the document vector d = 1\nT Ux. Closely examining R(uj) leads to several interesting \ufb01ndings: 1. the regularizer penalizes more\non the embeddings of common words. A word jthat frequently appears across the training corpus,\ni.e, xij = 1 often, will have a bigger regularization than a rare word; 2. on the other hand, the\nregularization is modulated by \u03c3w,c,x(1 \u2212\u03c3w,c,x), which is small if \u03c3w,c,x \u21921 or 0. In other\nwords, if uj is critical to a con\ufb01dent prediction \u03c3w,c,x when it is active, then the regularization is\ndiminished. Similar effect was observed for dropout training for logistic regression model (Wager\net al., 2013) and denoising autoencoders (Chen et al., 2014). 4 E XPERIMENTS\nWe evaluate Doc2VecC on a sentiment analysis task, a document classi\ufb01cation task and a semantic\nrelatedness task, along with several document representation learning algorithms. All experiments\ncan be reproduced using the code available at https://github.com/mchen24/iclr2017\n4.1 B ASELINES\nWe compare against the following document representation baselines: bag-of-words (BoW); De-\nnoising Autoencoders (DEA) (Vincent et al., 2008), a representation learned from reconstruct-\ning original document x using corrupted one \u02dcx. SDAs have been shown to be the state-of-the-art\nfor sentiment analysis tasks (Glorot et al., 2011). We used Kullback-Liebler divergence as the re-\nconstruction error and an af\ufb01ne encoder. To scale up the algorithm to large vocabulary, we only\ntake into account the non-zero elements of x in the reconstruction error and employed negative\nsampling for the remainings; Word2Vec (Mikolov et al., 2013a)+IDF, a representation generated\nthrough weighted average of word vectors learned using Word2Vec; Doc2Vec (Le and Mikolov,\n2014); Skip-thought Vectors(Kiros et al., 2015), a generic, distributed sentence encoder that ex-\ntends the Word2Vec skip-gram model to sentence level. It has been shown to produce highly generic\nsentence representations that apply to various natural language processing tasks. We also include\nRNNLM (Mikolov et al., 2010), a recurrent neural network based language model in the compar-\nison. In the semantic relatedness task, we further compare to LSTM-based methods (Tai et al.,\n2015) that have been reported on this dataset. 5\n\nPublished as a conference paper at ICLR 2017\nTable 1: Classi\ufb01cation error of a linear classi\ufb01er trained on various document representations on the\nImdb dataset. Model Error rate % (include test) Error rate % (exclude test)\nBag-of-Words (BOW) 12.53 12.59\nRNN-LM 13.59 13.59\nDenoising Autoencoders (DEA) 11.58 12.54\nWord2Vec + A VG 12.11 12.69\nWord2Vec + IDF 11.28 11.92\nParagraph Vectors 10.81 12.10\nSkip-thought Vectors - 17.42\nDoc2VecC 10.48 11.70\n4.2 S ENTIMENT ANALYSIS\nFor sentiment analysis, we use the IMDB movie review dataset. It contains 100,000 movies reviews\ncategorized as either positive or negative. It comes with prede\ufb01ned train/test split (Maas et al.,\n2011): 25,000 reviews are used for training, 25,000 for testing, and the rest as unlabeled data. The\ntwo classes are balanced in the training and testing sets. We remove words that appear less than 10\ntimes in the training set, resulting in a vocabulary of 43,375 distinct words and symbols. Setup. We test the various representation learning algorithms under two settings: one follows the\nsame protocol proposed in (Mesnil et al., 2014), where representation is learned using all the avail-\nable data, including the test set; another one where the representation is learned using training and\nunlabeled set only. For both settings, a linear support vector machine (SVM) (Fan et al., 2008)\nis trained afterwards on the learned representation for classi\ufb01cation. For Skip-thought Vectors, we\nused the generic model1 trained on a much bigger book corpus to encode the documents. A vector of\n4800 dimensions, \ufb01rst 2400 from the uni-skip model, and the last 2400 from the bi-skip model, are\ngenerated for each document. In comparison, all the other algorithms produce a vector representa-\ntion of size 100. The supervised RNN-LM is learned on the training set only. The hyper-parameters\nare tuned on a validation set subsampled from the training set. Accuracy. Comparing the two columns in Table 1, we can see that all the representation learn-\ning algorithms bene\ufb01ts from including the testing data during the representation learning phrase. Doc2VecC achieved similar or even better performance than Paragraph Vectors. Both methods\noutperforms the other baselines, beating the BOW representation by 15%. In comparison with\nWord2Vec+IDF, which applies post-processing on learned word embeddings to form document rep-\nresentation, Doc2VecC naturally enforces document semantics to be captured by averaged word\nembeddings during training. This leads to better performance. Doc2VecC reduces to Denoising Au-\ntoencoders (DEA) if the local context words are removed from the paradigm shown in Figure 1. By\nincluding the context words, Doc2VecC allows the document vector to focus more on capturing the\nglobal context. Skip-thought vectors perform surprisingly poor on this dataset comparing to other\nmethods. We hypothesized that it is due to the length of paragraphs in this dataset. The average\nlength of paragraphs in the IMDB movie review dataset is 296.5, much longer than the ones used\nfor training and testing in the original paper, which is in the order of 10. As noted in (Tai et al.,\n2015), the performance of LSTM based method (similarly, the gated RNN used in Skip-thought\nvectors) drops signi\ufb01cantly with increasing paragraph length, as it is hard to preserve state over long\nsequences of words. Time. Table 2 summarizes the time required by these algorithms to learn and generate the document\nrepresentation. Word2Vec is the fastest one to train. Denoising Autoencoders and Doc2VecC second\nthat. The number of parameters that needs to be back-propagated in each update was increased by\nthe number of surviving words in \u02dcx. We found that both models are not sensitive to the corruption\nrate q in the noise model. Since the learning time decreases with higher corruption rate, we used\nq = 0 .9 throughout the experiments. Paragraph Vectors takes longer time to train as there are\nmore parameters (linear to the number of document in the learning set) to learn. At test time,\nWord2Vec+IDF, DEA and Doc2VecC all use (weighted) averaging of word embeddings as document\n1available at https://github.com/ryankiros/skip-thoughts\n6\n\nPublished as a conference paper at ICLR 2017\nTable 2: Learning time and representation generation time required by different representation learn-\ning algorithms. Model Learning time Generation time\nDenoising Autoencoders 3m 23s 7s\nWord2Vec + IDF 2m 33s 7s\nParagraph Vectors 4m 54s 4m 17s\nSkip-thought 2h 2h\nDoc2VecC 4m 30s 7s\nTable 3: Words with embeddings closest to 0 learned by different algorithms. Word2Vec harp(118) distasteful(115) switzerland(101) shabby(103) \ufb01reworks(101) heav-\nens(100) thornton(108) endeavor(100) dense(108) circumstance(119) debacle(103)\nParaVectors harp(118) dense(108) reels(115) \ufb01reworks(101) its\u2019(103) unnoticed(112) pony(102)\nful\ufb01lled(107) heavens(100) bliss(110) canned(114) shabby(103) debacle(103)\nDoc2VecC ,(1099319) .(1306691) the(1340408) of(581667) and(651119) up(49871) to(537570)\nthat(275240) time(48205) endeavor(100) here(21118) way(31302) own(13456)\nrepresentation. Paragraph Vectors, on the other hand, requires another round of inference to produce\nthe vector representation of unseen test documents. It takes Paragraph Vectors 4 minutes and 17\nseconds to infer the vector representations for the 25,000 test documents, in comparison to 7 seconds\nfor the other methods. As we did not re-train the Skip-thought vector models on this dataset, the\ntraining time2 reported in the table is the time it takes to generate the embeddings for the 25,000\ntraining documents. Due to repeated high-dimensional matrix operations required for encoding long\nparagraphs, it takes fairly long time to generate the representations for these documents. Similarly\nfor testing.\n\n(2015). From word embeddings to\ndocument distances. In Proceedings of the 32nd International Conference on Machine Learning\n(ICML 2015), pages 957\u2013966. 11\n\nPublished as a conference paper at ICLR 2017\nLe, Q. V . and Mikolov, T. (2014). Distributed representations of sentences and documents. InICML,\nvolume 14, pages 1188\u20131196. Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y ., and Potts, C. (2011). Learning word\nvectors for sentiment analysis. In ACL, pages 142\u2013150. Maaten, L. v.",
            1.0552962080000157
          ]
        },
        {
          "answer": "They determine which words are informative by using a data-dependent regularization technique in Doc2VecC that suppresses the embeddings of frequent but uninformative words, such as symbols and stop words. This technique favors informative or rare words while dampening the embeddings of common and non-discriminative words.",
          "relevant_chunks": [
            "(1988). Term-weighting approaches in automatic text retrieval.Informa-\ntion processing & management, 24(5):513\u2013523. Socher, R., Karpathy, A., Le, Q.\n\nThe experiments were conducted on a desktop with Intel i7 2.2Ghz cpu. Data dependent regularization. As explained in Section 3.1, the corruption introduced in\nDoc2VecC acts as a data-dependent regularization that suppresses the embeddings of frequent but\nuninformative words. Here we conduct an experiment to exam the effect. We used a cutoff of 100\nin this experiment. Table 3 lists the words having the smallest l2 norm of embeddings found by\ndifferent algorithms. The number inside the parenthesis after each word is the number of times this\nword appears in the learning set. In word2Vec or Paragraph Vectors, the least frequent words have\nembeddings that are close to zero, despite some of them being indicative of sentiment such as deba-\ncle, bliss and shabby. In contrast, Doc2VecC manages to clamp down the representation of words\nfrequently appear in the training set, but are uninformative, such as symbols and stop words. Subsampling frequent words. Note that for all the numbers reported, we applied the trick of\nsubsampling of frequent words introduced in (Mikolov and Dean, 2013) to counter the imbalance\nbetween frequent and rare words. It is critical to the performance of simple Word2Vec+A VG as the\nsole remedy to diminish the contribution of common words in the \ufb01nal document representation. If\nwe were to remove this step, the error rate of Word2Vec+A VG will increases from12.1% to 13.2%. Doc2VecC on the other hand naturally exerts a stronger regularization toward embeddings of words\nthat are frequent but uninformative, therefore does not rely on this trick. 4.3 W ORD ANALOGY\nIn table 3, we demonstrated that the corruption model introduced in Doc2VecC dampens the embed-\ndings of words which are common and non-discriminative (stop words). In this experiment, we are\ngoing to quantatively compare the word embeddings generated by Doc2VecC to the ones generated\nby Word2Vec, or Paragraph Vectors on the word analogy task introduced by Mikolov et al. (2013a). The dataset contains \ufb01ve types of semantic questions, and nine types of syntactic questions, with a\ntotal of 8,869 semantic and 10,675 syntactic questions. The questions are answered through simple\nlinear algebraic operations on the word embeddings generated by different methods. Please refer to\nthe original paper for more details on the evaluation protocol. 2As reported in the original paper, training of the skip-thought vector model on the book corpus dataset\ntakes around 2 weeks on GPU. 7\n\nPublished as a conference paper at ICLR 2017\n1M 2M 4M 8M 15M\n0\n20\n40\n60\n3.8 6.1 8.3 9.1\n13.3\n18.7\n26.4\n32.7\n36.1 38.9\n20.3\n28.1\n36.4\n42.5\n46.7\nNumber of paragraphs used for learning\nAccuracy (%)\nParagraphVectorsWord2VecDoc2VecC\n(a) h=50\n1M 2M 4M 8M 15M\n0\n20\n40\n60\n5.1 7.5\n10.9 10.2 10.2\n23.6\n34.7\n42.4\n48.2 50.7\n24.3\n34.1\n44.1\n52.6\n58.2\nNumber of paragraphs used for learning\nParagraphVectorsWord2VecDoc2VecC\n(b) h=100\nFigure 2: Accuracy on subset of the Semantic-Syntactic Word Relationship test set. Only questions\ncontaining words from the most frequent 30k words are included in the test. Semantic questions Word2Vec Doc2VecC Syntactic questions Word2Vec Doc2VecC\ncapital-common-countries 73.59 81.82 gram1-adjective-to-adverb 19.25 20.32\ncapital-world 67.94 77.96 gram2-opposite 14.07 25.54\ncurrency 17.14 12.86 gram3-comparative 60.21 74.47\ncity-in-state 34.49 42.86 gram4-superlative 52.87 55.40\nfamily 68.71 64.62 gram5-present-participle 56.34 65.81\ngram6-nationality-adjective 88.71 91.03\ngram7-past-tense 47.05 51.86\ngram8-plural 50.28 61.27\ngram9-plural-verbs 25.38 39.69\nTable 4: Top 1 accuracy on the 5 type of semantics and 9 types of syntactic questions. We trained the word embeddings of different methods using the English news dataset released under\nthe ACL workshop on statistical machine translation. The training set includes close to 15M para-\ngraphs with 355M tokens. We compare the performance of word embeddings trained by different\nmethods with increasing embedding dimensionality as well as increasing training data. We observe similar trends as in Mikolov et al. (2013a). Increasing embedding dimensionality as\nwell as training data size improves performance of the word embeddings on this task. However, the\nimprovement is diminishing. Doc2VecC produces word embeddings which performs signi\ufb01cantly\nbetter than the ones generated by Word2Vec. We observe close to 20% uplift when we train on the\nfull training corpus. Paragraph vectors on the other hand performs surprisingly bad on this dataset. Our hypothesis is that due to the large capacity of the model architecture, Paragraph Vectors relies\nmostly on the unique document vectors to capture the information in a text document instead of\nlearning the word semantic or syntactic similarities. This also explains why the PV-DBOW Le and\nMikolov (2014) model architecture proposed in the original work, which completely removes word\nembedding layers, performs comparable to the distributed memory version. In table 5, we list a detailed comparison of the performance of word embeddings generated by\nWord2Vec and Doc2VecC on the 14 subtasks, when trained on the full dataset with embedding of\nsize 100. We can see that Doc2VecC signi\ufb01cantly outperforms the word embeddings produced by\nWord2Vec across almost all the subtasks. 4.4 D OCUMENT CLASSIFICATION\nFor the document classi\ufb01cation task, we use a subset of the wikipedia dump, which contains over\n300,000 wikipedia pages in 100 categories. The 100 categories includes categories under sports,\n8\n\nPublished as a conference paper at ICLR 2017\nTable 5: Classi\ufb01cation error (%) of a linear classi\ufb01er trained on various document representations\non the Wikipedia dataset. Model BOW DEA Word2Vec + A VG Word2Vec + IDF ParagraphVectors Doc2VecC\nh= 100 36.03 32.30 33.2 33.16 35.78 31.92\nh= 200 36.03 31.36 32.46 32.48 34.92 30.84\nh= 500 36.03 31.10 32.02 32.13 33.93 30.43\nh= 1000 36.03 31.13 31.78 32.06 33.02 30.24\n(a) Doc2Vec\n (b) Doc2VecC\nFigure 3: Visualization of document vectors on Wikipedia dataset using t-SNE. entertainment, literature, and politics etc.\n\nExamples of categories include American drama \ufb01lms,\nDirectorial debut \ufb01lms, Major League Baseball pitchers and Sydney Swans players. Body texts\n(the second paragraph) were extracted for each page as a document. For each category, we select\n1,000 documents with unique category label, and 100 documents were used for training and 900\ndocuments for testing. The remaining documents are used as unlabeled data. The 100 classes\nare balanced in the training and testing sets. For this data set, we learn the word embedding and\ndocument representation for all the algorithms using all the available data. We apply a cutoff of 10,\nresulting in a vocabulary of size 107,691. Table 5 summarizes the classi\ufb01cation error of a linear SVM trained on representations of different\nsizes. We can see that most of the algorithms are not sensitive to the size of the vector represen-\ntation. Doc2Vec bene\ufb01ts most from increasing representation size. Across all sizes of representa-\ntions, Doc2VecC outperform the existing algorithms by a signi\ufb01cant margin. In fact, Doc2VecC can\nachieve same or better performance with a much smaller representation vector. Figure 4: Visualization of Wikipedia Doc2VecC\nvectors using t-SNE. Figure 3 visualizes the document representa-\ntions learned by Doc2Vec (left) and Doc2VecC\n(right) using t-SNE (Maaten and Hinton, 2008). We can see that documents from the same cat-\negory are nicely clustered using the representa-\ntion generated by Doc2VecC. Doc2Vec, on the\nother hand, does not produce a clear separation\nbetween different categories, which explains its\nworse performance reported in Table 5. Figure 4 visualizes the vector representation\ngenerated by Doc2VecC w.r.t. coarser catego-\nrization. we manually grouped the 100 cate-\ngories into 7 coarse categories, television, al-\nbums, writers, musicians, athletes, species and\nactors. Categories that do no belong to any of\nthese 7 groups are not included in the \ufb01gure. 9\n\nPublished as a conference paper at ICLR 2017\nWe can see that documents belonging to a coarser category are grouped together. This subset in-\ncludes is a wide range of sports descriptions, ranging from football, crickets, baseball, and cycling\netc., which explains why the athletes category are less concentrated. In the projection, we can see\ndocuments belonging to the musician category are closer to those belonging to albums category than\nthose of athletes or species. 4.5 S EMANTIC RELATEDNESS\nWe test Doc2VecC on the SemEval 2014 Task 1: semantic relatedness SICK dataset (Marelli et al.,\n2014). Given two sentences, the task is to determine how closely they are semantically related. The\nset contains 9,927 pairs of sentences with human annotated relatedness score, ranging from 1 to 5. A score of 1 indicates that the two sentences are not related, while 5 indicates high relatedness. The\nset is splitted into a training set of 4,500 instances, a validation set of 500, and a test set of 4,927. We compare Doc2VecC with several winning solutions of the competition as well as several more\nrecent techniques reported on this dataset, including bi-directional LSTM and Tree-LSTM 3 trained\nfrom scratch on this dataset, Skip-thought vectors learned a large book corpus 4 (Zhu et al., 2015)\nand produced sentence embeddings of 4,800 dimensions on this dataset. We follow the same proto-\ncol as in skip-thought vectors, and train Doc2VecC on the larger book corpus dataset. Contrary to\nthe vocabulary expansion technique used in (Kiros et al., 2015) to handle out-of-vocabulary words,\nwe extend the vocabulary of the learned model directly on the target dataset in the following way:\nwe use the pre-trained word embedding as an initialization, and \ufb01ne-tune the word and sentence\nrepresentation on the SICK dataset. Notice that the \ufb01ne-tuning is done for sentence representation\nlearning only, and we did not use the relatedness score in the learning. This step brings small im-\nprovement to the performance of our algorithm. Given the sentence embeddings, we used the exact\nsame training and testing protocol as in (Kiros et al., 2015) to score each pair of sentences: with\ntwo sentence embedding u1 and u2, we concatenate their component-wise product,u1 \u00b7u2 and their\nabsolute difference, |u1 \u2212u2|as the feature representation. Table 6 summarizes the performance of various algorithms on this dataset. Despite its simplicity,\nDoc2VecC signi\ufb01cantly out-performs the winning solutions of the competition, which are heavily\nfeature engineered toward this dataset and several baseline methods, noticeably the dependency-tree\nRNNs introduced in (Socher et al., 2014), which relies on expensive dependency parsers to compose\nsentence vectors from word embeddings. The performance of Doc2VecC is slightly worse than the\nLSTM based methods or skip-thought vectors on this dataset, while it signi\ufb01cantly outperforms\nskip-thought vectors on the IMDB movie review dataset ( 11.70% error rate vs 17.42%). As we\nhypothesized in previous section, while Doc2VecC is better at handling longer paragraphs, LSTM-\nbased methods are superior for relatively short sentences (of length in the order of 10s). We would\nlike to point out that Doc2VecC is much faster to train and test comparing to skip-thought vectors. It\ntakes less than 2 hours to learn the embeddings on the large book corpus for Doc2VecC on a desktop\nwith Intel i7 2.2Ghz cpu, in comparison to the 2 weeks on GPU required by skip-thought vectors. 5 C ONCLUSION\nWe introduce a new model architecture Doc2VecC for document representation learning. It is very\nef\ufb01cient to train and test thanks to its simple model architecture. Doc2VecC intrinsically makes sure\ndocument representation generated by averaging word embeddings capture semantics of document\nduring learning. It also introduces a data-dependent regularization which favors informative or rare\nwords while dampening the embeddings of common and non-discriminative words. As such, each\ndocument can be ef\ufb01ciently represented as a simple average of the learned word embeddings. In\ncomparison to several existing document representation learning algorithms, Doc2VecC outperforms\nnot only in testing ef\ufb01ciency, but also in the expressiveness of the generated representations. 3The word representation was initialized using publicly available 300-dimensional Glove vectors trained on\n840 billion tokens of Common Crawl data\n4The dataset contains 11,038 books with over one billion words\n10\n\nPublished as a conference paper at ICLR 2017\nTable 6: Test set results on the SICK semantic relatedness task. The \ufb01rst group of results are from the\nsubmission to the 2014 SemEval competition; the second group includes several baseline methods\nreported in (Tai et al., 2015); the third group are methods based on LSTM reported in (Tai et al.,\n2015) as well as the skip-thought vectors (Kiros et al., 2015). Method Pearson\u2019s\u03b3 Spearman\u2019s\u03c1 MSE\nIllinois-LH 0.7993 0.7538 0.3692\nUNAL-NLP 0.8070 0.7489 0.3550\nMeaning Factory 0.8268 0.7721 0.3224\nECNU 0.8279 0.7689 0.3250\nMean vectors (Word2Vec + avg) 0.7577 0.6738 0.4557\nDT-RNN (Socher et al., 2014) 0.7923 0.7319 0.3822\nSDT-RNN (Socher et al., 2014) 0.7900 0.7304 0.3848\nLSTM (Tai et al., 2015) 0.8528 0.7911 0.2831\nBidirectional LSTM (Tai et al., 2015) 0.8567 0.7966 0.2736\nDependency Tree-LSTM (Tai et al., 2015) 0.8676 0.8083 0.2532\ncombine-skip (Kiros et al., 2015) 0.8584 0.7916 0.2687\nDoc2VecC 0.8381 0.7621 0.3053\nREFERENCES\nBlei, D. M., Ng, A.",
            0.5158034590000113
          ]
        }
      ],
      "reference_answers": [
        "RNNLM BIBREF11",
        "Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained.",
        "Informative are those that will not be suppressed by regularization performed."
      ]
    },
    "1801.02243.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.07195491187593171,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.13560334528076465,
          "recall": 0.42857142857142855,
          "f1": 0.20418679549114335
        },
        "rouge-2": {
          "precision": 0.0857142857142857,
          "recall": 0.3333333333333333,
          "f1": 0.13636363636363635
        },
        "rouge-L": {
          "precision": 0.12485065710872163,
          "recall": 0.4047619047619047,
          "f1": 0.18937198067632852
        }
      },
      "per_question_metrics": [
        {
          "question": "Do the authors give any examples of major events which draw the public's attention and the impact they have on stock price?",
          "generated_answer": "No, the authors do not provide specific examples of major events that draw the public's attention and their impact on stock price in the provided context.",
          "reference_answer": "Yes",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            }
          },
          "bleu_score": 0,
          "exact_match": 0.0
        },
        {
          "question": "Which tweets are used to output the daily sentiment signal?",
          "generated_answer": "To output the daily sentiment signal, tweets with multiple sentences are used, and the average of the sentiment scores of all sentences is calculated as the sentiment score of the tweets.",
          "reference_answer": "Tesla and Ford are investigated on how Twitter sentiment could impact the stock price",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.12903225806451613,
              "recall": 0.2857142857142857,
              "f1": 0.17777777777777778
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.0967741935483871,
              "recall": 0.21428571428571427,
              "f1": 0.13333333333333333
            }
          },
          "bleu_score": 0.0077426364109627836,
          "exact_match": 0.0
        },
        {
          "question": "What is the baseline machine learning prediction approach?",
          "generated_answer": "The baseline machine learning prediction approach applies linear logistic regression to a set of stock technical signals to predict the following day's stock return sign (+ or -). No sentiment features are provided to the baseline model.",
          "reference_answer": "linear logistic regression to a set of stock technical signals",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.2777777777777778,
              "recall": 1.0,
              "f1": 0.4347826086956522
            },
            "rouge-2": {
              "precision": 0.2571428571428571,
              "recall": 1.0,
              "f1": 0.40909090909090906
            },
            "rouge-L": {
              "precision": 0.2777777777777778,
              "recall": 1.0,
              "f1": 0.4347826086956522
            }
          },
          "bleu_score": 0.20812209921683233,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "Do the authors give any examples of major events which draw the public's attention and the impact they have on stock price?",
        "Which tweets are used to output the daily sentiment signal?",
        "What is the baseline machine learning prediction approach?"
      ],
      "generated_answers": [
        {
          "answer": "No, the authors do not provide specific examples of major events that draw the public's attention and their impact on stock price in the provided context.",
          "relevant_chunks": [
            "INTRODUCTION\nIn a world where traditional \ufb01nancial information is ubiq-\nuitous and the \ufb01nancial models are largely homogeneous,\n\ufb01nding hidden information that has not been priced in from\nalternative data is critical. The recent development in Natural\nLanguage Processing provides such opportunities to look\ninto text data in addition to numerical data. When the\nmarket sets the stock price, it is not uncommon that the\nexpectation of the company growth outweighs the company\nfundamentals. Twitter, a online news and social network\nwhere the users post and interact with messages to express\nviews about certain topics, contains valuable information on\nthe public mood and sentiment. A collection of research [1]\n[6] have shown that there is a positive correlation between\nthe \u201dpublic mood\u201d and the \u201dmarket mood\u201d. Other research[2]\nalso shows that signi\ufb01cant correlation exists between the\nTwitter sentiment and the abnormal return during the peaks\nof the Twitter volume during a major event. Once a signal that has predicting power on the stock\nmarket return is constructed, a trading strategy to express the\nview of the signal is needed. Traditionally, the quantitative \ufb01-\nnance industry relies on backtest, a process where the trading\nstrategies are tuned during the simulations or optimizations. Reinforcement learning provides a way to \ufb01nd the optimal\npolicy by maximizing the expected future utility. There are\nrecent attempts from the Arti\ufb01cial Intelligence community\nto apply reinforcement learning to asset allocation [5], algo-\nrithmic trading[3][7], and portfolio management[4]. The contribution of this paper is two-fold: First, the pre-\ndicting power of Twitter sentiment is evaluated. Our results\nshow sentiment is more suitable to construct alpha signals\nrather than total return signals and shows predicting power\nespecially when the Twitter volume is high. Second, we\nproposed a trading strategy based on reinforcement learning\n(Q-learning) that takes the sentiment features as part of its\nstates. The paper is constructed as follows: In the second section,\nscraping Tweets from Twitter website and preprocessing the\ndata are described in details. In the third section, assigning\nsentiment scores to the text data is discussed. In the fourth\nsection, feature engineering and prediction based on the\nsentiment score is discussed. In the \ufb01fth section, how the\nreinforcement learning is applied to generate the optimal\ntrading strategy is described. II. T WITTER DATA SCRAPING AND PREPROCESSING\nThere are two options of getting the Tweets. First, Twitter\nprovides an API to download the Tweets. However, rate limit\nand history limit make it not an option for this paper. Second,\nscrapping Tweets directly from Twitter website. Using the\nsecond option, the daily Tweets for stocks of interest from\n2015 January to 2017 June were downloaded. The predicting power of Twitter sentiment varies from\nstock to stock. For stocks that are mostly driven by the com-\npany fundamentals and hold by the institutional investors,\nthe predicting power of the Twitter sentiment is limited. For stocks that are priced by the public expectation on the\ncompany\u2019s future growth, Twitter sentiment describes the\ncon\ufb01dence and expectation level of the investors. For this reason, two companies from the same industry,\nTesla and Ford are investigated on how Twitter sentiment\ncould impact the stock price. Tesla is an electronic car\ncompany that shows consecutive negative operating cash \ufb02ow\nand net income but carries very high expectation from the\npublic. Ford, is a traditional auto maker whose stock prices\nhas been stabilized to represent the company fundamentals. To investigate how different key words impact the predict-\ning power of the sentiment score, two Tweet sets, a ticker\nset and a product set, are prepared for each stock. The \ufb01rst\nset of Tweets are searched strictly according to the stock\nticker. The second set of Tweets are searched according to\nthe company\u2019s products and news. The keywords for the\nsecond dataset are de\ufb01ned according to the top twenty related\nkeywords of the stock ticker according to Google Trend, a\nweb facility shows how often a certain word is searched\nrelative to Google\u2019s total search volume. For example, \u201dElon\nMusk\u201d is among the set of keywords that retrieve the second\ntweets set for Tesla. Tweets contain irregular symbols, url and emoji etc which\nhas to be preprocessed so that the NLP algorithm can extract\nthe relevant information ef\ufb01ciently. Examples of preprocess-\ning are described as below:\n\u2022 Filter out tweets that contains http or .com Motiva-\ntion: They\u2019re usually ads e.g. #Fhotoroom #iPhone\narXiv:1801.02243v1  [cs.AI]  7 Jan 2018\n\nhttps://www.fhotoroom.com/fhotos/\n\u2022 Remove #hashtag, @user, tabs and extra spaces\n\u2022 Filter out tweets that contain consecutive two question\nmarks (?).\n\n5. The chart shows the trading strategy derived from Q-learning(in\nblue) outperform the backtest result using machine learning features(in red). Both of Q-learning strategy and machine learning strategy outperform the\nbaseline(in green). There are observations that worth a discussion. At the\nbeginning of the testing period, the Q-learning has not learnt\nhow to estimate the expected utility of a certain action yet. The performance of the initial period is more unstable than\n4\n\nlater. Q-learning does better when the state is more common\nbecause it accumulates more experience about the situation\nbut might not take the best action when a outlier state is\npresented. The performance of the q-learning varies during\ndifferent batch due to the random nature of exploitation\nand exploration. In general Q-learning is able to deliver\nbetter performance than using the binary prediction from\nthe machine learning models. Both of the Q-learning and\nmachine learning model outperform the baseline model. VI. F UTURE WORK\nThere are many areas that can be improved given more\nresource and data. Below is a list of the improvement that\ncould make this idea more robust. \u2022 Use intraday data to test the sentiment signal and Q-\nlearning. By training with more data and trading more\npromptly, we expect both sentiment machine learning\nmodel and Q-learning to do better. \u2022 With more data, more features can be considered and\nincorporated into the model. \u2022 Apply different function approximators, for example,\nneural net, to better generalize the states and provide\nmore stable behavior\n\u2022 Add another class to the existing binary classi\ufb01er in-\nsigni\ufb01cant price change. The is motivated by preventing\nthe classi\ufb01er to \ufb01t to the noise inherent in stock market\nprice movement, and lumps small, statistically insignif-\nicant upward or downward movements indiscriminately\nwith large ones. \u2022 Add crude oil future price as a feature to predict Tesla\nstock alpha return sign\n\u2022 Extend the sentiment analysis to other stocks and Cryp-\ntocurrency, which is an asset class that driven even more\nby the public sentiment. VII. CONCLUSIONS\nThe paper explores the possibility to predict stock price\nusing text data and reinforcement learning technique. Pre-\ndicting stock price direction using Twitter sentiment is chal-\nlenging but promising. Which stock and what to predict is\nmore important than how to predict. For example, Tesla,\na company driven by the expectation of the company\u2019s\ngrowth is a better target than Ford, a traditional auto maker. Reinforcement learning is applied to \ufb01nd the optimal trading\npolicy by learning the feedbacks from the market. The Q-\nlearning is able to adapt automatically if the market regime\nshifts and avoid backtesting, a process applied by investment\nindustry that often over\ufb01t the historical data. Both of the ma-\nchine learning model and the Q-learning model outperforms\nthe baseline model, which is a logistic regression without\nsentiment features. ACKNOWLEDGMENT\nWe would like to thank Anna Wang, who is the project\nmentor, gives very practical suggestions and guidance. We\nwould like to thank Standford University for the very chal-\nlenging and exciting CS221 course materials and Prof. Percy\nLiang who did such a great job getting us interested in\nsentiment analysis and reinforcement learning\nREFERENCES\n[1] Johan Bollen and Huina Mao. Twitter mood as a stock market predictor. Computer, 44(10):91\u201394, 2011. [2] Johan Bollen, Huina Mao, and Xiao-Jun Zeng. Twitter mood predicts\nthe stock market. Journal of Computational Science , 2(1):1\u20138, March\n2011.\n\n3. The chart displays the accuracy on predicting the \u201dalpha\u201d, which\nde\ufb01nes as the return of the stock minus the return of its sector ETF. product dataset is searched using keywords that related to\nthe company\u2019s product and other related topic(see session II\nfor more details). The former dataset represents the investors\u2019\nsentiment, while the latter dataset represents customers sen-\ntiment. In the Tesla case, using product tweets consistently outper-\nforms using the ticker tweets(accuracy 0.6 vs 0.5), it is less\nso in the Ford case(0.58 vs 0.55). The result is displayed\nin Figure 3 First, this is because Tesla\u2019s stock price is\ndriven more by the sentiment on its product instead of the\nstock itself. For Ford, not many people actually express their\nopinion about Ford\u2019s product via Twitter. Secondly, Tesla has\nmany more product tweets than ticker tweets, but Ford is\nopposite. D. Predicting using logistic regression and SVM\nIn most cases, SVM performs only slightly better than\nlogistic regression in validation set, although much better\nin testing set. This may be because the dataset is not\nlarge enough to prevent SVM over\ufb01tting. The comparision\nbetween the logistic regression and the SVM is displayed in\nFigure 3\nE. Predicting Total Return vs Alpha\nIt is important to identify which is a better target for the\nprediction. Two targets, predicting \u201dalpha or predicting \u201dtotal\nreturn\u201d are compared. \u201dAlpha\u201d de\ufb01nes as the excess stock\nreturn over its sector ETF. \u201dTotal return\u201d is the absolution\nstock return. Predicting \u201dalpha\u201d achieves better performance\nthan predicting total return. This is because the sentiment\nis more related to stocks idiosyncratic. Good sentiments\ntowards a speci\ufb01c company or its stock wont override the\noverall stock market or sectors impact on the stock return. F . Tesla vs Ford\nThe prediction accuracy on Tesla is higher than Ford\naccording to Figure3. The reason is because Tesla\u2019s stock\nprice largely re\ufb02ects the sentiment and con\ufb01dence level of\nthe public. The company has consecutive negative cash \ufb02ow\nand net income, making prediction based on its fundamental\ninformation unrealistic. On the other hand, the stock price\nof Ford, which is a traditional automaker, is not that related\nto the public sentiment. G. Feature selection and over\ufb01tting\nTo improve the model accuracy, more features were con-\nstructed. However, more features do not result in better\naccuracy. For example, in Figure 4, adding more features\nimprove the training accuracy but deteriorates out-of-sample\naccuracy due to over\ufb01tting. Fig. 4. The chart shows an example of over\ufb01tting in the SVM model. The\nover\ufb01tting is caused by adding too many features to the model inputs but\nnot providing enough data for the model to generalize. Different lines shows\nthe SVM performance under different \u03b3 parameter. None of the parameter\nachieves better accuracy than a restricted set of features. The recursive feature elimination and cross validation\n(RFECV) for feature selection is experimented during the\nfeature selection phase. However, only similar or even\nslightly worse performance was achieved by RFECV than\nselecting features according to domain knowledge and intu-\nition. This is because recursive feature elimination is a greedy\nalgorithm and thus doesnt guarantee optimal solution. V. Q- LEARNING\nQ-learning is a model-free reinforcement learning tech-\nnique. Speci\ufb01cally, Q-learning can be used to \ufb01nd an optimal\npolicy given a Markov Decision Process(MDP). Instead of\nlearning the transition probability, Q-learning directly learns\nthe expected utility of taking an action from a certain state. By maximizing the expected utility of the certain state, the\noptimal policy is found. Traditionally, quants propose trading strategies according\nto backtest, where the optimal parameters are tuned by\nmaximizing the objective function based on historical data. However, this common practice adopted by the investment\nindustry has drawbacks. First, it over-\ufb01ts the historical data\nand doesn\u2019t generalize to out of sample data. In addition, the\nmodel need to be recalibrated periodically due to the eco-\nnomic regime change. A strategy signi\ufb01cantly outperforms\nin a high volatility environment might suffer signi\ufb01cantly in\na low volatility environment. 3\n\nThe Q-learning, in the opposite, learns from the feedback\nfrom the market, and generates the optimal trading strategy\naccording to past experience, and automatically adapts to the\nnew market regime. In this paper, the Q-learning algorithm is applied to gen-\nerate the optimal trading strategy. The market is modeled as\na Markov Decision Process where the outcomes are random\nand not under the control of the decision maker. The states\ncontain information of three categories: technical indicators,\nsentiment features and portfolio information. The actions\ncontains buy, sell and hold. The reward is the next day market\nreturn. The limit of leverage and the loss-cutting threshold\nare implemented in the relation ship of successor state and\naction. For example, if the leverage constrain has been met,\nthe actions that valid for this state are only \u201dhold\u201d or \u201dsell\u201d. If\nthe loss cutting threshold has been triggered, say the portfolio\nlost half of the capital and this is the maximum tolerance of\nloss, only the action that exit current position is valid. A.",
            1.247523459000007
          ]
        },
        {
          "answer": "To output the daily sentiment signal, tweets with multiple sentences are used, and the average of the sentiment scores of all sentences is calculated as the sentiment score of the tweets.",
          "relevant_chunks": [
            "INTRODUCTION\nIn a world where traditional \ufb01nancial information is ubiq-\nuitous and the \ufb01nancial models are largely homogeneous,\n\ufb01nding hidden information that has not been priced in from\nalternative data is critical. The recent development in Natural\nLanguage Processing provides such opportunities to look\ninto text data in addition to numerical data. When the\nmarket sets the stock price, it is not uncommon that the\nexpectation of the company growth outweighs the company\nfundamentals. Twitter, a online news and social network\nwhere the users post and interact with messages to express\nviews about certain topics, contains valuable information on\nthe public mood and sentiment. A collection of research [1]\n[6] have shown that there is a positive correlation between\nthe \u201dpublic mood\u201d and the \u201dmarket mood\u201d. Other research[2]\nalso shows that signi\ufb01cant correlation exists between the\nTwitter sentiment and the abnormal return during the peaks\nof the Twitter volume during a major event. Once a signal that has predicting power on the stock\nmarket return is constructed, a trading strategy to express the\nview of the signal is needed. Traditionally, the quantitative \ufb01-\nnance industry relies on backtest, a process where the trading\nstrategies are tuned during the simulations or optimizations. Reinforcement learning provides a way to \ufb01nd the optimal\npolicy by maximizing the expected future utility. There are\nrecent attempts from the Arti\ufb01cial Intelligence community\nto apply reinforcement learning to asset allocation [5], algo-\nrithmic trading[3][7], and portfolio management[4]. The contribution of this paper is two-fold: First, the pre-\ndicting power of Twitter sentiment is evaluated. Our results\nshow sentiment is more suitable to construct alpha signals\nrather than total return signals and shows predicting power\nespecially when the Twitter volume is high. Second, we\nproposed a trading strategy based on reinforcement learning\n(Q-learning) that takes the sentiment features as part of its\nstates. The paper is constructed as follows: In the second section,\nscraping Tweets from Twitter website and preprocessing the\ndata are described in details. In the third section, assigning\nsentiment scores to the text data is discussed. In the fourth\nsection, feature engineering and prediction based on the\nsentiment score is discussed. In the \ufb01fth section, how the\nreinforcement learning is applied to generate the optimal\ntrading strategy is described. II. T WITTER DATA SCRAPING AND PREPROCESSING\nThere are two options of getting the Tweets. First, Twitter\nprovides an API to download the Tweets. However, rate limit\nand history limit make it not an option for this paper. Second,\nscrapping Tweets directly from Twitter website. Using the\nsecond option, the daily Tweets for stocks of interest from\n2015 January to 2017 June were downloaded. The predicting power of Twitter sentiment varies from\nstock to stock. For stocks that are mostly driven by the com-\npany fundamentals and hold by the institutional investors,\nthe predicting power of the Twitter sentiment is limited. For stocks that are priced by the public expectation on the\ncompany\u2019s future growth, Twitter sentiment describes the\ncon\ufb01dence and expectation level of the investors. For this reason, two companies from the same industry,\nTesla and Ford are investigated on how Twitter sentiment\ncould impact the stock price. Tesla is an electronic car\ncompany that shows consecutive negative operating cash \ufb02ow\nand net income but carries very high expectation from the\npublic. Ford, is a traditional auto maker whose stock prices\nhas been stabilized to represent the company fundamentals. To investigate how different key words impact the predict-\ning power of the sentiment score, two Tweet sets, a ticker\nset and a product set, are prepared for each stock. The \ufb01rst\nset of Tweets are searched strictly according to the stock\nticker. The second set of Tweets are searched according to\nthe company\u2019s products and news. The keywords for the\nsecond dataset are de\ufb01ned according to the top twenty related\nkeywords of the stock ticker according to Google Trend, a\nweb facility shows how often a certain word is searched\nrelative to Google\u2019s total search volume. For example, \u201dElon\nMusk\u201d is among the set of keywords that retrieve the second\ntweets set for Tesla. Tweets contain irregular symbols, url and emoji etc which\nhas to be preprocessed so that the NLP algorithm can extract\nthe relevant information ef\ufb01ciently. Examples of preprocess-\ning are described as below:\n\u2022 Filter out tweets that contains http or .com Motiva-\ntion: They\u2019re usually ads e.g. #Fhotoroom #iPhone\narXiv:1801.02243v1  [cs.AI]  7 Jan 2018\n\nhttps://www.fhotoroom.com/fhotos/\n\u2022 Remove #hashtag, @user, tabs and extra spaces\n\u2022 Filter out tweets that contain consecutive two question\nmarks (?).\n\n\u2022 Filtering out none-English tweets using Googles\nLangdetect package. III. S ENTIMENT SCORE\nTo translate each tweet into a sentiment score, the Stanford\ncoreNLP software was used. Stanford CoreNLP is designed\nto make linguistic analysis accessible to the general public. It\nprovides named Entity Recognition, co-reference and basic\ndependencies and many other text understanding applica-\ntions. An example that illustrate the basic functionality of\nStanford coreNLP is shown in Figure.1\nFig. 1. Demo of the functionalities provided by Stanford CoreNLP\ncoreNLP can compute a sentiment score for each sentence\nwith value ranging from 0 to 4 , where 0 stands for negative,\nand 4 stands for very positive. For tweets with multiple\nsentences, the average of the sentiment scores of all sentences\nis used as the sentiment score of the Tweets. The number of Tweets varies everyday from a couple of\nhundreds to over ten thousands, depends on if the company\nhas a major event that attracts the public attention. The\nsentiment scores are normalized between 0 to 1, and features\nbased on the sentiment score is constructed and normalized. Figure 2 shows the relationship between Tesla stock return\nand stock sentiment score. According the distribution of the\nsentiment score, the sentiment on Tesla is slightly skewed\ntowards positive during the testing period. The price has\nbeen increased signi\ufb01cantly during the testing period, which\nre\ufb02ected the positive sentiment. The predicting power of\nsentiment score is more signi\ufb01cant when the sentiment is\nmore extreme and less so when the sentiment is neutral. IV.\n\nTrading the Twitter Sentiment with Reinforcement Learning\nCatherine Xiao\ncatherine.xiao1@gmail.com\nWanfeng Chen\nwanfengc@gmail.com\nAbstract\u2014 This paper is to explore the possibility to use\nalternative data and arti\ufb01cial intelligence techniques to trade\nstocks. The ef\ufb01cacy of the daily Twitter sentiment on predicting\nthe stock return is examined using machine learning methods. Reinforcement learning(Q-learning) is applied to generate the\noptimal trading policy based on the sentiment signal. The\npredicting power of the sentiment signal is more signi\ufb01cant\nif the stock price is driven by the expectation on the company\ngrowth and when the company has a major event that draws\nthe public attention. The optimal trading strategy based on\nreinforcement learning outperforms the trading strategy based\non the machine learning prediction. I.",
            0.5121275410000123
          ]
        },
        {
          "answer": "The baseline machine learning prediction approach applies linear logistic regression to a set of stock technical signals to predict the following day's stock return sign (+ or -). No sentiment features are provided to the baseline model.",
          "relevant_chunks": [
            "Learning\nFormally, the learning process de\ufb01nes as below. In Q-\nlearning the optimal expected utility of a (state, action) pair\n\u02c6Qopt(s,a) is updated with the rewards r and the expected\nutility of the subsequent state \u02c6Vopt(s\u2032) after taking the action\na. \u02c6Qopt(s,a) \u2190(1 \u2212\u03b7) \u02c6Qopt(s,a) +\u03b7(r+ \u03b3\u02c6Vopt(s\u2032) (1)\nVopt(s\u2032) = max\na\u2032\u2208Actions(s\u2032)\n\u02c6Qopt(s\u2032,a\u2032) (2)\nThe optimal policy is proposed by Q-learning as\n\u03c0opt(s) = arg max\na\u2208act(s)\nQopt(s,a) (3)\nB. Function Approximation\nFunction approximation refers to the method to generalize\nunseen states by applying machine learning methods. The Q-\ntable stores the expected utility for each (state,action) pair\nthat has been explored. When predicting the expected utility\nfor a certain (state, action) pair, we will look up the Q-table. When the MDP has many states and actions, it is very likely\nthat a (state, action) pair has not been explored yet so the\nestimate is not accurate. It is too slow to look up a gigantic\ntable and most likely there is not enough training data to\nlearn each of the state individually. Function approximation\nuses features to capture the characteristics of the states and\napplies stochastic gradient descent to update the weights on\neach feature. More speci\ufb01cally, below equation is applied\nto generalize the unseen state in this paper. De\ufb01ne features\n\u03c6(s,a) and weights w, then\n\u02c6Qopt(s,a; w) =w\u00b7\u03c6(s,a) (4)\nFor each (s,a,r,s \u2032), apply stochastic gradient descent to\nupdate the weights. w\u2190w\u2212\u03b7[ \u02c6Qopt(s,a; w) \u2212(r+ \u03b3\u02c6Vopt(s\u2032))]\u03c6(s,a) (5)\nwhere \u03b7 is the learning rate, r is the reward and \u03b3 is the\ndiscount factor. C. Exploration and Exploitation\nIt is necessary to balance the exploration and exploitation. One might suggest naively to take action only according\nto the optimal policy estimated by maximizing \u02c6Qopt(s,a). However, this greedy strategy is equivalent to stay in the\ncomfortable zone all the time in life, without gaining new\nexperience and unable to give reasonable prediction when\nencounters unseen situations. Another extreme is to always\nexplore by choosing an action randomly. Without apply-\ning the hard lesson learned and obtaining the rewards,\nthe algorithm can lead to unsatis\ufb01able utility at the end. Therefore, in this paper the Epsilon-greedy strategy is applied\nfor exploration. For a certain probability, the algorithm\nacts randomly(exploration), for the rest the algorithm acts\noptimally(exploitation). D. Result and Discussion\nFigure 5 shows the cumulative return over 1 year period. The strategy trades daily. The Q-learning states include\nportfolio position, sentiment features and technical indicators\nsuch as price momentum. The machine learning strategy\npredicts the binary movement (+ or -) of next trading day\nprice based on sentiment features and technical indicators. The backtest rule based on the machine learning prediction\nis to long the stock if the prediction is +, short the stock if -. The baseline is the same with machine learning except only\nthe technical indicator was used as the feature. The oracle\nmodel of this project is a trader who has insider information\nabout the stock and be able to bet and act correctly on every\nsingle day of the testing period. The oracle model is able\nto achieve 6 times of the initial capital at the end of testing\nperiod. Fig.\n\n2. Histogram\nfeatures reasonable relative to the training data size. Two\ncategories of features are de\ufb01nes: technical features and\nsentiment features. The technical features include previous\nday\u2019s return and volume, price momentum and volatility. The\nsentiment features include number of Tweets, daily average\nsentiment score, cross-section sentiment volatility, sentiment\nmomentum and reversal. B. Machine Learning Prediction Model\nThe logistic regression with L1 regularization and RBF-\nkernel SVM are applied to predict a binary outcome, i.e. whether the stock return will be positive or negative in the\nnext day. Both technical and sentiment-based features carry\nimportant information about the stock price and therefore are\nprovided as the model inputs. Half of the dataset is used for\ntraining and the rest is used for testing. The 3 fold cross validation is applied to learn the model\nhyper-parameters. Speci\ufb01cally, the hyper-parameters C of\nboth models and of RBF-kernel SVM are learned such that\nthe dev set accuracy is maximized. The hyper-parameter C\nin logistic regression determines the degree of regularization. Smaller C means more regularization, i.e. high bias and low\nvariance. RBF-kernel SVM has two hyper-parameters, C and\n. C controls the width of soft margin, smaller C allows\nplacing more samples on the wrong side of the margin. is a\nparameter in RBF kernel. A larger means a Gaussian with\nsmaller variance and thus less in\ufb02uence of support vectors. Typically, small C and large lead to high bias and low\nvariance. To evaluate if the sentiment feature improves the pre-\ndiction accuracy, a baseline model is de\ufb01ned. The baseline\napplies linear logistic regression to a set of stock technical\nsignals to predict the following days stock return sign (+/). No sentiment features are provided to the baseline model. C. Predicting using ticker dataset and product dataset\nThe predicting power for the ticker dataset and product\ndataset are compared. The ticker dataset contains tweets\nthat searched strictly according to the stock ticker. The\n2\n\nFig.\n\n3. The chart displays the accuracy on predicting the \u201dalpha\u201d, which\nde\ufb01nes as the return of the stock minus the return of its sector ETF. product dataset is searched using keywords that related to\nthe company\u2019s product and other related topic(see session II\nfor more details). The former dataset represents the investors\u2019\nsentiment, while the latter dataset represents customers sen-\ntiment. In the Tesla case, using product tweets consistently outper-\nforms using the ticker tweets(accuracy 0.6 vs 0.5), it is less\nso in the Ford case(0.58 vs 0.55). The result is displayed\nin Figure 3 First, this is because Tesla\u2019s stock price is\ndriven more by the sentiment on its product instead of the\nstock itself. For Ford, not many people actually express their\nopinion about Ford\u2019s product via Twitter. Secondly, Tesla has\nmany more product tweets than ticker tweets, but Ford is\nopposite. D. Predicting using logistic regression and SVM\nIn most cases, SVM performs only slightly better than\nlogistic regression in validation set, although much better\nin testing set. This may be because the dataset is not\nlarge enough to prevent SVM over\ufb01tting. The comparision\nbetween the logistic regression and the SVM is displayed in\nFigure 3\nE. Predicting Total Return vs Alpha\nIt is important to identify which is a better target for the\nprediction. Two targets, predicting \u201dalpha or predicting \u201dtotal\nreturn\u201d are compared. \u201dAlpha\u201d de\ufb01nes as the excess stock\nreturn over its sector ETF. \u201dTotal return\u201d is the absolution\nstock return. Predicting \u201dalpha\u201d achieves better performance\nthan predicting total return. This is because the sentiment\nis more related to stocks idiosyncratic. Good sentiments\ntowards a speci\ufb01c company or its stock wont override the\noverall stock market or sectors impact on the stock return. F . Tesla vs Ford\nThe prediction accuracy on Tesla is higher than Ford\naccording to Figure3. The reason is because Tesla\u2019s stock\nprice largely re\ufb02ects the sentiment and con\ufb01dence level of\nthe public. The company has consecutive negative cash \ufb02ow\nand net income, making prediction based on its fundamental\ninformation unrealistic. On the other hand, the stock price\nof Ford, which is a traditional automaker, is not that related\nto the public sentiment. G. Feature selection and over\ufb01tting\nTo improve the model accuracy, more features were con-\nstructed. However, more features do not result in better\naccuracy. For example, in Figure 4, adding more features\nimprove the training accuracy but deteriorates out-of-sample\naccuracy due to over\ufb01tting. Fig. 4. The chart shows an example of over\ufb01tting in the SVM model. The\nover\ufb01tting is caused by adding too many features to the model inputs but\nnot providing enough data for the model to generalize. Different lines shows\nthe SVM performance under different \u03b3 parameter. None of the parameter\nachieves better accuracy than a restricted set of features. The recursive feature elimination and cross validation\n(RFECV) for feature selection is experimented during the\nfeature selection phase. However, only similar or even\nslightly worse performance was achieved by RFECV than\nselecting features according to domain knowledge and intu-\nition. This is because recursive feature elimination is a greedy\nalgorithm and thus doesnt guarantee optimal solution. V. Q- LEARNING\nQ-learning is a model-free reinforcement learning tech-\nnique. Speci\ufb01cally, Q-learning can be used to \ufb01nd an optimal\npolicy given a Markov Decision Process(MDP). Instead of\nlearning the transition probability, Q-learning directly learns\nthe expected utility of taking an action from a certain state. By maximizing the expected utility of the certain state, the\noptimal policy is found. Traditionally, quants propose trading strategies according\nto backtest, where the optimal parameters are tuned by\nmaximizing the objective function based on historical data. However, this common practice adopted by the investment\nindustry has drawbacks. First, it over-\ufb01ts the historical data\nand doesn\u2019t generalize to out of sample data. In addition, the\nmodel need to be recalibrated periodically due to the eco-\nnomic regime change. A strategy signi\ufb01cantly outperforms\nin a high volatility environment might suffer signi\ufb01cantly in\na low volatility environment. 3\n\nThe Q-learning, in the opposite, learns from the feedback\nfrom the market, and generates the optimal trading strategy\naccording to past experience, and automatically adapts to the\nnew market regime. In this paper, the Q-learning algorithm is applied to gen-\nerate the optimal trading strategy. The market is modeled as\na Markov Decision Process where the outcomes are random\nand not under the control of the decision maker. The states\ncontain information of three categories: technical indicators,\nsentiment features and portfolio information. The actions\ncontains buy, sell and hold. The reward is the next day market\nreturn. The limit of leverage and the loss-cutting threshold\nare implemented in the relation ship of successor state and\naction. For example, if the leverage constrain has been met,\nthe actions that valid for this state are only \u201dhold\u201d or \u201dsell\u201d. If\nthe loss cutting threshold has been triggered, say the portfolio\nlost half of the capital and this is the maximum tolerance of\nloss, only the action that exit current position is valid. A.",
            0.5997013330000129
          ]
        }
      ],
      "reference_answers": [
        "Yes",
        "Tesla and Ford are investigated on how Twitter sentiment could impact the stock price",
        "linear logistic regression to a set of stock technical signals"
      ]
    },
    "1706.01875.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.10364899993640168,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.2570762570762571,
          "recall": 0.4943181818181818,
          "f1": 0.32842712842712846
        },
        "rouge-2": {
          "precision": 0.18530701754385964,
          "recall": 0.42857142857142855,
          "f1": 0.25483812624344365
        },
        "rouge-L": {
          "precision": 0.2570762570762571,
          "recall": 0.4943181818181818,
          "f1": 0.32842712842712846
        }
      },
      "per_question_metrics": [
        {
          "question": "What was the baseline?",
          "generated_answer": "The context provided does not specify a baseline for the study or measurements being discussed.",
          "reference_answer": "stochastic gradient descent, naive bayes, decision tree",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            }
          },
          "bleu_score": 0,
          "exact_match": 0.0
        },
        {
          "question": "What was their system's performance?",
          "generated_answer": "The system's performance, as described in the context, showed that their offensive speech classifier achieved an accuracy of 92.0% and an F1-score of 91.9% during 10-fold cross-validation on the training/validation set. Additionally, when evaluating the classifier on a holdout set, it achieved an accuracy and F1-score of 89.6% and 89.2%, respectively. These results indicate that the classifier was not only accurate but also robust against over-fitting.",
          "reference_answer": "accuracy and F1-score of 89.6% and 89.2%, respectively accuracy and F1-score of 89.6% and 89.2%, respectively",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.2077922077922078,
              "recall": 0.7272727272727273,
              "f1": 0.32323232323232326
            },
            "rouge-2": {
              "precision": 0.15789473684210525,
              "recall": 0.5714285714285714,
              "f1": 0.24742268041237112
            },
            "rouge-L": {
              "precision": 0.2077922077922078,
              "recall": 0.7272727272727273,
              "f1": 0.32323232323232326
            }
          },
          "bleu_score": 0.14410865307969575,
          "exact_match": 0.0
        },
        {
          "question": "What other political events are included in the database?",
          "generated_answer": "The database includes several divisive political events including the 2016 US presidential elections.",
          "reference_answer": "US presidential primaries, Democratic and Republican National Conventions",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.15384615384615385,
              "recall": 0.25,
              "f1": 0.1904761904761905
            },
            "rouge-2": {
              "precision": 0.08333333333333333,
              "recall": 0.14285714285714285,
              "f1": 0.10526315789473685
            },
            "rouge-L": {
              "precision": 0.15384615384615385,
              "recall": 0.25,
              "f1": 0.1904761904761905
            }
          },
          "bleu_score": 0.030206205252435548,
          "exact_match": 0.0
        },
        {
          "question": "What classifier did they use?",
          "generated_answer": "Random Forest Classifier",
          "reference_answer": "Random Forest",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.6666666666666666,
              "recall": 1.0,
              "f1": 0.8
            },
            "rouge-2": {
              "precision": 0.5,
              "recall": 1.0,
              "f1": 0.6666666666666666
            },
            "rouge-L": {
              "precision": 0.6666666666666666,
              "recall": 1.0,
              "f1": 0.8
            }
          },
          "bleu_score": 0.24028114141347542,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "What was the baseline?",
        "What was their system's performance?",
        "What other political events are included in the database?",
        "What classifier did they use?"
      ],
      "generated_answers": [
        {
          "answer": "The context provided does not specify a baseline for the study or measurements being discussed.",
          "relevant_chunks": [
            "Measuring Offensive Speech in Online Political Discourse\nRishab Nithyanand1, Brian Schaffner2, Phillipa Gill1\n1{rishab, phillipa}@cs.umass.edu, 2schaffne@polsci.umass.edu\nUniversity of Massachusetts, Amherst\nAbstract\nThe Internet and online forums such as Reddit have be-\ncome an increasingly popular medium for citizens to\nengage in political conversations. However, the on-\nline disinhibition effect resulting from the ability to use\npseudonymous identities may manifest in the form of\noffensive speech, consequently making political discus-\nsions more aggressive and polarizing than they already\nare. Such environments may result in harassment and\nself-censorship from its targets. In this paper, we present\npreliminary results from a large-scale temporal measure-\nment aimed at quantifying offensiveness in online politi-\ncal discussions. To enable our measurements, we develop and evalu-\nate an offensive speech classi\ufb01er. We then use this clas-\nsi\ufb01er to quantify and compare offensiveness in the po-\nlitical and general contexts. We perform our study us-\ning a database of over 168M Reddit comments made by\nover 7M pseudonyms between January 2015 and January\n2017 \u2013 a period covering several divisive political events\nincluding the 2016 US presidential elections. 1 Introduction\nThe apparent rise in political incivility has attracted sub-\nstantial attention from scholars in recent years. These\nstudies have largely focused on the extent to which politi-\ncians and elected of\ufb01cials are increasingly employing\nrhetoric that appears to violate norms of civility [4, 12]. For the purposes of our work, we use the incidence of of-\nfensive rhetoric as a stand in for incivility. The 2016 US\npresidential election was an especially noteworthy case\nin this regard, particularly in terms of Donald Trump\u2019s\ncampaign which frequently violated norms of civility\nboth in how he spoke about broad groups in the pub-\nlic (such as Muslims, Mexicans, and African Americans)\nand the attacks he leveled at his opponents [2]. The con-\nsequences of incivility are thought to be crucial to the\nfunctioning of democracy since \u201cpublic civility and in-\nterpersonal politeness sustain social harmony and allow\npeople who disagree with one another to maintain ongo-\ning relationships\u201d [17]. While political incivility appears to be on the rise\namong elites, it is less clear whether this is true among\nthe mass public as well. Is political discourse particularly\nlacking in civility compared to discourse more generally? Does the incivility of mass political discourse respond to\nthe dynamics of political campaigns? Addressing these\nquestions has been dif\ufb01cult for political scientists be-\ncause traditional tools for studying mass behavior, such\nas public opinion surveys, are ill-equipped to measure\nhow citizens discuss politics with one another. Survey\ndata does reveal that the public tends to perceive politics\nas becoming increasingly less civil during the course of a\npolitical campaign [18]. Yet, it is not clear whether these\nperceptions match the reality, particularly in terms of the\ntypes of discussions that citizens have with each other. An additional question is how incivility is received\nby others. On one hand, violations of norms regard-\ning offensive discourse may be policed by members of\na community, rendering such speech ineffectual. On the\nother hand, offensive speech may be effective as a means\nfor drawing attention to a particular argument. Indeed,\nthere is evidence that increasing incivility in political\nspeech results in higher levels of attention from the pub-\nlic [12]. During the 2016 campaign, the use of swearing\nin comments posted on Donald Trump\u2019s YouTube chan-\nnel tended to result in additional responses that mimicked\nsuch swearing [8]. Thus, offensive speech in online fora\nmay attract more attention from the community and lead\nto the spread of even more offensive speech in subse-\nquent posts. To address these questions regarding political incivil-\nity, we examine the use of offensive speech in politi-\ncal discussions housed on Reddit. Scholars tend to de-\n\ufb01ne uncivil discourse as \u201ccommunication that violates\nthe norms of politeness\u201d [12] a de\ufb01nition that clearly in-\narXiv:1706.01875v2  [cs.CL]  19 Jul 2017\n\ncludes offensive remarks. Reddit fora represent a \u201cmost\nlikely\u201d case for the study of offensive political speech\ndue its strong free speech culture [14] and the ability of\nparticipants to use pseudonymous identities. That is, if\npolitical incivility in the public did increase during the\n2016 campaign, this should be especially evident on fora\nsuch as Reddit. Tracking Reddit discussions through-\nout all of 2015 and 2016, we \ufb01nd that online political\ndiscussions became increasingly more offensive as the\ngeneral election campaign intensi\ufb01ed. By comparison,\ndiscussions on non-political subreddits did not become\nincreasingly offensive during this period. Additionally,\nwe \ufb01nd that the presence of offensive comments did not\nsubside even three months after the election. 2 Datasets\nOur study makes use of multiple datasets in order to iden-\ntify and characterize trends in offensive speech. The CrowdFlower hate speech dataset. The Crowd-\nFlower hate speech dataset [1] contains 14.5K tweets,\neach receiving labels from at least three contributors. Contributors were allowed to classify each tweet into\none of three classes: Not Offensive (NO), Offensive but\nnot hateful (O), and Offensive and hateful (OH). Of the\n14.5K tweets, only 37.6% had a decisive class \u2013 i.e., the\nsame class was assigned by all contributors. For inde-\ncisive cases, the majority class was selected and a class\ncon\ufb01dence score (fraction of contributors that selected\nthe majority class) was made available. Using this ap-\nproach, 50.4%, 33.1%, and 16.5% of the tweets were cat-\negorized as NO, O, and OH, respectively. Since our goal\nis to identify any offensive speech (not just hate speech),\nwe consolidate assigned classes into Offensive and Not\nOffensive by relabeling OH tweets as Offensive. We use\nthis modi\ufb01ed dataset to train, validate, and test our offen-\nsive speech classi\ufb01er. To the best of our knowledge, this\nis the only dataset that provides offensive and not offen-\nsive annotations to a large dataset. Offensive word lists. We also use two offensive\nword lists as auxiliary input to our classi\ufb01er: (1) The\nHatebase hate speech vocabulary [3] consisting of 1122\nhateful words and (2) 422 offensive words banned from\nGoogle\u2019s What Do You Love project [7]. Reddit comments dataset. Finally, after building our\noffensive speech classi\ufb01er using the above datasets, we\nuse it to classify comments made on Reddit. While the\ncomplete Reddit dataset contains 2B comments made be-\ntween the period of January 2015 and January 2017, we\nonly analyze only 168M. We select comments to be ana-\nlyzed using the following process: (1) we exclude com-\nments shorter than 10 characters in length, (2) we ex-\nclude comments made by [deleted] authors, and (3)\nFigure 1: Number of analyzed political and apoliti-\ncal comments belonging to each week between January\n2015 and January 2017. we randomly sample and include 10% of all remaining\ncomments. We categorize comments made in any of 21\npopular political subreddits as political and the remain-\nder as apolitical. Our \ufb01nal dataset contains 129M apo-\nlitical and 39M political comments. Figure 1 shows the\nnumber of comments in our dataset that were made dur-\ning each week included in our study. We see an increas-\ning number of political comments per week starting in\nFebruary 2016 \u2013 the start of the 2016 US presidential\nprimaries. 3 Offensive Speech Classi\ufb01cation\nIn order to identify offensive speech, we propose a fully\nautomated technique that classi\ufb01es comments into two\nclasses: Offensive and Not Offensive. 3.1 Classi\ufb01cation approach\nAt a high-level, our approach works as follows:\n\u2022Build a word embedding. We construct a 100-\ndimensional word embedding using all comments\nfrom our complete Reddit dataset (2B comments). \u2022Construct a hate vector. We construct a list of of-\nfensive and hateful words identi\ufb01ed from external\ndata and map them into a single vector within the\nhigh-dimensional word embedding. \u2022Text transformation and classi\ufb01cation. Finally,\nwe transform text to be classi\ufb01ed into scalars rep-\nresenting their distance from the constructed hate\nvector and use these as input to a Random Forest\nclassi\ufb01er. Building a word embedding. At a high-level, a word\nembedding maps words into a high-dimensional continu-\nous vector space in such a way that semantic similarities\nbetween words are maintained. This mapping is achieved\n\nFigure 2: Proximity of offensive and non-offensive com-\nments to the hate vector. Dimension reduction is per-\nformed using t-SNE. by exploiting the distributional properties of words and\ntheir occurrences in the input corpus. Rather than using an off-the-shelf word embedding\n(e.g., the GloVe embeddings [13] trained using public do-\nmain data sources such as Wikipedia and news articles),\nwe construct a 100-dimensional embedding using the\ncomplete Reddit dataset (2B comments) as the input cor-\npus. The constructed embedding consists of over 400M\nunique words (words occurring less than 25 times in the\nentire corpus are excluded) using the Word2Vec [10] im-\nplementation provided by the Gensim library [15]. Prior\nto constructing the embedding, we perform stop-word\nremoval and lemmatize each word in the input corpus\nusing the SpaCy NLP framework [5]. The main reason\nfor building a custom embedding is to ensure that our\nembeddings capture semantics speci\ufb01c to the data be-\ning measured (Reddit) \u2013 e.g., while the word \u201ckarma\u201d in\nthe non-Reddit context may be associated with spiritual-\nity, it is associated with points (comment and submission\nscores) on Reddit. Constructing a hate vector. We use two lists of words\nassociated with hate [3] and offensive [7] speech to con-\nstruct a hate vector in our word embedding. This is\ndone by mapping each word in the list into the 100-\ndimensional embedding and computing the mean vector. This vector represents the average of all known offen-\nsive words. The main idea behind creating a hate vector\nis to capture the point (in our embedding) to which the\nmost offensive observed comments are likely to be near. Although clustering our offensive word lists into similar\ngroups and constructing multiple hate vectors \u2013 one for\neach cluster \u2013 results in marginally better accuracy for\nour classi\ufb01er, we use this approach due to the fact that\nour classi\ufb01cation cost grows linearly with the number of\nhate vectors \u2013 i.e., we need to perform O(|S|) distance\ncomputations per hate vector to classify string S. Transforming and classifying text. We \ufb01rst remove\nstop-words and perform lemmatization of each word in\nthe text to be classi\ufb01ed. We then obtain the vector repre-\nsenting each word in the text and compute its similarity\nto the constructed hate vector using the cosine similar-\nity metric. A 0-vector is used to represent words in the\ntext that are not present in the embedding. Finally, the\nmaximum cosine similarity score is used to represent the\ncomment. Equation 1 shows the transformation function\non a string S = {s1,..., sn}where si is the vector rep-\nresenting the ith lemmatized non-stop-word, cos is the\ncosine-similarity function, and H is the hate vector. T(S) =max\n1\u2264i\u2264n\n[cos(si,H)] (1)\nIn words, the numerical value assigned to a text is the\ncosine similarity between the hate vector and the vec-\ntor representing the word (in the text) closest to the hate\nvector. This approach allows us to transform a string of\ntext into a single numerical value that captures its se-\nmantic similarity to the most offensive comment. We\nuse these scalars as input to a random forest classi\ufb01er to\nperform classi\ufb01cation into Offensive and Not Offensive\nclasses. Figure 2 shows the proximity of Offensive and\nNon Offensive comments to our constructed hate vector\nafter using t-distributed Stochastic Neighbor Embedding\n(t-SNE) [9] to reduce our 100-dimension vector space\ninto 2 dimensions. 3.2 Classi\ufb01er evaluation\nWe now present results to (1) validate our choice\nof classi\ufb01er and (2) demonstrate the impact of train-\ning/validation sample count on our classi\ufb01ers perfor-\nmance. Classi\ufb01er Accuracy (%) F1-Score (%)\nStochastic Gradient Descent 80.7 80.0\nNaive Bayes 81.5 81.2\nDecision Tree 91.8 91.4\nRandom Forest 92.0 91.9\nTable 1: Average classi\ufb01er performance during 10-fold\ncross-validation on the training/validation set. Results\nshown are for the best performing parameters obtained\nusing a grid search. Classi\ufb01er selection methodology. To identify the\nmost suitable classi\ufb01er for classifying the scalars as-\nsociated with each text, we perform evaluations us-\ning the stochastic gradient descent, naive bayes, deci-\nsion tree, and random forest classi\ufb01ers. For each clas-\nsi\ufb01er, we split the CrowdFlower hate speech dataset\ninto a training/validation set (75%), and a holdout set\n(25%). We perform 10-fold cross-validation on the train-\ning/validation set to identify the best classi\ufb01er model and\n\n(a) Classi\ufb01er accuracy. (b) Classi\ufb01er precision. (c) Classi\ufb01er recall. Figure 3: Classi\ufb01er performance on holdout sets while\nvarying holdout set sizes and minimum con\ufb01dence\nthresholds. parameters (using a grid search). Based on the results of\nthis evaluation, we select a 100-estimator entropy-based\nsplitting random forest model as our classi\ufb01er. Table 1\nshows the mean accuracy and F1-score for each evalu-\nated classi\ufb01er during the 10-fold cross-validation. Real-world classi\ufb01er performance. To evaluate real-\nworld performance of our selected classi\ufb01er ( i.e., per-\nformance in the absence of model and parameter bias),\nwe perform classi\ufb01cation of the holdout set. On this set,\nour classi\ufb01er had an accuracy and F1-score of 89.6% and\n89.2%, respectively. These results show that in addition\nto superior accuracy during training and validation, our\nchosen classi\ufb01er is also robust against over-\ufb01tting. Impact of dataset quality and size. To understand\nhow the performance of our chosen classi\ufb01er model and\nparameters are impacted by: (1) the quality and consis-\ntency of manually assigned classes in the CrowdFlower\ndataset and (2) the size of the dataset, we re-evaluate the\nclassi\ufb01er while only considering tweets having a mini-\nmum con\ufb01dence score and varying the size of the holdout\nset. Speci\ufb01cally, our experiments considered con\ufb01dence\nthresholds of 0 (all tweets considered), .35 (only tweets\nwhere at least 35% of contributors agreed on a class were\nconsidered), and .70 (only tweets where at least 70% of\nthe contributors agreed on a class were considered) and\nvaried the holdout set sizes between 5% and 95% of all\ntweets meeting the con\ufb01dence threshold set for the ex-\nperiment. The results illustrated in Figure 3 show the perfor-\nmance of the classi\ufb01er while evaluating the correspond-\ning holdout set. We make several conclusions from these\nresults:\n\u2022Beyond a (fairly low) threshold, the size of the train-\ning and validation set has little impact on classi-\n\ufb01er performance. We see that the accuracy, pre-\ncision, and recall have, at best, marginal improve-\nments with holdout set sizes smaller than 60%. This\nimplies that the CrowdFlower dataset is suf\ufb01cient\nfor building an offensive speech classi\ufb01er. \u2022Quality of manual labeling has a signi\ufb01cant impact\non the accuracy and precision of the classi\ufb01er. Us-\ning only tweets which had at least 70% of contrib-\nutors agreeing on a class resulted in between 5-7%\nhigher accuracy and up to 5% higher precision. \u2022Our classi\ufb01er achieves precision of over 95% and\nrecall of over 85% when considering only high con-\n\ufb01dence samples. This implies that the classi\ufb01er is\nmore likely to underestimate the presence of offen-\nsive speech \u2013 i.e., our results likely provide a lower-\nbound on the quantity of observed offensive speech. 4 Measurements\nIn this section we quantify and characterize offensive-\nness in the political and general contexts using our offen-\nsive speech classi\ufb01er and the Reddit comments dataset\nwhich considers a random sample of comments made be-\ntween January 2015 and January 2017. Offensiveness over time. We \ufb01nd that on average\n8.4% of all political comments are offensive compared to\n7.8% of all apolitical comments. Figure 4 illustrates the\nfraction of offensive political and apolitical comments\nmade during each week in our study. We see that while\nthe fraction of apolitical offensive comments has stayed\nsteady, there has been an increase in the fraction of of-\nfensive political comments starting in July 2016. No-\ntably, this increase is observed after the conclusion of the\nUS presidential primaries and during the period of the\nDemocratic and Republican National Conventions and\ndoes not reduce even after the conclusion of the US presi-\ndential elections held on November 8. Participants in po-\nlitical subreddits were 2.6% more likely to observe offen-\n\nsive comments prior to July 2016 but 14.9% more likely\nto observe offensive comments from July 2016 onwards. Figure 4: Fraction of offensive comments identi\ufb01ed in\npolitical and all subreddits. Reactions to offensive comments. We use the com-\nment score, roughly the difference between up-votes and\ndown-votes received, as a proxy for understanding how\nusers reacted to offensive comments. We \ufb01nd that com-\nments that were offensive: (1) on average, had a higher\nscore than non-offensive comments (average scores: 8.9\nvs. 6.7) and (2) were better received when they were\nposted in the general context than in the political context\n(average scores: 8.6 vs. 9.0). To understand how peo-\nples reactions to offensive comments evolved over time,\nFigure 5 shows the average scores received by offensive\ncomments over time. Again, we observe an increasing\ntrend in average scores received by offensive and politi-\ncal comments after July 2016. Figure 5: Average scores of offensive comments identi-\n\ufb01ed in political and all subreddits. Characteristics of offensive authors. We now focus\non understanding characteristics of authors of offensive\ncomments. Speci\ufb01cally, we are interested in identify-\ning the use of throwaway and troll accounts. For the\npurposes of this study, we characterize throwaway ac-\ncounts as those with less than \ufb01ve total comments \u2013 i.e.,\naccounts that are used to make a small number of com-\nments. Similarly, we de\ufb01ne troll accounts as those with\nover 15 comments of which over 75% are classi\ufb01ed as\noffensive \u2013 i.e., accounts that are used to make a larger\nnumber of comments, of which a signi\ufb01cant majority are\noffensive. We \ufb01nd that 93.7% of the accounts which\nhave over 75% of their comments tagged as offensive are\nthrowaways and 1.3% are trolls. Complete results are\nillustrated in Figure 6. Figure 6: CDF of the fraction of each authors comments\nthat were identi\ufb01ed as offensive. Green, orange, and red\ndots are used to represent authors with <5, 5-15, and\n>15 total comments, respectively. The legend provides\na breakdown per quartile. Characteristics of offensive communities. We break-\ndown subreddits by their category (default, political, and\nother) and identify the most and least offensive commu-\nnities in each. Figure 7 shows the distribution of the frac-\ntion of offensive comments in each category and Table 2\nshows the most and least offensive subreddits in the po-\nlitical and default categories (we exclude the \u201cother\u201d cat-\negory due to the inappropriateness of their names). We\n\ufb01nd that less than 19% of all subreddits (that account\nfor over 23% of all comments) have over 10% offensive\ncomments. Further, several default and political subred-\ndits fall in this category, including r/the donald \u2013 the\nmost offensive political subreddit and the subreddit ded-\nicated to the US President. Figure 7: Distribution of the fraction of offensive com-\nments observed in each subreddit category. Only subred-\ndits with over 1000 comments are considered. Flow of offensive authors. Finally, we uncover\npatterns in the movement of offensive authors between\ncommunities. In Figure 8 we show the communities\n\nCategory Most offensive (%) Least offensive (%)\nDefault r/tifu(15.1%) r/askscience(2.4%)\nr/announcements(13.2%)r/personalfinance(3.4%)\nr/askreddit(11.0%) r/science(3.8%)\nPolitical r/thedonald(11.4%) r/republican(4.4%)\nr/elections(10.2%) r/sandersforpresident(4.9%)\nr/worldpolitics(9.8%) r/tedcruz(5.1%)\nTable 2: Subreddits in the default and political categories\nwith the highest and lowest fraction of offensive com-\nments. in which large number of authors of offensive content\non the r/politics subreddit had previously made\noffensive comments (we refer to these communities\nas sources). Unsurprisingly, the most popular sources\nbelonged to the default subreddits ( e.g., r/worldnews,\nr/wtf, r/videos, r/askreddit, and r/news). We\n\ufb01nd that several other political subreddits also serve as\nlarge sources of offensive authors. In fact, the subreddits\ndedicated to the three most popular US presidential can-\ndidates \u2013 r/the donald, r/sandersforpresident,\nand r/hillaryclinton rank in the top three. Fi-\nnally, outside of the default and political subreddits,\nwe \ufb01nd that r/nfl, r/conspiracy, r/dota2,\nr/reactiongifs, r/blackpeopletwitter, and\nr/imgoingtohellforthis were the largest sources of\noffensive political authors. Figure 8: Flow of offensive authors. An edge between\ntwo subreddits indicates that authors made offensive\ncomments in the source subreddit before the \ufb01rst time\nthey made offensive comments in the destination subred-\ndit. Darker and thicker edges indicate larger \ufb02ow sizes\n(only \ufb02ows \u2265200 authors are shown). 5 Conclusions and Future Work\nWe develop and validate an offensive speech classi\ufb01er\nto quantify the presence of offensive online comments\nfrom January 2015 through January 2017. We \ufb01nd that\npolitical discussions on Reddit became increasingly less\ncivil \u2013 as measured by the incidence of offensive com-\nments \u2013 during the 2016 general election campaign. In\nfact, during the height of the campaign, nearly one of ev-\nery 10 comments posted on a political subreddit were\nclassi\ufb01ed as offensive. Offensive comments also re-\nceived more positive feedback from the community, even\nthough most of the accounts responsible for such com-\nments appear to be throwaway accounts. While offensive\nposts were increasingly common on political subreddits\nas the campaign wore on, there was no such increase in\nnon-political fora. This contrast provides additional ev-\nidence that the increasing use of offensive speech was\ndirectly related to the ramping up of the general election\ncampaign for president. Even though our study relies on just a single source\nof online political discussions \u2013 Reddit, we believe that\nour \ufb01ndings generally present an upper-bound on the in-\ncidence of offensiveness in online political discussions\nfor the following reasons: First, Reddit allows the use\nof pseudonymous identities that enables the online dis-\ninhibition effect (unlike social-media platforms such as\nFacebook). Second, Reddit enables users to engage in\ncomplex discussions that are unrestricted in length (un-\nlike Twitter). Finally, Reddit is known for enabling a\ngeneral culture of free speech and delegating content reg-\nulation to moderators of individual subreddits. This pro-\nvides users holding fringe views a variety of subreddits\nin which their content is welcome. Our \ufb01ndings provide a unique and important map-\nping of the increasing incivility of online political dis-\ncourse during the 2016 campaign. Such an investiga-\ntion is important because scholars have outlined many\nconsequences for incivility in political discourse. Inci-\nvility tends to \u201cturn off\u201d political moderates, leading to\nincreasing polarization among those who are actively en-\ngaged in politics [18]. More importantly, a lack of ci-\nvility in political discussions generally reduces the de-\ngree to which people view opponents as holding legit-\nimate viewpoints. This dynamic makes it dif\ufb01cult for\npeople to \ufb01nd common ground with those who disagree\nwith them [11] and it may ultimately lead citizens to\nview electoral victories by opponents as lacking legiti-\nmacy [12]. Thus, from a normative standpoint, the fact\nthat the 2016 campaign sparked a marked increase in the\noffensiveness of political comments posted to Reddit is\nof concern in its own right; that the incidence of offensive\npolitical comments has remained high even three months\nafter the election is all the more troubling. In future work, we will extend our analysis of Reddit\nback to 2007 with the aim of formulating a more com-\nplete understanding of the dynamics of political incivil-\nity. For example, we seek to understand whether the\nhigh incidence of offensive speech we \ufb01nd in 2016 is\nunique to this particular campaign or if previous presi-\ndential campaigns witnessed similar spikes in incivility. We will also examine whether there is a more general\nlong-term trend toward offensive online political speech,\nwhich would be consistent with what scholars have found\nwhen studying political elites [6, 16]. References\n[1] CrowdFlower Blog. Hate speech identi\ufb01cation. URL: https://www.crowdflower.com/data/h\nate-speech-identification/ (Accessed May\n24, 2017). [2] Justin H Gross and Kaylee T Johnson. Twitter\ntaunts and tirades: Negative campaigning in the age\nof trump. PS: Political Science &amp; Politics,\n49(4):748\u2013754, 2016. [3] Hatebase. Meet the Hatebase API. URL: https:\n//www.hatebase.org/connect_api (Accessed\nMay 24, 2017). [4] Susan Herbst. Rude democracy: Civility and in-\ncivility in American politics. Temple University\nPress, 2010.\n\nAssociation for Computational Linguistics. [6] Kathleen Hall Jamieson and Erika Falk. Continu-\nity and change in civility in the house. In Polar-\nized politics: Congress and the president in a parti-\nsan era, pages 96\u2013108. Washington, DC: CQ Press,\n2000.\n\n[16] Daniel M Shea and Alex Sproveri. The rise and fall\nof nasty politics in america. PS: Political Science\n&amp; Politics, 45(03):416\u2013421, 2012. [17] J Cherie Strachan and Michael R Wolf. Politi-\ncal civility. PS: Political Science &amp; Politics,\n45(03):401\u2013404, 2012. [18] Michael R Wolf, J Cherie Strachan, and Daniel M\nShea. Incivility and standing \ufb01rm: A second layer\nof partisan division. PS: Political Science &amp;\nPolitics, 45(03):428\u2013434, 2012.",
            0.5264403749999929
          ]
        },
        {
          "answer": "The system's performance, as described in the context, showed that their offensive speech classifier achieved an accuracy of 92.0% and an F1-score of 91.9% during 10-fold cross-validation on the training/validation set. Additionally, when evaluating the classifier on a holdout set, it achieved an accuracy and F1-score of 89.6% and 89.2%, respectively. These results indicate that the classifier was not only accurate but also robust against over-fitting.",
          "relevant_chunks": [
            "[5] Matthew Honnibal and Mark Johnson. An Im-\nproved Non-monotonic Transition System for De-\npendency Parsing. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1373\u20131378, Lisbon, Portugal,\n2015.\n\n[13] Jeffrey Pennington, Richard Socher, and Christo-\npher D.\n\nMeasuring Offensive Speech in Online Political Discourse\nRishab Nithyanand1, Brian Schaffner2, Phillipa Gill1\n1{rishab, phillipa}@cs.umass.edu, 2schaffne@polsci.umass.edu\nUniversity of Massachusetts, Amherst\nAbstract\nThe Internet and online forums such as Reddit have be-\ncome an increasingly popular medium for citizens to\nengage in political conversations. However, the on-\nline disinhibition effect resulting from the ability to use\npseudonymous identities may manifest in the form of\noffensive speech, consequently making political discus-\nsions more aggressive and polarizing than they already\nare. Such environments may result in harassment and\nself-censorship from its targets. In this paper, we present\npreliminary results from a large-scale temporal measure-\nment aimed at quantifying offensiveness in online politi-\ncal discussions. To enable our measurements, we develop and evalu-\nate an offensive speech classi\ufb01er. We then use this clas-\nsi\ufb01er to quantify and compare offensiveness in the po-\nlitical and general contexts. We perform our study us-\ning a database of over 168M Reddit comments made by\nover 7M pseudonyms between January 2015 and January\n2017 \u2013 a period covering several divisive political events\nincluding the 2016 US presidential elections. 1 Introduction\nThe apparent rise in political incivility has attracted sub-\nstantial attention from scholars in recent years. These\nstudies have largely focused on the extent to which politi-\ncians and elected of\ufb01cials are increasingly employing\nrhetoric that appears to violate norms of civility [4, 12]. For the purposes of our work, we use the incidence of of-\nfensive rhetoric as a stand in for incivility. The 2016 US\npresidential election was an especially noteworthy case\nin this regard, particularly in terms of Donald Trump\u2019s\ncampaign which frequently violated norms of civility\nboth in how he spoke about broad groups in the pub-\nlic (such as Muslims, Mexicans, and African Americans)\nand the attacks he leveled at his opponents [2]. The con-\nsequences of incivility are thought to be crucial to the\nfunctioning of democracy since \u201cpublic civility and in-\nterpersonal politeness sustain social harmony and allow\npeople who disagree with one another to maintain ongo-\ning relationships\u201d [17]. While political incivility appears to be on the rise\namong elites, it is less clear whether this is true among\nthe mass public as well. Is political discourse particularly\nlacking in civility compared to discourse more generally? Does the incivility of mass political discourse respond to\nthe dynamics of political campaigns? Addressing these\nquestions has been dif\ufb01cult for political scientists be-\ncause traditional tools for studying mass behavior, such\nas public opinion surveys, are ill-equipped to measure\nhow citizens discuss politics with one another. Survey\ndata does reveal that the public tends to perceive politics\nas becoming increasingly less civil during the course of a\npolitical campaign [18]. Yet, it is not clear whether these\nperceptions match the reality, particularly in terms of the\ntypes of discussions that citizens have with each other. An additional question is how incivility is received\nby others. On one hand, violations of norms regard-\ning offensive discourse may be policed by members of\na community, rendering such speech ineffectual. On the\nother hand, offensive speech may be effective as a means\nfor drawing attention to a particular argument. Indeed,\nthere is evidence that increasing incivility in political\nspeech results in higher levels of attention from the pub-\nlic [12]. During the 2016 campaign, the use of swearing\nin comments posted on Donald Trump\u2019s YouTube chan-\nnel tended to result in additional responses that mimicked\nsuch swearing [8]. Thus, offensive speech in online fora\nmay attract more attention from the community and lead\nto the spread of even more offensive speech in subse-\nquent posts. To address these questions regarding political incivil-\nity, we examine the use of offensive speech in politi-\ncal discussions housed on Reddit. Scholars tend to de-\n\ufb01ne uncivil discourse as \u201ccommunication that violates\nthe norms of politeness\u201d [12] a de\ufb01nition that clearly in-\narXiv:1706.01875v2  [cs.CL]  19 Jul 2017\n\ncludes offensive remarks. Reddit fora represent a \u201cmost\nlikely\u201d case for the study of offensive political speech\ndue its strong free speech culture [14] and the ability of\nparticipants to use pseudonymous identities. That is, if\npolitical incivility in the public did increase during the\n2016 campaign, this should be especially evident on fora\nsuch as Reddit. Tracking Reddit discussions through-\nout all of 2015 and 2016, we \ufb01nd that online political\ndiscussions became increasingly more offensive as the\ngeneral election campaign intensi\ufb01ed. By comparison,\ndiscussions on non-political subreddits did not become\nincreasingly offensive during this period. Additionally,\nwe \ufb01nd that the presence of offensive comments did not\nsubside even three months after the election. 2 Datasets\nOur study makes use of multiple datasets in order to iden-\ntify and characterize trends in offensive speech. The CrowdFlower hate speech dataset. The Crowd-\nFlower hate speech dataset [1] contains 14.5K tweets,\neach receiving labels from at least three contributors. Contributors were allowed to classify each tweet into\none of three classes: Not Offensive (NO), Offensive but\nnot hateful (O), and Offensive and hateful (OH). Of the\n14.5K tweets, only 37.6% had a decisive class \u2013 i.e., the\nsame class was assigned by all contributors. For inde-\ncisive cases, the majority class was selected and a class\ncon\ufb01dence score (fraction of contributors that selected\nthe majority class) was made available. Using this ap-\nproach, 50.4%, 33.1%, and 16.5% of the tweets were cat-\negorized as NO, O, and OH, respectively. Since our goal\nis to identify any offensive speech (not just hate speech),\nwe consolidate assigned classes into Offensive and Not\nOffensive by relabeling OH tweets as Offensive. We use\nthis modi\ufb01ed dataset to train, validate, and test our offen-\nsive speech classi\ufb01er. To the best of our knowledge, this\nis the only dataset that provides offensive and not offen-\nsive annotations to a large dataset. Offensive word lists. We also use two offensive\nword lists as auxiliary input to our classi\ufb01er: (1) The\nHatebase hate speech vocabulary [3] consisting of 1122\nhateful words and (2) 422 offensive words banned from\nGoogle\u2019s What Do You Love project [7]. Reddit comments dataset. Finally, after building our\noffensive speech classi\ufb01er using the above datasets, we\nuse it to classify comments made on Reddit. While the\ncomplete Reddit dataset contains 2B comments made be-\ntween the period of January 2015 and January 2017, we\nonly analyze only 168M. We select comments to be ana-\nlyzed using the following process: (1) we exclude com-\nments shorter than 10 characters in length, (2) we ex-\nclude comments made by [deleted] authors, and (3)\nFigure 1: Number of analyzed political and apoliti-\ncal comments belonging to each week between January\n2015 and January 2017. we randomly sample and include 10% of all remaining\ncomments. We categorize comments made in any of 21\npopular political subreddits as political and the remain-\nder as apolitical. Our \ufb01nal dataset contains 129M apo-\nlitical and 39M political comments. Figure 1 shows the\nnumber of comments in our dataset that were made dur-\ning each week included in our study. We see an increas-\ning number of political comments per week starting in\nFebruary 2016 \u2013 the start of the 2016 US presidential\nprimaries. 3 Offensive Speech Classi\ufb01cation\nIn order to identify offensive speech, we propose a fully\nautomated technique that classi\ufb01es comments into two\nclasses: Offensive and Not Offensive. 3.1 Classi\ufb01cation approach\nAt a high-level, our approach works as follows:\n\u2022Build a word embedding. We construct a 100-\ndimensional word embedding using all comments\nfrom our complete Reddit dataset (2B comments). \u2022Construct a hate vector. We construct a list of of-\nfensive and hateful words identi\ufb01ed from external\ndata and map them into a single vector within the\nhigh-dimensional word embedding. \u2022Text transformation and classi\ufb01cation. Finally,\nwe transform text to be classi\ufb01ed into scalars rep-\nresenting their distance from the constructed hate\nvector and use these as input to a Random Forest\nclassi\ufb01er. Building a word embedding. At a high-level, a word\nembedding maps words into a high-dimensional continu-\nous vector space in such a way that semantic similarities\nbetween words are maintained. This mapping is achieved\n\nFigure 2: Proximity of offensive and non-offensive com-\nments to the hate vector. Dimension reduction is per-\nformed using t-SNE. by exploiting the distributional properties of words and\ntheir occurrences in the input corpus. Rather than using an off-the-shelf word embedding\n(e.g., the GloVe embeddings [13] trained using public do-\nmain data sources such as Wikipedia and news articles),\nwe construct a 100-dimensional embedding using the\ncomplete Reddit dataset (2B comments) as the input cor-\npus. The constructed embedding consists of over 400M\nunique words (words occurring less than 25 times in the\nentire corpus are excluded) using the Word2Vec [10] im-\nplementation provided by the Gensim library [15]. Prior\nto constructing the embedding, we perform stop-word\nremoval and lemmatize each word in the input corpus\nusing the SpaCy NLP framework [5]. The main reason\nfor building a custom embedding is to ensure that our\nembeddings capture semantics speci\ufb01c to the data be-\ning measured (Reddit) \u2013 e.g., while the word \u201ckarma\u201d in\nthe non-Reddit context may be associated with spiritual-\nity, it is associated with points (comment and submission\nscores) on Reddit. Constructing a hate vector. We use two lists of words\nassociated with hate [3] and offensive [7] speech to con-\nstruct a hate vector in our word embedding. This is\ndone by mapping each word in the list into the 100-\ndimensional embedding and computing the mean vector. This vector represents the average of all known offen-\nsive words. The main idea behind creating a hate vector\nis to capture the point (in our embedding) to which the\nmost offensive observed comments are likely to be near. Although clustering our offensive word lists into similar\ngroups and constructing multiple hate vectors \u2013 one for\neach cluster \u2013 results in marginally better accuracy for\nour classi\ufb01er, we use this approach due to the fact that\nour classi\ufb01cation cost grows linearly with the number of\nhate vectors \u2013 i.e., we need to perform O(|S|) distance\ncomputations per hate vector to classify string S. Transforming and classifying text. We \ufb01rst remove\nstop-words and perform lemmatization of each word in\nthe text to be classi\ufb01ed. We then obtain the vector repre-\nsenting each word in the text and compute its similarity\nto the constructed hate vector using the cosine similar-\nity metric. A 0-vector is used to represent words in the\ntext that are not present in the embedding. Finally, the\nmaximum cosine similarity score is used to represent the\ncomment. Equation 1 shows the transformation function\non a string S = {s1,..., sn}where si is the vector rep-\nresenting the ith lemmatized non-stop-word, cos is the\ncosine-similarity function, and H is the hate vector. T(S) =max\n1\u2264i\u2264n\n[cos(si,H)] (1)\nIn words, the numerical value assigned to a text is the\ncosine similarity between the hate vector and the vec-\ntor representing the word (in the text) closest to the hate\nvector. This approach allows us to transform a string of\ntext into a single numerical value that captures its se-\nmantic similarity to the most offensive comment. We\nuse these scalars as input to a random forest classi\ufb01er to\nperform classi\ufb01cation into Offensive and Not Offensive\nclasses. Figure 2 shows the proximity of Offensive and\nNon Offensive comments to our constructed hate vector\nafter using t-distributed Stochastic Neighbor Embedding\n(t-SNE) [9] to reduce our 100-dimension vector space\ninto 2 dimensions. 3.2 Classi\ufb01er evaluation\nWe now present results to (1) validate our choice\nof classi\ufb01er and (2) demonstrate the impact of train-\ning/validation sample count on our classi\ufb01ers perfor-\nmance. Classi\ufb01er Accuracy (%) F1-Score (%)\nStochastic Gradient Descent 80.7 80.0\nNaive Bayes 81.5 81.2\nDecision Tree 91.8 91.4\nRandom Forest 92.0 91.9\nTable 1: Average classi\ufb01er performance during 10-fold\ncross-validation on the training/validation set. Results\nshown are for the best performing parameters obtained\nusing a grid search. Classi\ufb01er selection methodology. To identify the\nmost suitable classi\ufb01er for classifying the scalars as-\nsociated with each text, we perform evaluations us-\ning the stochastic gradient descent, naive bayes, deci-\nsion tree, and random forest classi\ufb01ers. For each clas-\nsi\ufb01er, we split the CrowdFlower hate speech dataset\ninto a training/validation set (75%), and a holdout set\n(25%). We perform 10-fold cross-validation on the train-\ning/validation set to identify the best classi\ufb01er model and\n\n(a) Classi\ufb01er accuracy. (b) Classi\ufb01er precision. (c) Classi\ufb01er recall. Figure 3: Classi\ufb01er performance on holdout sets while\nvarying holdout set sizes and minimum con\ufb01dence\nthresholds. parameters (using a grid search). Based on the results of\nthis evaluation, we select a 100-estimator entropy-based\nsplitting random forest model as our classi\ufb01er. Table 1\nshows the mean accuracy and F1-score for each evalu-\nated classi\ufb01er during the 10-fold cross-validation. Real-world classi\ufb01er performance. To evaluate real-\nworld performance of our selected classi\ufb01er ( i.e., per-\nformance in the absence of model and parameter bias),\nwe perform classi\ufb01cation of the holdout set. On this set,\nour classi\ufb01er had an accuracy and F1-score of 89.6% and\n89.2%, respectively. These results show that in addition\nto superior accuracy during training and validation, our\nchosen classi\ufb01er is also robust against over-\ufb01tting. Impact of dataset quality and size. To understand\nhow the performance of our chosen classi\ufb01er model and\nparameters are impacted by: (1) the quality and consis-\ntency of manually assigned classes in the CrowdFlower\ndataset and (2) the size of the dataset, we re-evaluate the\nclassi\ufb01er while only considering tweets having a mini-\nmum con\ufb01dence score and varying the size of the holdout\nset. Speci\ufb01cally, our experiments considered con\ufb01dence\nthresholds of 0 (all tweets considered), .35 (only tweets\nwhere at least 35% of contributors agreed on a class were\nconsidered), and .70 (only tweets where at least 70% of\nthe contributors agreed on a class were considered) and\nvaried the holdout set sizes between 5% and 95% of all\ntweets meeting the con\ufb01dence threshold set for the ex-\nperiment. The results illustrated in Figure 3 show the perfor-\nmance of the classi\ufb01er while evaluating the correspond-\ning holdout set. We make several conclusions from these\nresults:\n\u2022Beyond a (fairly low) threshold, the size of the train-\ning and validation set has little impact on classi-\n\ufb01er performance. We see that the accuracy, pre-\ncision, and recall have, at best, marginal improve-\nments with holdout set sizes smaller than 60%. This\nimplies that the CrowdFlower dataset is suf\ufb01cient\nfor building an offensive speech classi\ufb01er. \u2022Quality of manual labeling has a signi\ufb01cant impact\non the accuracy and precision of the classi\ufb01er. Us-\ning only tweets which had at least 70% of contrib-\nutors agreeing on a class resulted in between 5-7%\nhigher accuracy and up to 5% higher precision. \u2022Our classi\ufb01er achieves precision of over 95% and\nrecall of over 85% when considering only high con-\n\ufb01dence samples. This implies that the classi\ufb01er is\nmore likely to underestimate the presence of offen-\nsive speech \u2013 i.e., our results likely provide a lower-\nbound on the quantity of observed offensive speech. 4 Measurements\nIn this section we quantify and characterize offensive-\nness in the political and general contexts using our offen-\nsive speech classi\ufb01er and the Reddit comments dataset\nwhich considers a random sample of comments made be-\ntween January 2015 and January 2017. Offensiveness over time. We \ufb01nd that on average\n8.4% of all political comments are offensive compared to\n7.8% of all apolitical comments. Figure 4 illustrates the\nfraction of offensive political and apolitical comments\nmade during each week in our study. We see that while\nthe fraction of apolitical offensive comments has stayed\nsteady, there has been an increase in the fraction of of-\nfensive political comments starting in July 2016. No-\ntably, this increase is observed after the conclusion of the\nUS presidential primaries and during the period of the\nDemocratic and Republican National Conventions and\ndoes not reduce even after the conclusion of the US presi-\ndential elections held on November 8. Participants in po-\nlitical subreddits were 2.6% more likely to observe offen-\n\nsive comments prior to July 2016 but 14.9% more likely\nto observe offensive comments from July 2016 onwards. Figure 4: Fraction of offensive comments identi\ufb01ed in\npolitical and all subreddits. Reactions to offensive comments. We use the com-\nment score, roughly the difference between up-votes and\ndown-votes received, as a proxy for understanding how\nusers reacted to offensive comments. We \ufb01nd that com-\nments that were offensive: (1) on average, had a higher\nscore than non-offensive comments (average scores: 8.9\nvs. 6.7) and (2) were better received when they were\nposted in the general context than in the political context\n(average scores: 8.6 vs. 9.0). To understand how peo-\nples reactions to offensive comments evolved over time,\nFigure 5 shows the average scores received by offensive\ncomments over time. Again, we observe an increasing\ntrend in average scores received by offensive and politi-\ncal comments after July 2016. Figure 5: Average scores of offensive comments identi-\n\ufb01ed in political and all subreddits. Characteristics of offensive authors. We now focus\non understanding characteristics of authors of offensive\ncomments. Speci\ufb01cally, we are interested in identify-\ning the use of throwaway and troll accounts. For the\npurposes of this study, we characterize throwaway ac-\ncounts as those with less than \ufb01ve total comments \u2013 i.e.,\naccounts that are used to make a small number of com-\nments. Similarly, we de\ufb01ne troll accounts as those with\nover 15 comments of which over 75% are classi\ufb01ed as\noffensive \u2013 i.e., accounts that are used to make a larger\nnumber of comments, of which a signi\ufb01cant majority are\noffensive. We \ufb01nd that 93.7% of the accounts which\nhave over 75% of their comments tagged as offensive are\nthrowaways and 1.3% are trolls. Complete results are\nillustrated in Figure 6. Figure 6: CDF of the fraction of each authors comments\nthat were identi\ufb01ed as offensive. Green, orange, and red\ndots are used to represent authors with <5, 5-15, and\n>15 total comments, respectively. The legend provides\na breakdown per quartile. Characteristics of offensive communities. We break-\ndown subreddits by their category (default, political, and\nother) and identify the most and least offensive commu-\nnities in each. Figure 7 shows the distribution of the frac-\ntion of offensive comments in each category and Table 2\nshows the most and least offensive subreddits in the po-\nlitical and default categories (we exclude the \u201cother\u201d cat-\negory due to the inappropriateness of their names). We\n\ufb01nd that less than 19% of all subreddits (that account\nfor over 23% of all comments) have over 10% offensive\ncomments. Further, several default and political subred-\ndits fall in this category, including r/the donald \u2013 the\nmost offensive political subreddit and the subreddit ded-\nicated to the US President. Figure 7: Distribution of the fraction of offensive com-\nments observed in each subreddit category. Only subred-\ndits with over 1000 comments are considered. Flow of offensive authors. Finally, we uncover\npatterns in the movement of offensive authors between\ncommunities. In Figure 8 we show the communities\n\nCategory Most offensive (%) Least offensive (%)\nDefault r/tifu(15.1%) r/askscience(2.4%)\nr/announcements(13.2%)r/personalfinance(3.4%)\nr/askreddit(11.0%) r/science(3.8%)\nPolitical r/thedonald(11.4%) r/republican(4.4%)\nr/elections(10.2%) r/sandersforpresident(4.9%)\nr/worldpolitics(9.8%) r/tedcruz(5.1%)\nTable 2: Subreddits in the default and political categories\nwith the highest and lowest fraction of offensive com-\nments. in which large number of authors of offensive content\non the r/politics subreddit had previously made\noffensive comments (we refer to these communities\nas sources). Unsurprisingly, the most popular sources\nbelonged to the default subreddits ( e.g., r/worldnews,\nr/wtf, r/videos, r/askreddit, and r/news). We\n\ufb01nd that several other political subreddits also serve as\nlarge sources of offensive authors. In fact, the subreddits\ndedicated to the three most popular US presidential can-\ndidates \u2013 r/the donald, r/sandersforpresident,\nand r/hillaryclinton rank in the top three. Fi-\nnally, outside of the default and political subreddits,\nwe \ufb01nd that r/nfl, r/conspiracy, r/dota2,\nr/reactiongifs, r/blackpeopletwitter, and\nr/imgoingtohellforthis were the largest sources of\noffensive political authors. Figure 8: Flow of offensive authors. An edge between\ntwo subreddits indicates that authors made offensive\ncomments in the source subreddit before the \ufb01rst time\nthey made offensive comments in the destination subred-\ndit. Darker and thicker edges indicate larger \ufb02ow sizes\n(only \ufb02ows \u2265200 authors are shown). 5 Conclusions and Future Work\nWe develop and validate an offensive speech classi\ufb01er\nto quantify the presence of offensive online comments\nfrom January 2015 through January 2017. We \ufb01nd that\npolitical discussions on Reddit became increasingly less\ncivil \u2013 as measured by the incidence of offensive com-\nments \u2013 during the 2016 general election campaign. In\nfact, during the height of the campaign, nearly one of ev-\nery 10 comments posted on a political subreddit were\nclassi\ufb01ed as offensive. Offensive comments also re-\nceived more positive feedback from the community, even\nthough most of the accounts responsible for such com-\nments appear to be throwaway accounts. While offensive\nposts were increasingly common on political subreddits\nas the campaign wore on, there was no such increase in\nnon-political fora. This contrast provides additional ev-\nidence that the increasing use of offensive speech was\ndirectly related to the ramping up of the general election\ncampaign for president. Even though our study relies on just a single source\nof online political discussions \u2013 Reddit, we believe that\nour \ufb01ndings generally present an upper-bound on the in-\ncidence of offensiveness in online political discussions\nfor the following reasons: First, Reddit allows the use\nof pseudonymous identities that enables the online dis-\ninhibition effect (unlike social-media platforms such as\nFacebook). Second, Reddit enables users to engage in\ncomplex discussions that are unrestricted in length (un-\nlike Twitter). Finally, Reddit is known for enabling a\ngeneral culture of free speech and delegating content reg-\nulation to moderators of individual subreddits. This pro-\nvides users holding fringe views a variety of subreddits\nin which their content is welcome. Our \ufb01ndings provide a unique and important map-\nping of the increasing incivility of online political dis-\ncourse during the 2016 campaign. Such an investiga-\ntion is important because scholars have outlined many\nconsequences for incivility in political discourse. Inci-\nvility tends to \u201cturn off\u201d political moderates, leading to\nincreasing polarization among those who are actively en-\ngaged in politics [18]. More importantly, a lack of ci-\nvility in political discussions generally reduces the de-\ngree to which people view opponents as holding legit-\nimate viewpoints. This dynamic makes it dif\ufb01cult for\npeople to \ufb01nd common ground with those who disagree\nwith them [11] and it may ultimately lead citizens to\nview electoral victories by opponents as lacking legiti-\nmacy [12]. Thus, from a normative standpoint, the fact\nthat the 2016 campaign sparked a marked increase in the\noffensiveness of political comments posted to Reddit is\nof concern in its own right; that the incidence of offensive\npolitical comments has remained high even three months\nafter the election is all the more troubling. In future work, we will extend our analysis of Reddit\nback to 2007 with the aim of formulating a more com-\nplete understanding of the dynamics of political incivil-\nity. For example, we seek to understand whether the\nhigh incidence of offensive speech we \ufb01nd in 2016 is\nunique to this particular campaign or if previous presi-\ndential campaigns witnessed similar spikes in incivility. We will also examine whether there is a more general\nlong-term trend toward offensive online political speech,\nwhich would be consistent with what scholars have found\nwhen studying political elites [6, 16]. References\n[1] CrowdFlower Blog. Hate speech identi\ufb01cation. URL: https://www.crowdflower.com/data/h\nate-speech-identification/ (Accessed May\n24, 2017). [2] Justin H Gross and Kaylee T Johnson. Twitter\ntaunts and tirades: Negative campaigning in the age\nof trump. PS: Political Science &amp; Politics,\n49(4):748\u2013754, 2016. [3] Hatebase. Meet the Hatebase API. URL: https:\n//www.hatebase.org/connect_api (Accessed\nMay 24, 2017). [4] Susan Herbst. Rude democracy: Civility and in-\ncivility in American politics. Temple University\nPress, 2010.",
            0.619154041999991
          ]
        },
        {
          "answer": "The database includes several divisive political events including the 2016 US presidential elections.",
          "relevant_chunks": [
            "Measuring Offensive Speech in Online Political Discourse\nRishab Nithyanand1, Brian Schaffner2, Phillipa Gill1\n1{rishab, phillipa}@cs.umass.edu, 2schaffne@polsci.umass.edu\nUniversity of Massachusetts, Amherst\nAbstract\nThe Internet and online forums such as Reddit have be-\ncome an increasingly popular medium for citizens to\nengage in political conversations. However, the on-\nline disinhibition effect resulting from the ability to use\npseudonymous identities may manifest in the form of\noffensive speech, consequently making political discus-\nsions more aggressive and polarizing than they already\nare. Such environments may result in harassment and\nself-censorship from its targets. In this paper, we present\npreliminary results from a large-scale temporal measure-\nment aimed at quantifying offensiveness in online politi-\ncal discussions. To enable our measurements, we develop and evalu-\nate an offensive speech classi\ufb01er. We then use this clas-\nsi\ufb01er to quantify and compare offensiveness in the po-\nlitical and general contexts. We perform our study us-\ning a database of over 168M Reddit comments made by\nover 7M pseudonyms between January 2015 and January\n2017 \u2013 a period covering several divisive political events\nincluding the 2016 US presidential elections. 1 Introduction\nThe apparent rise in political incivility has attracted sub-\nstantial attention from scholars in recent years. These\nstudies have largely focused on the extent to which politi-\ncians and elected of\ufb01cials are increasingly employing\nrhetoric that appears to violate norms of civility [4, 12]. For the purposes of our work, we use the incidence of of-\nfensive rhetoric as a stand in for incivility. The 2016 US\npresidential election was an especially noteworthy case\nin this regard, particularly in terms of Donald Trump\u2019s\ncampaign which frequently violated norms of civility\nboth in how he spoke about broad groups in the pub-\nlic (such as Muslims, Mexicans, and African Americans)\nand the attacks he leveled at his opponents [2]. The con-\nsequences of incivility are thought to be crucial to the\nfunctioning of democracy since \u201cpublic civility and in-\nterpersonal politeness sustain social harmony and allow\npeople who disagree with one another to maintain ongo-\ning relationships\u201d [17]. While political incivility appears to be on the rise\namong elites, it is less clear whether this is true among\nthe mass public as well. Is political discourse particularly\nlacking in civility compared to discourse more generally? Does the incivility of mass political discourse respond to\nthe dynamics of political campaigns? Addressing these\nquestions has been dif\ufb01cult for political scientists be-\ncause traditional tools for studying mass behavior, such\nas public opinion surveys, are ill-equipped to measure\nhow citizens discuss politics with one another. Survey\ndata does reveal that the public tends to perceive politics\nas becoming increasingly less civil during the course of a\npolitical campaign [18]. Yet, it is not clear whether these\nperceptions match the reality, particularly in terms of the\ntypes of discussions that citizens have with each other. An additional question is how incivility is received\nby others. On one hand, violations of norms regard-\ning offensive discourse may be policed by members of\na community, rendering such speech ineffectual. On the\nother hand, offensive speech may be effective as a means\nfor drawing attention to a particular argument. Indeed,\nthere is evidence that increasing incivility in political\nspeech results in higher levels of attention from the pub-\nlic [12]. During the 2016 campaign, the use of swearing\nin comments posted on Donald Trump\u2019s YouTube chan-\nnel tended to result in additional responses that mimicked\nsuch swearing [8]. Thus, offensive speech in online fora\nmay attract more attention from the community and lead\nto the spread of even more offensive speech in subse-\nquent posts. To address these questions regarding political incivil-\nity, we examine the use of offensive speech in politi-\ncal discussions housed on Reddit. Scholars tend to de-\n\ufb01ne uncivil discourse as \u201ccommunication that violates\nthe norms of politeness\u201d [12] a de\ufb01nition that clearly in-\narXiv:1706.01875v2  [cs.CL]  19 Jul 2017\n\ncludes offensive remarks. Reddit fora represent a \u201cmost\nlikely\u201d case for the study of offensive political speech\ndue its strong free speech culture [14] and the ability of\nparticipants to use pseudonymous identities. That is, if\npolitical incivility in the public did increase during the\n2016 campaign, this should be especially evident on fora\nsuch as Reddit. Tracking Reddit discussions through-\nout all of 2015 and 2016, we \ufb01nd that online political\ndiscussions became increasingly more offensive as the\ngeneral election campaign intensi\ufb01ed. By comparison,\ndiscussions on non-political subreddits did not become\nincreasingly offensive during this period. Additionally,\nwe \ufb01nd that the presence of offensive comments did not\nsubside even three months after the election. 2 Datasets\nOur study makes use of multiple datasets in order to iden-\ntify and characterize trends in offensive speech. The CrowdFlower hate speech dataset. The Crowd-\nFlower hate speech dataset [1] contains 14.5K tweets,\neach receiving labels from at least three contributors. Contributors were allowed to classify each tweet into\none of three classes: Not Offensive (NO), Offensive but\nnot hateful (O), and Offensive and hateful (OH). Of the\n14.5K tweets, only 37.6% had a decisive class \u2013 i.e., the\nsame class was assigned by all contributors. For inde-\ncisive cases, the majority class was selected and a class\ncon\ufb01dence score (fraction of contributors that selected\nthe majority class) was made available. Using this ap-\nproach, 50.4%, 33.1%, and 16.5% of the tweets were cat-\negorized as NO, O, and OH, respectively. Since our goal\nis to identify any offensive speech (not just hate speech),\nwe consolidate assigned classes into Offensive and Not\nOffensive by relabeling OH tweets as Offensive. We use\nthis modi\ufb01ed dataset to train, validate, and test our offen-\nsive speech classi\ufb01er. To the best of our knowledge, this\nis the only dataset that provides offensive and not offen-\nsive annotations to a large dataset. Offensive word lists. We also use two offensive\nword lists as auxiliary input to our classi\ufb01er: (1) The\nHatebase hate speech vocabulary [3] consisting of 1122\nhateful words and (2) 422 offensive words banned from\nGoogle\u2019s What Do You Love project [7]. Reddit comments dataset. Finally, after building our\noffensive speech classi\ufb01er using the above datasets, we\nuse it to classify comments made on Reddit. While the\ncomplete Reddit dataset contains 2B comments made be-\ntween the period of January 2015 and January 2017, we\nonly analyze only 168M. We select comments to be ana-\nlyzed using the following process: (1) we exclude com-\nments shorter than 10 characters in length, (2) we ex-\nclude comments made by [deleted] authors, and (3)\nFigure 1: Number of analyzed political and apoliti-\ncal comments belonging to each week between January\n2015 and January 2017. we randomly sample and include 10% of all remaining\ncomments. We categorize comments made in any of 21\npopular political subreddits as political and the remain-\nder as apolitical. Our \ufb01nal dataset contains 129M apo-\nlitical and 39M political comments. Figure 1 shows the\nnumber of comments in our dataset that were made dur-\ning each week included in our study. We see an increas-\ning number of political comments per week starting in\nFebruary 2016 \u2013 the start of the 2016 US presidential\nprimaries. 3 Offensive Speech Classi\ufb01cation\nIn order to identify offensive speech, we propose a fully\nautomated technique that classi\ufb01es comments into two\nclasses: Offensive and Not Offensive. 3.1 Classi\ufb01cation approach\nAt a high-level, our approach works as follows:\n\u2022Build a word embedding. We construct a 100-\ndimensional word embedding using all comments\nfrom our complete Reddit dataset (2B comments). \u2022Construct a hate vector. We construct a list of of-\nfensive and hateful words identi\ufb01ed from external\ndata and map them into a single vector within the\nhigh-dimensional word embedding. \u2022Text transformation and classi\ufb01cation. Finally,\nwe transform text to be classi\ufb01ed into scalars rep-\nresenting their distance from the constructed hate\nvector and use these as input to a Random Forest\nclassi\ufb01er. Building a word embedding. At a high-level, a word\nembedding maps words into a high-dimensional continu-\nous vector space in such a way that semantic similarities\nbetween words are maintained. This mapping is achieved\n\nFigure 2: Proximity of offensive and non-offensive com-\nments to the hate vector. Dimension reduction is per-\nformed using t-SNE. by exploiting the distributional properties of words and\ntheir occurrences in the input corpus. Rather than using an off-the-shelf word embedding\n(e.g., the GloVe embeddings [13] trained using public do-\nmain data sources such as Wikipedia and news articles),\nwe construct a 100-dimensional embedding using the\ncomplete Reddit dataset (2B comments) as the input cor-\npus. The constructed embedding consists of over 400M\nunique words (words occurring less than 25 times in the\nentire corpus are excluded) using the Word2Vec [10] im-\nplementation provided by the Gensim library [15]. Prior\nto constructing the embedding, we perform stop-word\nremoval and lemmatize each word in the input corpus\nusing the SpaCy NLP framework [5]. The main reason\nfor building a custom embedding is to ensure that our\nembeddings capture semantics speci\ufb01c to the data be-\ning measured (Reddit) \u2013 e.g., while the word \u201ckarma\u201d in\nthe non-Reddit context may be associated with spiritual-\nity, it is associated with points (comment and submission\nscores) on Reddit. Constructing a hate vector. We use two lists of words\nassociated with hate [3] and offensive [7] speech to con-\nstruct a hate vector in our word embedding. This is\ndone by mapping each word in the list into the 100-\ndimensional embedding and computing the mean vector. This vector represents the average of all known offen-\nsive words. The main idea behind creating a hate vector\nis to capture the point (in our embedding) to which the\nmost offensive observed comments are likely to be near. Although clustering our offensive word lists into similar\ngroups and constructing multiple hate vectors \u2013 one for\neach cluster \u2013 results in marginally better accuracy for\nour classi\ufb01er, we use this approach due to the fact that\nour classi\ufb01cation cost grows linearly with the number of\nhate vectors \u2013 i.e., we need to perform O(|S|) distance\ncomputations per hate vector to classify string S. Transforming and classifying text. We \ufb01rst remove\nstop-words and perform lemmatization of each word in\nthe text to be classi\ufb01ed. We then obtain the vector repre-\nsenting each word in the text and compute its similarity\nto the constructed hate vector using the cosine similar-\nity metric. A 0-vector is used to represent words in the\ntext that are not present in the embedding. Finally, the\nmaximum cosine similarity score is used to represent the\ncomment. Equation 1 shows the transformation function\non a string S = {s1,..., sn}where si is the vector rep-\nresenting the ith lemmatized non-stop-word, cos is the\ncosine-similarity function, and H is the hate vector. T(S) =max\n1\u2264i\u2264n\n[cos(si,H)] (1)\nIn words, the numerical value assigned to a text is the\ncosine similarity between the hate vector and the vec-\ntor representing the word (in the text) closest to the hate\nvector. This approach allows us to transform a string of\ntext into a single numerical value that captures its se-\nmantic similarity to the most offensive comment. We\nuse these scalars as input to a random forest classi\ufb01er to\nperform classi\ufb01cation into Offensive and Not Offensive\nclasses. Figure 2 shows the proximity of Offensive and\nNon Offensive comments to our constructed hate vector\nafter using t-distributed Stochastic Neighbor Embedding\n(t-SNE) [9] to reduce our 100-dimension vector space\ninto 2 dimensions. 3.2 Classi\ufb01er evaluation\nWe now present results to (1) validate our choice\nof classi\ufb01er and (2) demonstrate the impact of train-\ning/validation sample count on our classi\ufb01ers perfor-\nmance. Classi\ufb01er Accuracy (%) F1-Score (%)\nStochastic Gradient Descent 80.7 80.0\nNaive Bayes 81.5 81.2\nDecision Tree 91.8 91.4\nRandom Forest 92.0 91.9\nTable 1: Average classi\ufb01er performance during 10-fold\ncross-validation on the training/validation set. Results\nshown are for the best performing parameters obtained\nusing a grid search. Classi\ufb01er selection methodology. To identify the\nmost suitable classi\ufb01er for classifying the scalars as-\nsociated with each text, we perform evaluations us-\ning the stochastic gradient descent, naive bayes, deci-\nsion tree, and random forest classi\ufb01ers. For each clas-\nsi\ufb01er, we split the CrowdFlower hate speech dataset\ninto a training/validation set (75%), and a holdout set\n(25%). We perform 10-fold cross-validation on the train-\ning/validation set to identify the best classi\ufb01er model and\n\n(a) Classi\ufb01er accuracy. (b) Classi\ufb01er precision. (c) Classi\ufb01er recall. Figure 3: Classi\ufb01er performance on holdout sets while\nvarying holdout set sizes and minimum con\ufb01dence\nthresholds. parameters (using a grid search). Based on the results of\nthis evaluation, we select a 100-estimator entropy-based\nsplitting random forest model as our classi\ufb01er. Table 1\nshows the mean accuracy and F1-score for each evalu-\nated classi\ufb01er during the 10-fold cross-validation. Real-world classi\ufb01er performance. To evaluate real-\nworld performance of our selected classi\ufb01er ( i.e., per-\nformance in the absence of model and parameter bias),\nwe perform classi\ufb01cation of the holdout set. On this set,\nour classi\ufb01er had an accuracy and F1-score of 89.6% and\n89.2%, respectively. These results show that in addition\nto superior accuracy during training and validation, our\nchosen classi\ufb01er is also robust against over-\ufb01tting. Impact of dataset quality and size. To understand\nhow the performance of our chosen classi\ufb01er model and\nparameters are impacted by: (1) the quality and consis-\ntency of manually assigned classes in the CrowdFlower\ndataset and (2) the size of the dataset, we re-evaluate the\nclassi\ufb01er while only considering tweets having a mini-\nmum con\ufb01dence score and varying the size of the holdout\nset. Speci\ufb01cally, our experiments considered con\ufb01dence\nthresholds of 0 (all tweets considered), .35 (only tweets\nwhere at least 35% of contributors agreed on a class were\nconsidered), and .70 (only tweets where at least 70% of\nthe contributors agreed on a class were considered) and\nvaried the holdout set sizes between 5% and 95% of all\ntweets meeting the con\ufb01dence threshold set for the ex-\nperiment. The results illustrated in Figure 3 show the perfor-\nmance of the classi\ufb01er while evaluating the correspond-\ning holdout set. We make several conclusions from these\nresults:\n\u2022Beyond a (fairly low) threshold, the size of the train-\ning and validation set has little impact on classi-\n\ufb01er performance. We see that the accuracy, pre-\ncision, and recall have, at best, marginal improve-\nments with holdout set sizes smaller than 60%. This\nimplies that the CrowdFlower dataset is suf\ufb01cient\nfor building an offensive speech classi\ufb01er. \u2022Quality of manual labeling has a signi\ufb01cant impact\non the accuracy and precision of the classi\ufb01er. Us-\ning only tweets which had at least 70% of contrib-\nutors agreeing on a class resulted in between 5-7%\nhigher accuracy and up to 5% higher precision. \u2022Our classi\ufb01er achieves precision of over 95% and\nrecall of over 85% when considering only high con-\n\ufb01dence samples. This implies that the classi\ufb01er is\nmore likely to underestimate the presence of offen-\nsive speech \u2013 i.e., our results likely provide a lower-\nbound on the quantity of observed offensive speech. 4 Measurements\nIn this section we quantify and characterize offensive-\nness in the political and general contexts using our offen-\nsive speech classi\ufb01er and the Reddit comments dataset\nwhich considers a random sample of comments made be-\ntween January 2015 and January 2017. Offensiveness over time. We \ufb01nd that on average\n8.4% of all political comments are offensive compared to\n7.8% of all apolitical comments. Figure 4 illustrates the\nfraction of offensive political and apolitical comments\nmade during each week in our study. We see that while\nthe fraction of apolitical offensive comments has stayed\nsteady, there has been an increase in the fraction of of-\nfensive political comments starting in July 2016. No-\ntably, this increase is observed after the conclusion of the\nUS presidential primaries and during the period of the\nDemocratic and Republican National Conventions and\ndoes not reduce even after the conclusion of the US presi-\ndential elections held on November 8. Participants in po-\nlitical subreddits were 2.6% more likely to observe offen-\n\nsive comments prior to July 2016 but 14.9% more likely\nto observe offensive comments from July 2016 onwards. Figure 4: Fraction of offensive comments identi\ufb01ed in\npolitical and all subreddits. Reactions to offensive comments. We use the com-\nment score, roughly the difference between up-votes and\ndown-votes received, as a proxy for understanding how\nusers reacted to offensive comments. We \ufb01nd that com-\nments that were offensive: (1) on average, had a higher\nscore than non-offensive comments (average scores: 8.9\nvs. 6.7) and (2) were better received when they were\nposted in the general context than in the political context\n(average scores: 8.6 vs. 9.0). To understand how peo-\nples reactions to offensive comments evolved over time,\nFigure 5 shows the average scores received by offensive\ncomments over time. Again, we observe an increasing\ntrend in average scores received by offensive and politi-\ncal comments after July 2016. Figure 5: Average scores of offensive comments identi-\n\ufb01ed in political and all subreddits. Characteristics of offensive authors. We now focus\non understanding characteristics of authors of offensive\ncomments. Speci\ufb01cally, we are interested in identify-\ning the use of throwaway and troll accounts. For the\npurposes of this study, we characterize throwaway ac-\ncounts as those with less than \ufb01ve total comments \u2013 i.e.,\naccounts that are used to make a small number of com-\nments. Similarly, we de\ufb01ne troll accounts as those with\nover 15 comments of which over 75% are classi\ufb01ed as\noffensive \u2013 i.e., accounts that are used to make a larger\nnumber of comments, of which a signi\ufb01cant majority are\noffensive. We \ufb01nd that 93.7% of the accounts which\nhave over 75% of their comments tagged as offensive are\nthrowaways and 1.3% are trolls. Complete results are\nillustrated in Figure 6. Figure 6: CDF of the fraction of each authors comments\nthat were identi\ufb01ed as offensive. Green, orange, and red\ndots are used to represent authors with <5, 5-15, and\n>15 total comments, respectively. The legend provides\na breakdown per quartile. Characteristics of offensive communities. We break-\ndown subreddits by their category (default, political, and\nother) and identify the most and least offensive commu-\nnities in each. Figure 7 shows the distribution of the frac-\ntion of offensive comments in each category and Table 2\nshows the most and least offensive subreddits in the po-\nlitical and default categories (we exclude the \u201cother\u201d cat-\negory due to the inappropriateness of their names). We\n\ufb01nd that less than 19% of all subreddits (that account\nfor over 23% of all comments) have over 10% offensive\ncomments. Further, several default and political subred-\ndits fall in this category, including r/the donald \u2013 the\nmost offensive political subreddit and the subreddit ded-\nicated to the US President. Figure 7: Distribution of the fraction of offensive com-\nments observed in each subreddit category. Only subred-\ndits with over 1000 comments are considered. Flow of offensive authors. Finally, we uncover\npatterns in the movement of offensive authors between\ncommunities. In Figure 8 we show the communities\n\nCategory Most offensive (%) Least offensive (%)\nDefault r/tifu(15.1%) r/askscience(2.4%)\nr/announcements(13.2%)r/personalfinance(3.4%)\nr/askreddit(11.0%) r/science(3.8%)\nPolitical r/thedonald(11.4%) r/republican(4.4%)\nr/elections(10.2%) r/sandersforpresident(4.9%)\nr/worldpolitics(9.8%) r/tedcruz(5.1%)\nTable 2: Subreddits in the default and political categories\nwith the highest and lowest fraction of offensive com-\nments. in which large number of authors of offensive content\non the r/politics subreddit had previously made\noffensive comments (we refer to these communities\nas sources). Unsurprisingly, the most popular sources\nbelonged to the default subreddits ( e.g., r/worldnews,\nr/wtf, r/videos, r/askreddit, and r/news). We\n\ufb01nd that several other political subreddits also serve as\nlarge sources of offensive authors. In fact, the subreddits\ndedicated to the three most popular US presidential can-\ndidates \u2013 r/the donald, r/sandersforpresident,\nand r/hillaryclinton rank in the top three. Fi-\nnally, outside of the default and political subreddits,\nwe \ufb01nd that r/nfl, r/conspiracy, r/dota2,\nr/reactiongifs, r/blackpeopletwitter, and\nr/imgoingtohellforthis were the largest sources of\noffensive political authors. Figure 8: Flow of offensive authors. An edge between\ntwo subreddits indicates that authors made offensive\ncomments in the source subreddit before the \ufb01rst time\nthey made offensive comments in the destination subred-\ndit. Darker and thicker edges indicate larger \ufb02ow sizes\n(only \ufb02ows \u2265200 authors are shown). 5 Conclusions and Future Work\nWe develop and validate an offensive speech classi\ufb01er\nto quantify the presence of offensive online comments\nfrom January 2015 through January 2017. We \ufb01nd that\npolitical discussions on Reddit became increasingly less\ncivil \u2013 as measured by the incidence of offensive com-\nments \u2013 during the 2016 general election campaign. In\nfact, during the height of the campaign, nearly one of ev-\nery 10 comments posted on a political subreddit were\nclassi\ufb01ed as offensive. Offensive comments also re-\nceived more positive feedback from the community, even\nthough most of the accounts responsible for such com-\nments appear to be throwaway accounts. While offensive\nposts were increasingly common on political subreddits\nas the campaign wore on, there was no such increase in\nnon-political fora. This contrast provides additional ev-\nidence that the increasing use of offensive speech was\ndirectly related to the ramping up of the general election\ncampaign for president. Even though our study relies on just a single source\nof online political discussions \u2013 Reddit, we believe that\nour \ufb01ndings generally present an upper-bound on the in-\ncidence of offensiveness in online political discussions\nfor the following reasons: First, Reddit allows the use\nof pseudonymous identities that enables the online dis-\ninhibition effect (unlike social-media platforms such as\nFacebook). Second, Reddit enables users to engage in\ncomplex discussions that are unrestricted in length (un-\nlike Twitter). Finally, Reddit is known for enabling a\ngeneral culture of free speech and delegating content reg-\nulation to moderators of individual subreddits. This pro-\nvides users holding fringe views a variety of subreddits\nin which their content is welcome. Our \ufb01ndings provide a unique and important map-\nping of the increasing incivility of online political dis-\ncourse during the 2016 campaign. Such an investiga-\ntion is important because scholars have outlined many\nconsequences for incivility in political discourse. Inci-\nvility tends to \u201cturn off\u201d political moderates, leading to\nincreasing polarization among those who are actively en-\ngaged in politics [18]. More importantly, a lack of ci-\nvility in political discussions generally reduces the de-\ngree to which people view opponents as holding legit-\nimate viewpoints. This dynamic makes it dif\ufb01cult for\npeople to \ufb01nd common ground with those who disagree\nwith them [11] and it may ultimately lead citizens to\nview electoral victories by opponents as lacking legiti-\nmacy [12]. Thus, from a normative standpoint, the fact\nthat the 2016 campaign sparked a marked increase in the\noffensiveness of political comments posted to Reddit is\nof concern in its own right; that the incidence of offensive\npolitical comments has remained high even three months\nafter the election is all the more troubling. In future work, we will extend our analysis of Reddit\nback to 2007 with the aim of formulating a more com-\nplete understanding of the dynamics of political incivil-\nity. For example, we seek to understand whether the\nhigh incidence of offensive speech we \ufb01nd in 2016 is\nunique to this particular campaign or if previous presi-\ndential campaigns witnessed similar spikes in incivility. We will also examine whether there is a more general\nlong-term trend toward offensive online political speech,\nwhich would be consistent with what scholars have found\nwhen studying political elites [6, 16]. References\n[1] CrowdFlower Blog. Hate speech identi\ufb01cation. URL: https://www.crowdflower.com/data/h\nate-speech-identification/ (Accessed May\n24, 2017). [2] Justin H Gross and Kaylee T Johnson. Twitter\ntaunts and tirades: Negative campaigning in the age\nof trump. PS: Political Science &amp; Politics,\n49(4):748\u2013754, 2016. [3] Hatebase. Meet the Hatebase API. URL: https:\n//www.hatebase.org/connect_api (Accessed\nMay 24, 2017). [4] Susan Herbst. Rude democracy: Civility and in-\ncivility in American politics. Temple University\nPress, 2010.\n\n[16] Daniel M Shea and Alex Sproveri. The rise and fall\nof nasty politics in america. PS: Political Science\n&amp; Politics, 45(03):416\u2013421, 2012. [17] J Cherie Strachan and Michael R Wolf. Politi-\ncal civility. PS: Political Science &amp; Politics,\n45(03):401\u2013404, 2012. [18] Michael R Wolf, J Cherie Strachan, and Daniel M\nShea. Incivility and standing \ufb01rm: A second layer\nof partisan division. PS: Political Science &amp;\nPolitics, 45(03):428\u2013434, 2012.\n\nAssociation for Computational Linguistics. [6] Kathleen Hall Jamieson and Erika Falk. Continu-\nity and change in civility in the house. In Polar-\nized politics: Congress and the president in a parti-\nsan era, pages 96\u2013108. Washington, DC: CQ Press,\n2000.",
            0.41731145800000036
          ]
        },
        {
          "answer": "Random Forest Classifier",
          "relevant_chunks": [
            "[9] Laurens van der Maaten and Geoffrey Hinton. Vi-\nsualizing Data Using t-SNE. Journal of Machine\nLearning Research, 9(Nov):2579\u20132605, 2008.\n\nMeasuring Offensive Speech in Online Political Discourse\nRishab Nithyanand1, Brian Schaffner2, Phillipa Gill1\n1{rishab, phillipa}@cs.umass.edu, 2schaffne@polsci.umass.edu\nUniversity of Massachusetts, Amherst\nAbstract\nThe Internet and online forums such as Reddit have be-\ncome an increasingly popular medium for citizens to\nengage in political conversations. However, the on-\nline disinhibition effect resulting from the ability to use\npseudonymous identities may manifest in the form of\noffensive speech, consequently making political discus-\nsions more aggressive and polarizing than they already\nare. Such environments may result in harassment and\nself-censorship from its targets. In this paper, we present\npreliminary results from a large-scale temporal measure-\nment aimed at quantifying offensiveness in online politi-\ncal discussions. To enable our measurements, we develop and evalu-\nate an offensive speech classi\ufb01er. We then use this clas-\nsi\ufb01er to quantify and compare offensiveness in the po-\nlitical and general contexts. We perform our study us-\ning a database of over 168M Reddit comments made by\nover 7M pseudonyms between January 2015 and January\n2017 \u2013 a period covering several divisive political events\nincluding the 2016 US presidential elections. 1 Introduction\nThe apparent rise in political incivility has attracted sub-\nstantial attention from scholars in recent years. These\nstudies have largely focused on the extent to which politi-\ncians and elected of\ufb01cials are increasingly employing\nrhetoric that appears to violate norms of civility [4, 12]. For the purposes of our work, we use the incidence of of-\nfensive rhetoric as a stand in for incivility. The 2016 US\npresidential election was an especially noteworthy case\nin this regard, particularly in terms of Donald Trump\u2019s\ncampaign which frequently violated norms of civility\nboth in how he spoke about broad groups in the pub-\nlic (such as Muslims, Mexicans, and African Americans)\nand the attacks he leveled at his opponents [2]. The con-\nsequences of incivility are thought to be crucial to the\nfunctioning of democracy since \u201cpublic civility and in-\nterpersonal politeness sustain social harmony and allow\npeople who disagree with one another to maintain ongo-\ning relationships\u201d [17]. While political incivility appears to be on the rise\namong elites, it is less clear whether this is true among\nthe mass public as well. Is political discourse particularly\nlacking in civility compared to discourse more generally? Does the incivility of mass political discourse respond to\nthe dynamics of political campaigns? Addressing these\nquestions has been dif\ufb01cult for political scientists be-\ncause traditional tools for studying mass behavior, such\nas public opinion surveys, are ill-equipped to measure\nhow citizens discuss politics with one another. Survey\ndata does reveal that the public tends to perceive politics\nas becoming increasingly less civil during the course of a\npolitical campaign [18]. Yet, it is not clear whether these\nperceptions match the reality, particularly in terms of the\ntypes of discussions that citizens have with each other. An additional question is how incivility is received\nby others. On one hand, violations of norms regard-\ning offensive discourse may be policed by members of\na community, rendering such speech ineffectual. On the\nother hand, offensive speech may be effective as a means\nfor drawing attention to a particular argument. Indeed,\nthere is evidence that increasing incivility in political\nspeech results in higher levels of attention from the pub-\nlic [12]. During the 2016 campaign, the use of swearing\nin comments posted on Donald Trump\u2019s YouTube chan-\nnel tended to result in additional responses that mimicked\nsuch swearing [8]. Thus, offensive speech in online fora\nmay attract more attention from the community and lead\nto the spread of even more offensive speech in subse-\nquent posts. To address these questions regarding political incivil-\nity, we examine the use of offensive speech in politi-\ncal discussions housed on Reddit. Scholars tend to de-\n\ufb01ne uncivil discourse as \u201ccommunication that violates\nthe norms of politeness\u201d [12] a de\ufb01nition that clearly in-\narXiv:1706.01875v2  [cs.CL]  19 Jul 2017\n\ncludes offensive remarks. Reddit fora represent a \u201cmost\nlikely\u201d case for the study of offensive political speech\ndue its strong free speech culture [14] and the ability of\nparticipants to use pseudonymous identities. That is, if\npolitical incivility in the public did increase during the\n2016 campaign, this should be especially evident on fora\nsuch as Reddit. Tracking Reddit discussions through-\nout all of 2015 and 2016, we \ufb01nd that online political\ndiscussions became increasingly more offensive as the\ngeneral election campaign intensi\ufb01ed. By comparison,\ndiscussions on non-political subreddits did not become\nincreasingly offensive during this period. Additionally,\nwe \ufb01nd that the presence of offensive comments did not\nsubside even three months after the election. 2 Datasets\nOur study makes use of multiple datasets in order to iden-\ntify and characterize trends in offensive speech. The CrowdFlower hate speech dataset. The Crowd-\nFlower hate speech dataset [1] contains 14.5K tweets,\neach receiving labels from at least three contributors. Contributors were allowed to classify each tweet into\none of three classes: Not Offensive (NO), Offensive but\nnot hateful (O), and Offensive and hateful (OH). Of the\n14.5K tweets, only 37.6% had a decisive class \u2013 i.e., the\nsame class was assigned by all contributors. For inde-\ncisive cases, the majority class was selected and a class\ncon\ufb01dence score (fraction of contributors that selected\nthe majority class) was made available. Using this ap-\nproach, 50.4%, 33.1%, and 16.5% of the tweets were cat-\negorized as NO, O, and OH, respectively. Since our goal\nis to identify any offensive speech (not just hate speech),\nwe consolidate assigned classes into Offensive and Not\nOffensive by relabeling OH tweets as Offensive. We use\nthis modi\ufb01ed dataset to train, validate, and test our offen-\nsive speech classi\ufb01er. To the best of our knowledge, this\nis the only dataset that provides offensive and not offen-\nsive annotations to a large dataset. Offensive word lists. We also use two offensive\nword lists as auxiliary input to our classi\ufb01er: (1) The\nHatebase hate speech vocabulary [3] consisting of 1122\nhateful words and (2) 422 offensive words banned from\nGoogle\u2019s What Do You Love project [7]. Reddit comments dataset. Finally, after building our\noffensive speech classi\ufb01er using the above datasets, we\nuse it to classify comments made on Reddit. While the\ncomplete Reddit dataset contains 2B comments made be-\ntween the period of January 2015 and January 2017, we\nonly analyze only 168M. We select comments to be ana-\nlyzed using the following process: (1) we exclude com-\nments shorter than 10 characters in length, (2) we ex-\nclude comments made by [deleted] authors, and (3)\nFigure 1: Number of analyzed political and apoliti-\ncal comments belonging to each week between January\n2015 and January 2017. we randomly sample and include 10% of all remaining\ncomments. We categorize comments made in any of 21\npopular political subreddits as political and the remain-\nder as apolitical. Our \ufb01nal dataset contains 129M apo-\nlitical and 39M political comments. Figure 1 shows the\nnumber of comments in our dataset that were made dur-\ning each week included in our study. We see an increas-\ning number of political comments per week starting in\nFebruary 2016 \u2013 the start of the 2016 US presidential\nprimaries. 3 Offensive Speech Classi\ufb01cation\nIn order to identify offensive speech, we propose a fully\nautomated technique that classi\ufb01es comments into two\nclasses: Offensive and Not Offensive. 3.1 Classi\ufb01cation approach\nAt a high-level, our approach works as follows:\n\u2022Build a word embedding. We construct a 100-\ndimensional word embedding using all comments\nfrom our complete Reddit dataset (2B comments). \u2022Construct a hate vector. We construct a list of of-\nfensive and hateful words identi\ufb01ed from external\ndata and map them into a single vector within the\nhigh-dimensional word embedding. \u2022Text transformation and classi\ufb01cation. Finally,\nwe transform text to be classi\ufb01ed into scalars rep-\nresenting their distance from the constructed hate\nvector and use these as input to a Random Forest\nclassi\ufb01er. Building a word embedding. At a high-level, a word\nembedding maps words into a high-dimensional continu-\nous vector space in such a way that semantic similarities\nbetween words are maintained. This mapping is achieved\n\nFigure 2: Proximity of offensive and non-offensive com-\nments to the hate vector. Dimension reduction is per-\nformed using t-SNE. by exploiting the distributional properties of words and\ntheir occurrences in the input corpus. Rather than using an off-the-shelf word embedding\n(e.g., the GloVe embeddings [13] trained using public do-\nmain data sources such as Wikipedia and news articles),\nwe construct a 100-dimensional embedding using the\ncomplete Reddit dataset (2B comments) as the input cor-\npus. The constructed embedding consists of over 400M\nunique words (words occurring less than 25 times in the\nentire corpus are excluded) using the Word2Vec [10] im-\nplementation provided by the Gensim library [15]. Prior\nto constructing the embedding, we perform stop-word\nremoval and lemmatize each word in the input corpus\nusing the SpaCy NLP framework [5]. The main reason\nfor building a custom embedding is to ensure that our\nembeddings capture semantics speci\ufb01c to the data be-\ning measured (Reddit) \u2013 e.g., while the word \u201ckarma\u201d in\nthe non-Reddit context may be associated with spiritual-\nity, it is associated with points (comment and submission\nscores) on Reddit. Constructing a hate vector. We use two lists of words\nassociated with hate [3] and offensive [7] speech to con-\nstruct a hate vector in our word embedding. This is\ndone by mapping each word in the list into the 100-\ndimensional embedding and computing the mean vector. This vector represents the average of all known offen-\nsive words. The main idea behind creating a hate vector\nis to capture the point (in our embedding) to which the\nmost offensive observed comments are likely to be near. Although clustering our offensive word lists into similar\ngroups and constructing multiple hate vectors \u2013 one for\neach cluster \u2013 results in marginally better accuracy for\nour classi\ufb01er, we use this approach due to the fact that\nour classi\ufb01cation cost grows linearly with the number of\nhate vectors \u2013 i.e., we need to perform O(|S|) distance\ncomputations per hate vector to classify string S. Transforming and classifying text. We \ufb01rst remove\nstop-words and perform lemmatization of each word in\nthe text to be classi\ufb01ed. We then obtain the vector repre-\nsenting each word in the text and compute its similarity\nto the constructed hate vector using the cosine similar-\nity metric. A 0-vector is used to represent words in the\ntext that are not present in the embedding. Finally, the\nmaximum cosine similarity score is used to represent the\ncomment. Equation 1 shows the transformation function\non a string S = {s1,..., sn}where si is the vector rep-\nresenting the ith lemmatized non-stop-word, cos is the\ncosine-similarity function, and H is the hate vector. T(S) =max\n1\u2264i\u2264n\n[cos(si,H)] (1)\nIn words, the numerical value assigned to a text is the\ncosine similarity between the hate vector and the vec-\ntor representing the word (in the text) closest to the hate\nvector. This approach allows us to transform a string of\ntext into a single numerical value that captures its se-\nmantic similarity to the most offensive comment. We\nuse these scalars as input to a random forest classi\ufb01er to\nperform classi\ufb01cation into Offensive and Not Offensive\nclasses. Figure 2 shows the proximity of Offensive and\nNon Offensive comments to our constructed hate vector\nafter using t-distributed Stochastic Neighbor Embedding\n(t-SNE) [9] to reduce our 100-dimension vector space\ninto 2 dimensions. 3.2 Classi\ufb01er evaluation\nWe now present results to (1) validate our choice\nof classi\ufb01er and (2) demonstrate the impact of train-\ning/validation sample count on our classi\ufb01ers perfor-\nmance. Classi\ufb01er Accuracy (%) F1-Score (%)\nStochastic Gradient Descent 80.7 80.0\nNaive Bayes 81.5 81.2\nDecision Tree 91.8 91.4\nRandom Forest 92.0 91.9\nTable 1: Average classi\ufb01er performance during 10-fold\ncross-validation on the training/validation set. Results\nshown are for the best performing parameters obtained\nusing a grid search. Classi\ufb01er selection methodology. To identify the\nmost suitable classi\ufb01er for classifying the scalars as-\nsociated with each text, we perform evaluations us-\ning the stochastic gradient descent, naive bayes, deci-\nsion tree, and random forest classi\ufb01ers. For each clas-\nsi\ufb01er, we split the CrowdFlower hate speech dataset\ninto a training/validation set (75%), and a holdout set\n(25%). We perform 10-fold cross-validation on the train-\ning/validation set to identify the best classi\ufb01er model and\n\n(a) Classi\ufb01er accuracy. (b) Classi\ufb01er precision. (c) Classi\ufb01er recall. Figure 3: Classi\ufb01er performance on holdout sets while\nvarying holdout set sizes and minimum con\ufb01dence\nthresholds. parameters (using a grid search). Based on the results of\nthis evaluation, we select a 100-estimator entropy-based\nsplitting random forest model as our classi\ufb01er. Table 1\nshows the mean accuracy and F1-score for each evalu-\nated classi\ufb01er during the 10-fold cross-validation. Real-world classi\ufb01er performance. To evaluate real-\nworld performance of our selected classi\ufb01er ( i.e., per-\nformance in the absence of model and parameter bias),\nwe perform classi\ufb01cation of the holdout set. On this set,\nour classi\ufb01er had an accuracy and F1-score of 89.6% and\n89.2%, respectively. These results show that in addition\nto superior accuracy during training and validation, our\nchosen classi\ufb01er is also robust against over-\ufb01tting. Impact of dataset quality and size. To understand\nhow the performance of our chosen classi\ufb01er model and\nparameters are impacted by: (1) the quality and consis-\ntency of manually assigned classes in the CrowdFlower\ndataset and (2) the size of the dataset, we re-evaluate the\nclassi\ufb01er while only considering tweets having a mini-\nmum con\ufb01dence score and varying the size of the holdout\nset. Speci\ufb01cally, our experiments considered con\ufb01dence\nthresholds of 0 (all tweets considered), .35 (only tweets\nwhere at least 35% of contributors agreed on a class were\nconsidered), and .70 (only tweets where at least 70% of\nthe contributors agreed on a class were considered) and\nvaried the holdout set sizes between 5% and 95% of all\ntweets meeting the con\ufb01dence threshold set for the ex-\nperiment. The results illustrated in Figure 3 show the perfor-\nmance of the classi\ufb01er while evaluating the correspond-\ning holdout set. We make several conclusions from these\nresults:\n\u2022Beyond a (fairly low) threshold, the size of the train-\ning and validation set has little impact on classi-\n\ufb01er performance. We see that the accuracy, pre-\ncision, and recall have, at best, marginal improve-\nments with holdout set sizes smaller than 60%. This\nimplies that the CrowdFlower dataset is suf\ufb01cient\nfor building an offensive speech classi\ufb01er. \u2022Quality of manual labeling has a signi\ufb01cant impact\non the accuracy and precision of the classi\ufb01er. Us-\ning only tweets which had at least 70% of contrib-\nutors agreeing on a class resulted in between 5-7%\nhigher accuracy and up to 5% higher precision. \u2022Our classi\ufb01er achieves precision of over 95% and\nrecall of over 85% when considering only high con-\n\ufb01dence samples. This implies that the classi\ufb01er is\nmore likely to underestimate the presence of offen-\nsive speech \u2013 i.e., our results likely provide a lower-\nbound on the quantity of observed offensive speech. 4 Measurements\nIn this section we quantify and characterize offensive-\nness in the political and general contexts using our offen-\nsive speech classi\ufb01er and the Reddit comments dataset\nwhich considers a random sample of comments made be-\ntween January 2015 and January 2017. Offensiveness over time. We \ufb01nd that on average\n8.4% of all political comments are offensive compared to\n7.8% of all apolitical comments. Figure 4 illustrates the\nfraction of offensive political and apolitical comments\nmade during each week in our study. We see that while\nthe fraction of apolitical offensive comments has stayed\nsteady, there has been an increase in the fraction of of-\nfensive political comments starting in July 2016. No-\ntably, this increase is observed after the conclusion of the\nUS presidential primaries and during the period of the\nDemocratic and Republican National Conventions and\ndoes not reduce even after the conclusion of the US presi-\ndential elections held on November 8. Participants in po-\nlitical subreddits were 2.6% more likely to observe offen-\n\nsive comments prior to July 2016 but 14.9% more likely\nto observe offensive comments from July 2016 onwards. Figure 4: Fraction of offensive comments identi\ufb01ed in\npolitical and all subreddits. Reactions to offensive comments. We use the com-\nment score, roughly the difference between up-votes and\ndown-votes received, as a proxy for understanding how\nusers reacted to offensive comments. We \ufb01nd that com-\nments that were offensive: (1) on average, had a higher\nscore than non-offensive comments (average scores: 8.9\nvs. 6.7) and (2) were better received when they were\nposted in the general context than in the political context\n(average scores: 8.6 vs. 9.0). To understand how peo-\nples reactions to offensive comments evolved over time,\nFigure 5 shows the average scores received by offensive\ncomments over time. Again, we observe an increasing\ntrend in average scores received by offensive and politi-\ncal comments after July 2016. Figure 5: Average scores of offensive comments identi-\n\ufb01ed in political and all subreddits. Characteristics of offensive authors. We now focus\non understanding characteristics of authors of offensive\ncomments. Speci\ufb01cally, we are interested in identify-\ning the use of throwaway and troll accounts. For the\npurposes of this study, we characterize throwaway ac-\ncounts as those with less than \ufb01ve total comments \u2013 i.e.,\naccounts that are used to make a small number of com-\nments. Similarly, we de\ufb01ne troll accounts as those with\nover 15 comments of which over 75% are classi\ufb01ed as\noffensive \u2013 i.e., accounts that are used to make a larger\nnumber of comments, of which a signi\ufb01cant majority are\noffensive. We \ufb01nd that 93.7% of the accounts which\nhave over 75% of their comments tagged as offensive are\nthrowaways and 1.3% are trolls. Complete results are\nillustrated in Figure 6. Figure 6: CDF of the fraction of each authors comments\nthat were identi\ufb01ed as offensive. Green, orange, and red\ndots are used to represent authors with <5, 5-15, and\n>15 total comments, respectively. The legend provides\na breakdown per quartile. Characteristics of offensive communities. We break-\ndown subreddits by their category (default, political, and\nother) and identify the most and least offensive commu-\nnities in each. Figure 7 shows the distribution of the frac-\ntion of offensive comments in each category and Table 2\nshows the most and least offensive subreddits in the po-\nlitical and default categories (we exclude the \u201cother\u201d cat-\negory due to the inappropriateness of their names). We\n\ufb01nd that less than 19% of all subreddits (that account\nfor over 23% of all comments) have over 10% offensive\ncomments. Further, several default and political subred-\ndits fall in this category, including r/the donald \u2013 the\nmost offensive political subreddit and the subreddit ded-\nicated to the US President. Figure 7: Distribution of the fraction of offensive com-\nments observed in each subreddit category. Only subred-\ndits with over 1000 comments are considered. Flow of offensive authors. Finally, we uncover\npatterns in the movement of offensive authors between\ncommunities. In Figure 8 we show the communities\n\nCategory Most offensive (%) Least offensive (%)\nDefault r/tifu(15.1%) r/askscience(2.4%)\nr/announcements(13.2%)r/personalfinance(3.4%)\nr/askreddit(11.0%) r/science(3.8%)\nPolitical r/thedonald(11.4%) r/republican(4.4%)\nr/elections(10.2%) r/sandersforpresident(4.9%)\nr/worldpolitics(9.8%) r/tedcruz(5.1%)\nTable 2: Subreddits in the default and political categories\nwith the highest and lowest fraction of offensive com-\nments. in which large number of authors of offensive content\non the r/politics subreddit had previously made\noffensive comments (we refer to these communities\nas sources). Unsurprisingly, the most popular sources\nbelonged to the default subreddits ( e.g., r/worldnews,\nr/wtf, r/videos, r/askreddit, and r/news). We\n\ufb01nd that several other political subreddits also serve as\nlarge sources of offensive authors. In fact, the subreddits\ndedicated to the three most popular US presidential can-\ndidates \u2013 r/the donald, r/sandersforpresident,\nand r/hillaryclinton rank in the top three. Fi-\nnally, outside of the default and political subreddits,\nwe \ufb01nd that r/nfl, r/conspiracy, r/dota2,\nr/reactiongifs, r/blackpeopletwitter, and\nr/imgoingtohellforthis were the largest sources of\noffensive political authors. Figure 8: Flow of offensive authors. An edge between\ntwo subreddits indicates that authors made offensive\ncomments in the source subreddit before the \ufb01rst time\nthey made offensive comments in the destination subred-\ndit. Darker and thicker edges indicate larger \ufb02ow sizes\n(only \ufb02ows \u2265200 authors are shown). 5 Conclusions and Future Work\nWe develop and validate an offensive speech classi\ufb01er\nto quantify the presence of offensive online comments\nfrom January 2015 through January 2017. We \ufb01nd that\npolitical discussions on Reddit became increasingly less\ncivil \u2013 as measured by the incidence of offensive com-\nments \u2013 during the 2016 general election campaign. In\nfact, during the height of the campaign, nearly one of ev-\nery 10 comments posted on a political subreddit were\nclassi\ufb01ed as offensive. Offensive comments also re-\nceived more positive feedback from the community, even\nthough most of the accounts responsible for such com-\nments appear to be throwaway accounts. While offensive\nposts were increasingly common on political subreddits\nas the campaign wore on, there was no such increase in\nnon-political fora. This contrast provides additional ev-\nidence that the increasing use of offensive speech was\ndirectly related to the ramping up of the general election\ncampaign for president. Even though our study relies on just a single source\nof online political discussions \u2013 Reddit, we believe that\nour \ufb01ndings generally present an upper-bound on the in-\ncidence of offensiveness in online political discussions\nfor the following reasons: First, Reddit allows the use\nof pseudonymous identities that enables the online dis-\ninhibition effect (unlike social-media platforms such as\nFacebook). Second, Reddit enables users to engage in\ncomplex discussions that are unrestricted in length (un-\nlike Twitter). Finally, Reddit is known for enabling a\ngeneral culture of free speech and delegating content reg-\nulation to moderators of individual subreddits. This pro-\nvides users holding fringe views a variety of subreddits\nin which their content is welcome. Our \ufb01ndings provide a unique and important map-\nping of the increasing incivility of online political dis-\ncourse during the 2016 campaign. Such an investiga-\ntion is important because scholars have outlined many\nconsequences for incivility in political discourse. Inci-\nvility tends to \u201cturn off\u201d political moderates, leading to\nincreasing polarization among those who are actively en-\ngaged in politics [18]. More importantly, a lack of ci-\nvility in political discussions generally reduces the de-\ngree to which people view opponents as holding legit-\nimate viewpoints. This dynamic makes it dif\ufb01cult for\npeople to \ufb01nd common ground with those who disagree\nwith them [11] and it may ultimately lead citizens to\nview electoral victories by opponents as lacking legiti-\nmacy [12]. Thus, from a normative standpoint, the fact\nthat the 2016 campaign sparked a marked increase in the\noffensiveness of political comments posted to Reddit is\nof concern in its own right; that the incidence of offensive\npolitical comments has remained high even three months\nafter the election is all the more troubling. In future work, we will extend our analysis of Reddit\nback to 2007 with the aim of formulating a more com-\nplete understanding of the dynamics of political incivil-\nity. For example, we seek to understand whether the\nhigh incidence of offensive speech we \ufb01nd in 2016 is\nunique to this particular campaign or if previous presi-\ndential campaigns witnessed similar spikes in incivility. We will also examine whether there is a more general\nlong-term trend toward offensive online political speech,\nwhich would be consistent with what scholars have found\nwhen studying political elites [6, 16]. References\n[1] CrowdFlower Blog. Hate speech identi\ufb01cation. URL: https://www.crowdflower.com/data/h\nate-speech-identification/ (Accessed May\n24, 2017). [2] Justin H Gross and Kaylee T Johnson. Twitter\ntaunts and tirades: Negative campaigning in the age\nof trump. PS: Political Science &amp; Politics,\n49(4):748\u2013754, 2016. [3] Hatebase. Meet the Hatebase API. URL: https:\n//www.hatebase.org/connect_api (Accessed\nMay 24, 2017). [4] Susan Herbst. Rude democracy: Civility and in-\ncivility in American politics. Temple University\nPress, 2010.\n\nSoftware Frame-\nwork for Topic Modelling with Large Corpora. In\nProceedings of the LREC 2010 Workshop on New\nChallenges for NLP Frameworks, pages 45\u201350,\nValletta, Malta, 2010. ELRA.",
            0.9061898749999955
          ]
        }
      ],
      "reference_answers": [
        "stochastic gradient descent, naive bayes, decision tree",
        "accuracy and F1-score of 89.6% and 89.2%, respectively accuracy and F1-score of 89.6% and 89.2%, respectively",
        "US presidential primaries, Democratic and Republican National Conventions",
        "Random Forest"
      ]
    },
    "1801.02073.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.0892796100385863,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.5111111111111111,
          "recall": 0.3645833333333333,
          "f1": 0.3981087470449172
        },
        "rouge-2": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        },
        "rouge-L": {
          "precision": 0.4888888888888889,
          "recall": 0.3541666666666667,
          "f1": 0.38392434988179663
        }
      },
      "per_question_metrics": [
        {
          "question": "Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?",
          "generated_answer": "Yes.",
          "reference_answer": "Yes No",
          "rouge_scores": {
            "rouge-1": {
              "precision": 1.0,
              "recall": 0.5,
              "f1": 0.6666666666666666
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 1.0,
              "recall": 0.5,
              "f1": 0.6666666666666666
            }
          },
          "bleu_score": 0.1495348781221221,
          "exact_match": 0.0
        },
        {
          "question": "How many question types do they find in the datasets analyzed?",
          "generated_answer": "Seven question types",
          "reference_answer": "seven  7",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.3333333333333333,
              "recall": 0.5,
              "f1": 0.4
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.3333333333333333,
              "recall": 0.5,
              "f1": 0.4
            }
          },
          "bleu_score": 0.11362193664674995,
          "exact_match": 0.0
        },
        {
          "question": "How do they analyze contextual similaries across datasets?",
          "generated_answer": "The context does not provide specific information on how they analyze contextual similarities across datasets.",
          "reference_answer": "They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap.",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.2,
              "recall": 0.09375,
              "f1": 0.12765957446808512
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.13333333333333333,
              "recall": 0.0625,
              "f1": 0.0851063829787234
            }
          },
          "bleu_score": 0.004682015346886826,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?",
        "How many question types do they find in the datasets analyzed?",
        "How do they analyze contextual similaries across datasets?"
      ],
      "generated_answers": [
        {
          "answer": "Yes.",
          "relevant_chunks": [
            "although WIKI QA comes very close.\n\nAnalysis of Wikipedia-based Corpora for Question Answering\nTomasz Jurczyk1, Amit Deshmane2, Jinho D. Choi1\n1Mathematics and Computer Science, Emory University\n2Infosys Ltd. {tomasz.jurczyk,jinho.choi}@emory.edu\namit deshmane@infosys.com\nAbstract\nThis paper gives comprehensive analyses\nof corpora based on Wikipedia for several\ntasks in question answering. Four recent\ncorpora are collected, WIKI QA, SELQA,\nSQUAD, and INFOBOX QA, and \ufb01rst an-\nalyzed intrinsically by contextual similari-\nties, question types, and answer categories. These corpora are then analyzed extrinsi-\ncally by three question answering tasks, an-\nswer retrieval, selection, and triggering. An\nindexing-based method for the creation of a\nsilver-standard dataset for answer retrieval\nusing the entire Wikipedia is also presented. Our analysis shows the uniqueness of these\ncorpora and suggests a better use of them\nfor statistical question answering learning. 1 Introduction\nQuestion answering (QA) has been a blooming re-\nsearch \ufb01eld for the last decade. Selection-based QA\nimplies a family of tasks that \ufb01nd answer contexts\nfrom large data given questions in natural language. Three tasks have been proposed for selection-based\nQA. Given a document, answer extraction (Shen\nand Klakow, 2006; Sultan et al., 2016) \ufb01nds answer\nphrases whereas answer selection (Wang et al.,\n2007; Yih et al., 2013; Yu et al., 2014; Wang et al.,\n2016) and answer triggering (Yang et al., 2015;\nJurczyk et al., 2016) \ufb01nd answer sentences instead,\nalthough the presence of the answer context is not\nassumed within the provided document for answer\ntriggering but it is for the other two tasks. Recently,\nvarious QA tasks that are not selection-based have\nbeen proposed (Reddy and Bandyopadhyay, 2006;\nHosseini et al., 2014; Jauhar et al., 2016; Sachan\net al., 2016); however, selection-based QA remains\nstill important because of its practical value to real\napplications (e.g., IBM Watson, MIT START). Several datasets have been released for selection-\nbased QA. Wang et al. (2007) created theQAS ENT\ndataset consisting of 277 questions, which has been\nwidely used for benchmarking the answer selection\ntask. Feng et al. (2015) presented INSURANCE QA\ncomprising 16K+ questions on insurance contexts. Yang et al. (2015) introduced WIKI QA for answer\nselection and triggering. Jurczyk et al. (2016) cre-\nated SELQA for large real-scale answer triggering. Rajpurkar et al. (2016) presented SQUAD for an-\nswer extraction and selection as well as for reading\ncomprehension. Finally, Morales et al. (2016) pro-\nvided INFOBOX QA for answer selection. These corpora make it possible to evaluate the ro-\nbustness of statistical question answering learning. Although all of these corpora target on selection-\nbased QA, they are designed for different purposes\nsuch that it is important to understand the nature of\nthese corpora so a better use of them can be made. In this paper, we make both intrinsic and extrinsic\nanalyses of four latest corpora based on Wikipedia,\nWIKI QA, SELQA, SQUAD, and INFOBOX QA. We \ufb01rst give a thorough intrinsic analysis regarding\ncontextual similarities, question types, and answer\ncategories (Section 2). We then map questions in all\ncorpora to the current version of English Wikipedia\nand benchmark another selection-based QA task,\nanswer retrieval (Section 3). Finally, we present\nan extrinsic analysis through a set of experiments\ncross-testing these corpora using a convolutional\nneural network architecture (Section 4).1\n2 Intrinsic Analysis\nFour publicly available corpora are selected for our\nanalysis. These corpora are based on Wikipedia, so\nmore comparable than the others, and have already\nbeen used for the evaluation of several QA systems. 1All our resources are publicly available\n: anonymous_url\narXiv:1801.02073v2  [cs.CL]  5 Feb 2018\n\nWIKIQA SELQA SQUAD INFOBOX QA\nSource Bing search queries Crowdsourced Crowdsourced Crowdsourced\nYear 2015 2016 2016 2016\n(AE, AS, AT) (O, O, O) (X, O, O) (O, O, X) (X, O, X)\n(q,c,c/q) (1,242, 12,153, 9.79) (7,904, 95,250, 12.05) (98,202, 496,167, 5.05) (15,271, 271,038, 17.75)\n(w,t) (386,440, 30,191) (3,469,015, 44,099) (19,445,863, 115,092) (5,034,625, 8,323)\n(\u00b5q,\u00b5c) (6.44, 25.36) (11.11, 25.31) (11.33, 27.86) (9.35, 9.22)\n(\u2126q,\u2126a,\u2126f) (46.72, 11.05, 16.96) (32.79, 16.98, 20.19) (32.27, 12.15, 16.54) (26.80, 35.70, 28.09)\nTable 1: Comparisons between the four corpora for answer selection. Note that both WIKI QA and SELQA provide separate\nannotation for answer triggering, which is not shown in this table. The SQUAD column shows statistics excluding the evaluation\nset, which is not publicly available. AE/AS/AT: annotation for answer extraction/selection/triggering, q/c: # of questions/answer\ncandidates, w/t: # of tokens/token types, \u00b5q/c: average length of questions/answer candidates, \u2126q/a: macro average in % of\noverlapping words between question-answer pairs normalized by the questions/answers lengths, \u2126f : (2\u00b7\u2126q\u00b7\u2126a)/(\u2126q+\u2126a). WIKI QA (Yang et al., 2015) comprises questions\nselected from the Bing search queries, where user\nclick data give the questions and their correspond-\ning Wikipedia articles. The abstracts of these arti-\ncles are then extracted to create answer candidates. The assumption is made that if many queries lead to\nthe same article, it must contain the answer context;\nhowever, this assumption fails for some occasions,\nwhich makes this dataset more challenging. Since\nthe existence of answer contexts is not guaranteed\nin this task, it is called answer triggering instead of\nanswer selection. SELQA (Jurczyk et al., 2016) is a product of \ufb01ve\nannotation tasks through crowdsourcing. It consists\nof about 8K questions where a half of the questions\nare paraphrased from the other half, aiming to re-\nduce contextual similarities between questions and\nanswers. Each question is associated with a section\nin Wikipedia where the answer context is guaran-\nteed, and also with \ufb01ve sections selected from the\nentire Wikipedia where the selection is made by\nthe Lucene search engine. This second dataset does\nnot assume the existence of the answer context, so\ncan be used for the evaluation of answer triggering. SQUAD (Rajpurkar et al., 2016) presents 107K+\ncrowdsourced questions on 536 Wikipedia articles,\nwhere the answer contexts are guaranteed to exist\nwithin the provided paragraph. It contains annota-\ntion of answer phrases as well as the pointers to the\nsentences including the answer phrases; thus, it can\nbe used for both answer extraction and selection. This corpus also provides human accuracy on those\nquestions, setting up a reasonable upper bound for\nmachines. To avoid over\ufb01tting, the evaluation set\nis not publicly available although system outputs\ncan be evaluated by their provided script. INFOBOX QA (Morales et al., 2016) gives 15K+\nquestions based on the infoboxes from 150 articles\nin Wikipedia. Each question is crowdsourced and\nassociated with an infobox, where each line of the\ninfobox is considered an answer candidate. This\ncorpus emphasizes the gravity of infoboxes, which\nsummary arguably the most commonly asked infor-\nmation about those articles. Although the nature of\nthis corpus is different from the others, it can also\nbe used to evaluate answer selection. Analysis\nAll corpora provide datasets/splits for answer se-\nlection, whereas only ( WIKI QA, SQUAD) and\n(WIKI QA, SELQA) provide datasets for answer\nextraction and answer triggering, respectively. SQUAD is much larger in size although ques-\ntions in this corpus are often paraphrased multi-\nple times. On the contrary, SQUAD\u2019s average\ncandidates per question ( c/q) is the smallest be-\ncause SQUAD extracts answer candidates from\nparagraphs whereas the others extract them from\nsections or infoboxes that consist of bigger con-\ntexts. Although INFOBOX QA is larger than WIK-\nIQA or SELQA, the number of token types (t) in\nINFOBOX QA is smaller than those two, due to the\nrepetitive nature of infoboxes. All corpora show similar average answer candi-\ndate lengths (\u00b5c), except for INFOBOX QA where\neach line in the infobox is considered a candidate. SELQA and SQUAD show similar average ques-\ntion lengths (\u00b5q) because of the similarity between\ntheir annotation schemes. It is not surprising that\nWIKI QA\u2019s average question length is the smallest,\nconsidering their questions are taken from search\nqueries. INFOBOX QA\u2019s average question length is\nrelatively small, due to the restricted information\nthat can be asked from the infoboxes.INFOBOX QA\nand WIKI QA show the least question-answer word\noverlaps over questions and answers (\u2126q and \u2126a in\nTable 1), respectively. In terms of the F1-score for\noverlapping words (\u2126f ), SQUAD gives the least\nportion of overlaps between question-answer pairs\n\nWIKI QA SELQA SQUAD\n(\u03c1,\u03b3c,\u03b3p),t \u2265 0.3 ( 92 .00, 1,203, 96.86) (90.00, 7,446, 94.28) (100.00, 93,928, 95.61)\n(\u03c1,\u03b3c,\u03b3p),t \u2265 0.4 ( 94.00, 1,139, 91.71) (94.00, 7,133, 90.31) (100.00, 93,928, 95.61)\n(\u03c1,\u03b3c,\u03b3p),t \u2265 0.5 (100.00, 1,051, 84.62) (98.00, 6,870, 86.98) (100.00, 93,928, 95.61)\nk= (1,5,10,20) (4.39,12.47,16.59,22.39) (20.01,34.07,40.29,46.40) (19.90,35.08,40.96,46.74)\nTable 2: Statistics of the silver-standard dataset (\ufb01rst three rows) and the accuracies of answer retrieval in % (last row). \u03c1: robustness of the silver-standard in %, \u03b3c/p: #/% of retrieved silver-standard passages (coverage).\n\nFig. 1 shows the distributions of seven question\ntypes grouped deterministically from the lexicons. Although these corpora have been independently\ndeveloped, a general trend is found, where thewhat\nquestion type dominates, followed byhow and who,\nfollowed by when and where, and so on. 0\n15\n30\n45\n60\nWhat How Who When Where Which Why\nWikiQA SelQA SQuAD InfoboxQA\nFigure 1: Distributions of question types in %. Fig. 2 shows the distributions of answer categories\nautomatically classi\ufb01ed by our Convolutional Neu-\nral Network model trained on the data distributed\nby Li and Roth (2002).2 Interestingly, each corpus\nfocuses on different categories, Numeric for WIK-\nIQA and SELQA, Entity for SQUAD, and Person\nfor INFOBOX QA, which gives enough diversities\nfor statistical learning to build robust models. Numeric\nPerson\nEntity\nDescription\nLocation\nAbbreviation\n0 10 20 30 40\nWikiQA SelQA SQuAD InfoboxQA\nFigure 2: Distributions of answer categories in %. 2Our CNN model shows 95.20% accuracy on their test set. 3 Answer Retrieval\nThis section describes another selection-based QA\ntask, called answer retrieval, that \ufb01nds the answer\ncontext from a larger dataset, the entire Wikipedia. SQUAD provides no mapping of the answer con-\ntexts to Wikipedia, whereas WIKI QA and SELQA\nprovide mappings; however, their data do not come\nfrom the same version of Wikipedia. We propose\nan automatic way of mapping the answer contexts\nfrom all corpora to the same version of Wikipeda3\nso they can be coherently used for answer retrieval. Each paragraph in Wikipedia is \ufb01rst indexed by\nLucene using {1,2,3}-grams, where the paragraphs\nare separated by WikiExtractor4 and segmented by\nNLP4J5 (28.7M+ paragraphs are indexed). Each\nanswer sentence from the corpora in Table 2 is then\nqueried to Lucene, and the top-5 ranked paragraphs\nare retrieved. The cosine similarity between each\nsentence in these paragraphs and the answer sen-\ntence is measured forn-grams, say n1,2,3. A weight\nis assigned to each n-gram score, say \u03bb1,2,3, and\nthe weighted sum is measured: t = \u22113\ni=1 \u03bbi \u00b7ni. The \ufb01xed weights of \u03bb1,2,3 = (0.25,0.35,0.4) are\nused for our experiments, which can be improved. If there exists a sentence whose t\u2265\u03b8, the para-\ngraph consisting of that sentence is considered the\nsilver-standard answer passage. Table 2 shows how\nrobust these silver-standard passages are based on\nhuman judgement (\u03c1) and how many passages are\ncollected (\u03b3) for \u03b8 = [0.3,0.5], where the human\njudgement is performed on 50 random samples for\neach case. For answer retrieval, a dataset is created\nby \u03b8 = 0.4, which gives \u03c1 \u226594% accuracy and\n\u03b3p > 90% coverage, respectively.6 Finally, each\nquestion is queried to Lucene and the top-kpara-\ngraphs are retrieved from the entire Wikipedia. If\nthe answer sentence exists within those retrieved\nparagraphs according to the silver-standard, it is\nconsidered correct. 3enwiki-20160820-pages-articles.xml.bz2\n4github.com/attardi/wikiextractor\n5github.com/emorynlp/nlp4j\n6SQUAD mapping was easier than the others because it was\nbased on a more recent version of Wikipedia. Trained on\nEvaluated on\nWIKI QA SELQA SQUAD INFOBOX QA\nMAP MRR F1 MAP MRR F1 MAP MRR F1 MAP MRR F1\nWIKI QA 65.54 67.41 13.33 53.47 54.12 8.68 73.16 73.72 11.26 30.85 30.85 -\nSELQA 49.05 49.64 24.30 82.72 83.70 48.66 77.22 78.04 44.70 63.13 63.13 -\nSQUAD 58.17 58.53 19.35 81.15 82.27 42.88 88.84 89.69 44.93 63.24 63.24 -\nINFOBOX QA 45.17 45.43 - 53.48 54.25 - 65.27 65.90 - 79.44 79.44 -\nW+S+Q 56.40 56.51 - 83.19 84.25 - 88.78 89.65 - 62.53 62.53 -\nW+S+Q+I 60.19 60.68 - 82.88 83.97 - 88.92 89.79 - 70.81 70.81 -\nTable 3: Results for answer selection and triggering in % trained and evaluated across all corpora splits. The \ufb01rst column shows\nthe training source, and the other columns show the evaluation sources. W: WIKI QA, S: SELQA, Q: SQUAD, I: INFOBOX QA. 4 Extrinsic Analysis\n4.1 Answer Selection\nAnswer selection is evaluated by two metrics, mean\naverage precision (MAP) and mean reciprocal rank\n(MRR). The bigram CNN introduced by Yu et al. (2014) is used to generate all the results in Table 3,\nwhere models are trained on either single or com-\nbined datasets. Clearly, the questions in WIKI QA\nare the most challenging, and adding more train-\ning data from the other corpora hurts accuracy due\nto the uniqueness of query-based questions in this\ncorpus. The best model is achieved by training on\nW+S+Q for SELQA; adding INFOBOX QA hurts\naccuracy for SELQA although it gives a marginal\ngain for SQUAD. Just like WIKI QA, INFOBOX QA\nperforms the best when it is trained on only itself. From our analysis, we suggest that to use models\ntrained on WIKI QA and INFOBOX QA for short\nquery-like questions, whereas to use ones trained\non SELQA and SQUAD for long natural questions. 4.2 Answer Retrieval\nFinding a paragraph that includes the answer con-\ntext out of the entire Wikipedia is an extremely\ndif\ufb01cult task ( 1/28.7M). The last row of Table 2\nshows results from answer retrieval. Given k= 5,\nSELQA and SQUAD show about 34% and 35% ac-\ncuracy, which are reasonable. However, WIKI QA\nshows a signi\ufb01cantly lower accuracy of 12.47%;\nthis is because the questions in WIKI QA is about\ntwice shorter than the questions in the other cor-\npora such that not enough lexicons can be extracted\nfrom these questions for the Lucene search. 4.3 Answer Triggering\nThe results of k= 5 from the answer retrieval task\nin Section 4.2 are used to create the datasets for an-\nswer triggering, where about 65% of the questions\nare not expected to \ufb01nd their answer contexts from\nthe provided paragraphs for SELQA and SQUAD\nand 87.5% are not expected for WIKI QA. Answer\ntriggering is evaluated by the F1 scores as presented\nin Table 3, where three corpora are cross validated. The results on WIKI QA are pretty low as expected\nfrom the poor accuracy on the answer retrieval task. Training on SELQA gives the best models for both\nWIKI QA and SELQA. Training on SQUAD gives\nthe best model for SQUAD although the model\ntrained on SELQA is comparable. Since the answer\ntriggering datasets are about 5 times larger than the\nanswer selection datasets, it is computationally too\nexpensive to combine all data for training. We plan\nto \ufb01nd a strong machine to perform this experiment\nin near future. 5 Related work\nLately, several deep learning approaches have been\nproposed for question answering. Yu et al. (2014)\npresented a CNN model that recognizes the seman-\ntic similarity between two sentences. Wang and\nNyberg (2015) presented a stacked bidirectional\nLSTM approach to read words in sequence, then\noutputs their similarity scores. Feng et al. (2015)\napplied a general deep learning framework to non-\nfactoid question answering. Santos et al. (2016)\nintroduced an attentive pooling mechanism that led\nto further improvements in selection-based QA. 6 Conclusion\nWe present a comprehensive comparison study of\nthe existing corpora for selection-based question\nanswering. Our intrinsic analysis provides a better\nunderstanding of the uniqueness or similarity be-\ntween these corpora. Our extrinsic analysis shows\nthe strength or weakness of combining these cor-\npora together for statistical learning. Additionally,\nwe create a silver-standard dataset for answer re-\ntrieval and triggering, which will be publicly avail-\nable. In the future, we will explore different ways\nof improving the quality of our silver-standard\ndatasets by \ufb01ne-tuning the hyper-parameters. References\nMinwei Feng, Bing Xiang, Michael R. Glass, Lidan\nWang, and Bowen Zhou.",
            0.5133924579999984
          ]
        },
        {
          "answer": "Seven question types",
          "relevant_chunks": [
            "Analysis of Wikipedia-based Corpora for Question Answering\nTomasz Jurczyk1, Amit Deshmane2, Jinho D. Choi1\n1Mathematics and Computer Science, Emory University\n2Infosys Ltd. {tomasz.jurczyk,jinho.choi}@emory.edu\namit deshmane@infosys.com\nAbstract\nThis paper gives comprehensive analyses\nof corpora based on Wikipedia for several\ntasks in question answering. Four recent\ncorpora are collected, WIKI QA, SELQA,\nSQUAD, and INFOBOX QA, and \ufb01rst an-\nalyzed intrinsically by contextual similari-\nties, question types, and answer categories. These corpora are then analyzed extrinsi-\ncally by three question answering tasks, an-\nswer retrieval, selection, and triggering. An\nindexing-based method for the creation of a\nsilver-standard dataset for answer retrieval\nusing the entire Wikipedia is also presented. Our analysis shows the uniqueness of these\ncorpora and suggests a better use of them\nfor statistical question answering learning. 1 Introduction\nQuestion answering (QA) has been a blooming re-\nsearch \ufb01eld for the last decade. Selection-based QA\nimplies a family of tasks that \ufb01nd answer contexts\nfrom large data given questions in natural language. Three tasks have been proposed for selection-based\nQA. Given a document, answer extraction (Shen\nand Klakow, 2006; Sultan et al., 2016) \ufb01nds answer\nphrases whereas answer selection (Wang et al.,\n2007; Yih et al., 2013; Yu et al., 2014; Wang et al.,\n2016) and answer triggering (Yang et al., 2015;\nJurczyk et al., 2016) \ufb01nd answer sentences instead,\nalthough the presence of the answer context is not\nassumed within the provided document for answer\ntriggering but it is for the other two tasks. Recently,\nvarious QA tasks that are not selection-based have\nbeen proposed (Reddy and Bandyopadhyay, 2006;\nHosseini et al., 2014; Jauhar et al., 2016; Sachan\net al., 2016); however, selection-based QA remains\nstill important because of its practical value to real\napplications (e.g., IBM Watson, MIT START). Several datasets have been released for selection-\nbased QA. Wang et al. (2007) created theQAS ENT\ndataset consisting of 277 questions, which has been\nwidely used for benchmarking the answer selection\ntask. Feng et al. (2015) presented INSURANCE QA\ncomprising 16K+ questions on insurance contexts. Yang et al. (2015) introduced WIKI QA for answer\nselection and triggering. Jurczyk et al. (2016) cre-\nated SELQA for large real-scale answer triggering. Rajpurkar et al. (2016) presented SQUAD for an-\nswer extraction and selection as well as for reading\ncomprehension. Finally, Morales et al. (2016) pro-\nvided INFOBOX QA for answer selection. These corpora make it possible to evaluate the ro-\nbustness of statistical question answering learning. Although all of these corpora target on selection-\nbased QA, they are designed for different purposes\nsuch that it is important to understand the nature of\nthese corpora so a better use of them can be made. In this paper, we make both intrinsic and extrinsic\nanalyses of four latest corpora based on Wikipedia,\nWIKI QA, SELQA, SQUAD, and INFOBOX QA. We \ufb01rst give a thorough intrinsic analysis regarding\ncontextual similarities, question types, and answer\ncategories (Section 2). We then map questions in all\ncorpora to the current version of English Wikipedia\nand benchmark another selection-based QA task,\nanswer retrieval (Section 3). Finally, we present\nan extrinsic analysis through a set of experiments\ncross-testing these corpora using a convolutional\nneural network architecture (Section 4).1\n2 Intrinsic Analysis\nFour publicly available corpora are selected for our\nanalysis. These corpora are based on Wikipedia, so\nmore comparable than the others, and have already\nbeen used for the evaluation of several QA systems. 1All our resources are publicly available\n: anonymous_url\narXiv:1801.02073v2  [cs.CL]  5 Feb 2018\n\nWIKIQA SELQA SQUAD INFOBOX QA\nSource Bing search queries Crowdsourced Crowdsourced Crowdsourced\nYear 2015 2016 2016 2016\n(AE, AS, AT) (O, O, O) (X, O, O) (O, O, X) (X, O, X)\n(q,c,c/q) (1,242, 12,153, 9.79) (7,904, 95,250, 12.05) (98,202, 496,167, 5.05) (15,271, 271,038, 17.75)\n(w,t) (386,440, 30,191) (3,469,015, 44,099) (19,445,863, 115,092) (5,034,625, 8,323)\n(\u00b5q,\u00b5c) (6.44, 25.36) (11.11, 25.31) (11.33, 27.86) (9.35, 9.22)\n(\u2126q,\u2126a,\u2126f) (46.72, 11.05, 16.96) (32.79, 16.98, 20.19) (32.27, 12.15, 16.54) (26.80, 35.70, 28.09)\nTable 1: Comparisons between the four corpora for answer selection. Note that both WIKI QA and SELQA provide separate\nannotation for answer triggering, which is not shown in this table. The SQUAD column shows statistics excluding the evaluation\nset, which is not publicly available. AE/AS/AT: annotation for answer extraction/selection/triggering, q/c: # of questions/answer\ncandidates, w/t: # of tokens/token types, \u00b5q/c: average length of questions/answer candidates, \u2126q/a: macro average in % of\noverlapping words between question-answer pairs normalized by the questions/answers lengths, \u2126f : (2\u00b7\u2126q\u00b7\u2126a)/(\u2126q+\u2126a). WIKI QA (Yang et al., 2015) comprises questions\nselected from the Bing search queries, where user\nclick data give the questions and their correspond-\ning Wikipedia articles. The abstracts of these arti-\ncles are then extracted to create answer candidates. The assumption is made that if many queries lead to\nthe same article, it must contain the answer context;\nhowever, this assumption fails for some occasions,\nwhich makes this dataset more challenging. Since\nthe existence of answer contexts is not guaranteed\nin this task, it is called answer triggering instead of\nanswer selection. SELQA (Jurczyk et al., 2016) is a product of \ufb01ve\nannotation tasks through crowdsourcing. It consists\nof about 8K questions where a half of the questions\nare paraphrased from the other half, aiming to re-\nduce contextual similarities between questions and\nanswers. Each question is associated with a section\nin Wikipedia where the answer context is guaran-\nteed, and also with \ufb01ve sections selected from the\nentire Wikipedia where the selection is made by\nthe Lucene search engine. This second dataset does\nnot assume the existence of the answer context, so\ncan be used for the evaluation of answer triggering. SQUAD (Rajpurkar et al., 2016) presents 107K+\ncrowdsourced questions on 536 Wikipedia articles,\nwhere the answer contexts are guaranteed to exist\nwithin the provided paragraph. It contains annota-\ntion of answer phrases as well as the pointers to the\nsentences including the answer phrases; thus, it can\nbe used for both answer extraction and selection. This corpus also provides human accuracy on those\nquestions, setting up a reasonable upper bound for\nmachines. To avoid over\ufb01tting, the evaluation set\nis not publicly available although system outputs\ncan be evaluated by their provided script. INFOBOX QA (Morales et al., 2016) gives 15K+\nquestions based on the infoboxes from 150 articles\nin Wikipedia. Each question is crowdsourced and\nassociated with an infobox, where each line of the\ninfobox is considered an answer candidate. This\ncorpus emphasizes the gravity of infoboxes, which\nsummary arguably the most commonly asked infor-\nmation about those articles. Although the nature of\nthis corpus is different from the others, it can also\nbe used to evaluate answer selection. Analysis\nAll corpora provide datasets/splits for answer se-\nlection, whereas only ( WIKI QA, SQUAD) and\n(WIKI QA, SELQA) provide datasets for answer\nextraction and answer triggering, respectively. SQUAD is much larger in size although ques-\ntions in this corpus are often paraphrased multi-\nple times. On the contrary, SQUAD\u2019s average\ncandidates per question ( c/q) is the smallest be-\ncause SQUAD extracts answer candidates from\nparagraphs whereas the others extract them from\nsections or infoboxes that consist of bigger con-\ntexts. Although INFOBOX QA is larger than WIK-\nIQA or SELQA, the number of token types (t) in\nINFOBOX QA is smaller than those two, due to the\nrepetitive nature of infoboxes. All corpora show similar average answer candi-\ndate lengths (\u00b5c), except for INFOBOX QA where\neach line in the infobox is considered a candidate. SELQA and SQUAD show similar average ques-\ntion lengths (\u00b5q) because of the similarity between\ntheir annotation schemes. It is not surprising that\nWIKI QA\u2019s average question length is the smallest,\nconsidering their questions are taken from search\nqueries. INFOBOX QA\u2019s average question length is\nrelatively small, due to the restricted information\nthat can be asked from the infoboxes.INFOBOX QA\nand WIKI QA show the least question-answer word\noverlaps over questions and answers (\u2126q and \u2126a in\nTable 1), respectively. In terms of the F1-score for\noverlapping words (\u2126f ), SQUAD gives the least\nportion of overlaps between question-answer pairs\n\nWIKI QA SELQA SQUAD\n(\u03c1,\u03b3c,\u03b3p),t \u2265 0.3 ( 92 .00, 1,203, 96.86) (90.00, 7,446, 94.28) (100.00, 93,928, 95.61)\n(\u03c1,\u03b3c,\u03b3p),t \u2265 0.4 ( 94.00, 1,139, 91.71) (94.00, 7,133, 90.31) (100.00, 93,928, 95.61)\n(\u03c1,\u03b3c,\u03b3p),t \u2265 0.5 (100.00, 1,051, 84.62) (98.00, 6,870, 86.98) (100.00, 93,928, 95.61)\nk= (1,5,10,20) (4.39,12.47,16.59,22.39) (20.01,34.07,40.29,46.40) (19.90,35.08,40.96,46.74)\nTable 2: Statistics of the silver-standard dataset (\ufb01rst three rows) and the accuracies of answer retrieval in % (last row). \u03c1: robustness of the silver-standard in %, \u03b3c/p: #/% of retrieved silver-standard passages (coverage).\n\nFig. 1 shows the distributions of seven question\ntypes grouped deterministically from the lexicons. Although these corpora have been independently\ndeveloped, a general trend is found, where thewhat\nquestion type dominates, followed byhow and who,\nfollowed by when and where, and so on. 0\n15\n30\n45\n60\nWhat How Who When Where Which Why\nWikiQA SelQA SQuAD InfoboxQA\nFigure 1: Distributions of question types in %. Fig. 2 shows the distributions of answer categories\nautomatically classi\ufb01ed by our Convolutional Neu-\nral Network model trained on the data distributed\nby Li and Roth (2002).2 Interestingly, each corpus\nfocuses on different categories, Numeric for WIK-\nIQA and SELQA, Entity for SQUAD, and Person\nfor INFOBOX QA, which gives enough diversities\nfor statistical learning to build robust models. Numeric\nPerson\nEntity\nDescription\nLocation\nAbbreviation\n0 10 20 30 40\nWikiQA SelQA SQuAD InfoboxQA\nFigure 2: Distributions of answer categories in %. 2Our CNN model shows 95.20% accuracy on their test set. 3 Answer Retrieval\nThis section describes another selection-based QA\ntask, called answer retrieval, that \ufb01nds the answer\ncontext from a larger dataset, the entire Wikipedia. SQUAD provides no mapping of the answer con-\ntexts to Wikipedia, whereas WIKI QA and SELQA\nprovide mappings; however, their data do not come\nfrom the same version of Wikipedia. We propose\nan automatic way of mapping the answer contexts\nfrom all corpora to the same version of Wikipeda3\nso they can be coherently used for answer retrieval. Each paragraph in Wikipedia is \ufb01rst indexed by\nLucene using {1,2,3}-grams, where the paragraphs\nare separated by WikiExtractor4 and segmented by\nNLP4J5 (28.7M+ paragraphs are indexed). Each\nanswer sentence from the corpora in Table 2 is then\nqueried to Lucene, and the top-5 ranked paragraphs\nare retrieved. The cosine similarity between each\nsentence in these paragraphs and the answer sen-\ntence is measured forn-grams, say n1,2,3. A weight\nis assigned to each n-gram score, say \u03bb1,2,3, and\nthe weighted sum is measured: t = \u22113\ni=1 \u03bbi \u00b7ni. The \ufb01xed weights of \u03bb1,2,3 = (0.25,0.35,0.4) are\nused for our experiments, which can be improved. If there exists a sentence whose t\u2265\u03b8, the para-\ngraph consisting of that sentence is considered the\nsilver-standard answer passage. Table 2 shows how\nrobust these silver-standard passages are based on\nhuman judgement (\u03c1) and how many passages are\ncollected (\u03b3) for \u03b8 = [0.3,0.5], where the human\njudgement is performed on 50 random samples for\neach case. For answer retrieval, a dataset is created\nby \u03b8 = 0.4, which gives \u03c1 \u226594% accuracy and\n\u03b3p > 90% coverage, respectively.6 Finally, each\nquestion is queried to Lucene and the top-kpara-\ngraphs are retrieved from the entire Wikipedia. If\nthe answer sentence exists within those retrieved\nparagraphs according to the silver-standard, it is\nconsidered correct. 3enwiki-20160820-pages-articles.xml.bz2\n4github.com/attardi/wikiextractor\n5github.com/emorynlp/nlp4j\n6SQUAD mapping was easier than the others because it was\nbased on a more recent version of Wikipedia. Trained on\nEvaluated on\nWIKI QA SELQA SQUAD INFOBOX QA\nMAP MRR F1 MAP MRR F1 MAP MRR F1 MAP MRR F1\nWIKI QA 65.54 67.41 13.33 53.47 54.12 8.68 73.16 73.72 11.26 30.85 30.85 -\nSELQA 49.05 49.64 24.30 82.72 83.70 48.66 77.22 78.04 44.70 63.13 63.13 -\nSQUAD 58.17 58.53 19.35 81.15 82.27 42.88 88.84 89.69 44.93 63.24 63.24 -\nINFOBOX QA 45.17 45.43 - 53.48 54.25 - 65.27 65.90 - 79.44 79.44 -\nW+S+Q 56.40 56.51 - 83.19 84.25 - 88.78 89.65 - 62.53 62.53 -\nW+S+Q+I 60.19 60.68 - 82.88 83.97 - 88.92 89.79 - 70.81 70.81 -\nTable 3: Results for answer selection and triggering in % trained and evaluated across all corpora splits. The \ufb01rst column shows\nthe training source, and the other columns show the evaluation sources. W: WIKI QA, S: SELQA, Q: SQUAD, I: INFOBOX QA. 4 Extrinsic Analysis\n4.1 Answer Selection\nAnswer selection is evaluated by two metrics, mean\naverage precision (MAP) and mean reciprocal rank\n(MRR). The bigram CNN introduced by Yu et al. (2014) is used to generate all the results in Table 3,\nwhere models are trained on either single or com-\nbined datasets. Clearly, the questions in WIKI QA\nare the most challenging, and adding more train-\ning data from the other corpora hurts accuracy due\nto the uniqueness of query-based questions in this\ncorpus. The best model is achieved by training on\nW+S+Q for SELQA; adding INFOBOX QA hurts\naccuracy for SELQA although it gives a marginal\ngain for SQUAD. Just like WIKI QA, INFOBOX QA\nperforms the best when it is trained on only itself. From our analysis, we suggest that to use models\ntrained on WIKI QA and INFOBOX QA for short\nquery-like questions, whereas to use ones trained\non SELQA and SQUAD for long natural questions. 4.2 Answer Retrieval\nFinding a paragraph that includes the answer con-\ntext out of the entire Wikipedia is an extremely\ndif\ufb01cult task ( 1/28.7M). The last row of Table 2\nshows results from answer retrieval. Given k= 5,\nSELQA and SQUAD show about 34% and 35% ac-\ncuracy, which are reasonable. However, WIKI QA\nshows a signi\ufb01cantly lower accuracy of 12.47%;\nthis is because the questions in WIKI QA is about\ntwice shorter than the questions in the other cor-\npora such that not enough lexicons can be extracted\nfrom these questions for the Lucene search. 4.3 Answer Triggering\nThe results of k= 5 from the answer retrieval task\nin Section 4.2 are used to create the datasets for an-\nswer triggering, where about 65% of the questions\nare not expected to \ufb01nd their answer contexts from\nthe provided paragraphs for SELQA and SQUAD\nand 87.5% are not expected for WIKI QA. Answer\ntriggering is evaluated by the F1 scores as presented\nin Table 3, where three corpora are cross validated. The results on WIKI QA are pretty low as expected\nfrom the poor accuracy on the answer retrieval task. Training on SELQA gives the best models for both\nWIKI QA and SELQA. Training on SQUAD gives\nthe best model for SQUAD although the model\ntrained on SELQA is comparable. Since the answer\ntriggering datasets are about 5 times larger than the\nanswer selection datasets, it is computationally too\nexpensive to combine all data for training. We plan\nto \ufb01nd a strong machine to perform this experiment\nin near future. 5 Related work\nLately, several deep learning approaches have been\nproposed for question answering. Yu et al. (2014)\npresented a CNN model that recognizes the seman-\ntic similarity between two sentences. Wang and\nNyberg (2015) presented a stacked bidirectional\nLSTM approach to read words in sequence, then\noutputs their similarity scores. Feng et al. (2015)\napplied a general deep learning framework to non-\nfactoid question answering. Santos et al. (2016)\nintroduced an attentive pooling mechanism that led\nto further improvements in selection-based QA. 6 Conclusion\nWe present a comprehensive comparison study of\nthe existing corpora for selection-based question\nanswering. Our intrinsic analysis provides a better\nunderstanding of the uniqueness or similarity be-\ntween these corpora. Our extrinsic analysis shows\nthe strength or weakness of combining these cor-\npora together for statistical learning. Additionally,\nwe create a silver-standard dataset for answer re-\ntrieval and triggering, which will be publicly avail-\nable. In the future, we will explore different ways\nof improving the quality of our silver-standard\ndatasets by \ufb01ne-tuning the hyper-parameters. References\nMinwei Feng, Bing Xiang, Michael R. Glass, Lidan\nWang, and Bowen Zhou.\n\nICTAI\u201916. Xin Li and Dan Roth. 2002. Learning Question Classi-\n\ufb01ers. In Proceedings of the 19th International Con-\nference on Computational Linguistics. COLING \u201902,\npages 1\u20137. Alvaro Morales, Varot Premtoon, Cordelia Avery, Sue\nFelshin, and Boris Katz. 2016. Learning to answer\nquestions from wikipedia infoboxes. In Proceedings\nof the 2016 Conference on Empirical Methods in\nNatural Language Processing. Association for Com-\nputational Linguistics, Austin, Texas, pages 1930\u2013\n1935. https://aclweb.org/anthology/D16-1199. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250 . Rami Reddy Nandi Reddy and Sivaji Bandyopadhyay.",
            0.6436314160000052
          ]
        },
        {
          "answer": "The context does not provide specific information on how they analyze contextual similarities across datasets.",
          "relevant_chunks": [
            "Fig. 1 shows the distributions of seven question\ntypes grouped deterministically from the lexicons. Although these corpora have been independently\ndeveloped, a general trend is found, where thewhat\nquestion type dominates, followed byhow and who,\nfollowed by when and where, and so on. 0\n15\n30\n45\n60\nWhat How Who When Where Which Why\nWikiQA SelQA SQuAD InfoboxQA\nFigure 1: Distributions of question types in %. Fig. 2 shows the distributions of answer categories\nautomatically classi\ufb01ed by our Convolutional Neu-\nral Network model trained on the data distributed\nby Li and Roth (2002).2 Interestingly, each corpus\nfocuses on different categories, Numeric for WIK-\nIQA and SELQA, Entity for SQUAD, and Person\nfor INFOBOX QA, which gives enough diversities\nfor statistical learning to build robust models. Numeric\nPerson\nEntity\nDescription\nLocation\nAbbreviation\n0 10 20 30 40\nWikiQA SelQA SQuAD InfoboxQA\nFigure 2: Distributions of answer categories in %. 2Our CNN model shows 95.20% accuracy on their test set. 3 Answer Retrieval\nThis section describes another selection-based QA\ntask, called answer retrieval, that \ufb01nds the answer\ncontext from a larger dataset, the entire Wikipedia. SQUAD provides no mapping of the answer con-\ntexts to Wikipedia, whereas WIKI QA and SELQA\nprovide mappings; however, their data do not come\nfrom the same version of Wikipedia. We propose\nan automatic way of mapping the answer contexts\nfrom all corpora to the same version of Wikipeda3\nso they can be coherently used for answer retrieval. Each paragraph in Wikipedia is \ufb01rst indexed by\nLucene using {1,2,3}-grams, where the paragraphs\nare separated by WikiExtractor4 and segmented by\nNLP4J5 (28.7M+ paragraphs are indexed). Each\nanswer sentence from the corpora in Table 2 is then\nqueried to Lucene, and the top-5 ranked paragraphs\nare retrieved. The cosine similarity between each\nsentence in these paragraphs and the answer sen-\ntence is measured forn-grams, say n1,2,3. A weight\nis assigned to each n-gram score, say \u03bb1,2,3, and\nthe weighted sum is measured: t = \u22113\ni=1 \u03bbi \u00b7ni. The \ufb01xed weights of \u03bb1,2,3 = (0.25,0.35,0.4) are\nused for our experiments, which can be improved. If there exists a sentence whose t\u2265\u03b8, the para-\ngraph consisting of that sentence is considered the\nsilver-standard answer passage. Table 2 shows how\nrobust these silver-standard passages are based on\nhuman judgement (\u03c1) and how many passages are\ncollected (\u03b3) for \u03b8 = [0.3,0.5], where the human\njudgement is performed on 50 random samples for\neach case. For answer retrieval, a dataset is created\nby \u03b8 = 0.4, which gives \u03c1 \u226594% accuracy and\n\u03b3p > 90% coverage, respectively.6 Finally, each\nquestion is queried to Lucene and the top-kpara-\ngraphs are retrieved from the entire Wikipedia. If\nthe answer sentence exists within those retrieved\nparagraphs according to the silver-standard, it is\nconsidered correct. 3enwiki-20160820-pages-articles.xml.bz2\n4github.com/attardi/wikiextractor\n5github.com/emorynlp/nlp4j\n6SQUAD mapping was easier than the others because it was\nbased on a more recent version of Wikipedia. Trained on\nEvaluated on\nWIKI QA SELQA SQUAD INFOBOX QA\nMAP MRR F1 MAP MRR F1 MAP MRR F1 MAP MRR F1\nWIKI QA 65.54 67.41 13.33 53.47 54.12 8.68 73.16 73.72 11.26 30.85 30.85 -\nSELQA 49.05 49.64 24.30 82.72 83.70 48.66 77.22 78.04 44.70 63.13 63.13 -\nSQUAD 58.17 58.53 19.35 81.15 82.27 42.88 88.84 89.69 44.93 63.24 63.24 -\nINFOBOX QA 45.17 45.43 - 53.48 54.25 - 65.27 65.90 - 79.44 79.44 -\nW+S+Q 56.40 56.51 - 83.19 84.25 - 88.78 89.65 - 62.53 62.53 -\nW+S+Q+I 60.19 60.68 - 82.88 83.97 - 88.92 89.79 - 70.81 70.81 -\nTable 3: Results for answer selection and triggering in % trained and evaluated across all corpora splits. The \ufb01rst column shows\nthe training source, and the other columns show the evaluation sources. W: WIKI QA, S: SELQA, Q: SQUAD, I: INFOBOX QA. 4 Extrinsic Analysis\n4.1 Answer Selection\nAnswer selection is evaluated by two metrics, mean\naverage precision (MAP) and mean reciprocal rank\n(MRR). The bigram CNN introduced by Yu et al. (2014) is used to generate all the results in Table 3,\nwhere models are trained on either single or com-\nbined datasets. Clearly, the questions in WIKI QA\nare the most challenging, and adding more train-\ning data from the other corpora hurts accuracy due\nto the uniqueness of query-based questions in this\ncorpus. The best model is achieved by training on\nW+S+Q for SELQA; adding INFOBOX QA hurts\naccuracy for SELQA although it gives a marginal\ngain for SQUAD. Just like WIKI QA, INFOBOX QA\nperforms the best when it is trained on only itself. From our analysis, we suggest that to use models\ntrained on WIKI QA and INFOBOX QA for short\nquery-like questions, whereas to use ones trained\non SELQA and SQUAD for long natural questions. 4.2 Answer Retrieval\nFinding a paragraph that includes the answer con-\ntext out of the entire Wikipedia is an extremely\ndif\ufb01cult task ( 1/28.7M). The last row of Table 2\nshows results from answer retrieval. Given k= 5,\nSELQA and SQUAD show about 34% and 35% ac-\ncuracy, which are reasonable. However, WIKI QA\nshows a signi\ufb01cantly lower accuracy of 12.47%;\nthis is because the questions in WIKI QA is about\ntwice shorter than the questions in the other cor-\npora such that not enough lexicons can be extracted\nfrom these questions for the Lucene search. 4.3 Answer Triggering\nThe results of k= 5 from the answer retrieval task\nin Section 4.2 are used to create the datasets for an-\nswer triggering, where about 65% of the questions\nare not expected to \ufb01nd their answer contexts from\nthe provided paragraphs for SELQA and SQUAD\nand 87.5% are not expected for WIKI QA. Answer\ntriggering is evaluated by the F1 scores as presented\nin Table 3, where three corpora are cross validated. The results on WIKI QA are pretty low as expected\nfrom the poor accuracy on the answer retrieval task. Training on SELQA gives the best models for both\nWIKI QA and SELQA. Training on SQUAD gives\nthe best model for SQUAD although the model\ntrained on SELQA is comparable. Since the answer\ntriggering datasets are about 5 times larger than the\nanswer selection datasets, it is computationally too\nexpensive to combine all data for training. We plan\nto \ufb01nd a strong machine to perform this experiment\nin near future. 5 Related work\nLately, several deep learning approaches have been\nproposed for question answering. Yu et al. (2014)\npresented a CNN model that recognizes the seman-\ntic similarity between two sentences. Wang and\nNyberg (2015) presented a stacked bidirectional\nLSTM approach to read words in sequence, then\noutputs their similarity scores. Feng et al. (2015)\napplied a general deep learning framework to non-\nfactoid question answering. Santos et al. (2016)\nintroduced an attentive pooling mechanism that led\nto further improvements in selection-based QA. 6 Conclusion\nWe present a comprehensive comparison study of\nthe existing corpora for selection-based question\nanswering. Our intrinsic analysis provides a better\nunderstanding of the uniqueness or similarity be-\ntween these corpora. Our extrinsic analysis shows\nthe strength or weakness of combining these cor-\npora together for statistical learning. Additionally,\nwe create a silver-standard dataset for answer re-\ntrieval and triggering, which will be publicly avail-\nable. In the future, we will explore different ways\nof improving the quality of our silver-standard\ndatasets by \ufb01ne-tuning the hyper-parameters. References\nMinwei Feng, Bing Xiang, Michael R. Glass, Lidan\nWang, and Bowen Zhou.\n\nAnalysis of Wikipedia-based Corpora for Question Answering\nTomasz Jurczyk1, Amit Deshmane2, Jinho D. Choi1\n1Mathematics and Computer Science, Emory University\n2Infosys Ltd. {tomasz.jurczyk,jinho.choi}@emory.edu\namit deshmane@infosys.com\nAbstract\nThis paper gives comprehensive analyses\nof corpora based on Wikipedia for several\ntasks in question answering. Four recent\ncorpora are collected, WIKI QA, SELQA,\nSQUAD, and INFOBOX QA, and \ufb01rst an-\nalyzed intrinsically by contextual similari-\nties, question types, and answer categories. These corpora are then analyzed extrinsi-\ncally by three question answering tasks, an-\nswer retrieval, selection, and triggering. An\nindexing-based method for the creation of a\nsilver-standard dataset for answer retrieval\nusing the entire Wikipedia is also presented. Our analysis shows the uniqueness of these\ncorpora and suggests a better use of them\nfor statistical question answering learning. 1 Introduction\nQuestion answering (QA) has been a blooming re-\nsearch \ufb01eld for the last decade. Selection-based QA\nimplies a family of tasks that \ufb01nd answer contexts\nfrom large data given questions in natural language. Three tasks have been proposed for selection-based\nQA. Given a document, answer extraction (Shen\nand Klakow, 2006; Sultan et al., 2016) \ufb01nds answer\nphrases whereas answer selection (Wang et al.,\n2007; Yih et al., 2013; Yu et al., 2014; Wang et al.,\n2016) and answer triggering (Yang et al., 2015;\nJurczyk et al., 2016) \ufb01nd answer sentences instead,\nalthough the presence of the answer context is not\nassumed within the provided document for answer\ntriggering but it is for the other two tasks. Recently,\nvarious QA tasks that are not selection-based have\nbeen proposed (Reddy and Bandyopadhyay, 2006;\nHosseini et al., 2014; Jauhar et al., 2016; Sachan\net al., 2016); however, selection-based QA remains\nstill important because of its practical value to real\napplications (e.g., IBM Watson, MIT START). Several datasets have been released for selection-\nbased QA. Wang et al. (2007) created theQAS ENT\ndataset consisting of 277 questions, which has been\nwidely used for benchmarking the answer selection\ntask. Feng et al. (2015) presented INSURANCE QA\ncomprising 16K+ questions on insurance contexts. Yang et al. (2015) introduced WIKI QA for answer\nselection and triggering. Jurczyk et al. (2016) cre-\nated SELQA for large real-scale answer triggering. Rajpurkar et al. (2016) presented SQUAD for an-\nswer extraction and selection as well as for reading\ncomprehension. Finally, Morales et al. (2016) pro-\nvided INFOBOX QA for answer selection. These corpora make it possible to evaluate the ro-\nbustness of statistical question answering learning. Although all of these corpora target on selection-\nbased QA, they are designed for different purposes\nsuch that it is important to understand the nature of\nthese corpora so a better use of them can be made. In this paper, we make both intrinsic and extrinsic\nanalyses of four latest corpora based on Wikipedia,\nWIKI QA, SELQA, SQUAD, and INFOBOX QA. We \ufb01rst give a thorough intrinsic analysis regarding\ncontextual similarities, question types, and answer\ncategories (Section 2). We then map questions in all\ncorpora to the current version of English Wikipedia\nand benchmark another selection-based QA task,\nanswer retrieval (Section 3). Finally, we present\nan extrinsic analysis through a set of experiments\ncross-testing these corpora using a convolutional\nneural network architecture (Section 4).1\n2 Intrinsic Analysis\nFour publicly available corpora are selected for our\nanalysis. These corpora are based on Wikipedia, so\nmore comparable than the others, and have already\nbeen used for the evaluation of several QA systems. 1All our resources are publicly available\n: anonymous_url\narXiv:1801.02073v2  [cs.CL]  5 Feb 2018\n\nWIKIQA SELQA SQUAD INFOBOX QA\nSource Bing search queries Crowdsourced Crowdsourced Crowdsourced\nYear 2015 2016 2016 2016\n(AE, AS, AT) (O, O, O) (X, O, O) (O, O, X) (X, O, X)\n(q,c,c/q) (1,242, 12,153, 9.79) (7,904, 95,250, 12.05) (98,202, 496,167, 5.05) (15,271, 271,038, 17.75)\n(w,t) (386,440, 30,191) (3,469,015, 44,099) (19,445,863, 115,092) (5,034,625, 8,323)\n(\u00b5q,\u00b5c) (6.44, 25.36) (11.11, 25.31) (11.33, 27.86) (9.35, 9.22)\n(\u2126q,\u2126a,\u2126f) (46.72, 11.05, 16.96) (32.79, 16.98, 20.19) (32.27, 12.15, 16.54) (26.80, 35.70, 28.09)\nTable 1: Comparisons between the four corpora for answer selection. Note that both WIKI QA and SELQA provide separate\nannotation for answer triggering, which is not shown in this table. The SQUAD column shows statistics excluding the evaluation\nset, which is not publicly available. AE/AS/AT: annotation for answer extraction/selection/triggering, q/c: # of questions/answer\ncandidates, w/t: # of tokens/token types, \u00b5q/c: average length of questions/answer candidates, \u2126q/a: macro average in % of\noverlapping words between question-answer pairs normalized by the questions/answers lengths, \u2126f : (2\u00b7\u2126q\u00b7\u2126a)/(\u2126q+\u2126a). WIKI QA (Yang et al., 2015) comprises questions\nselected from the Bing search queries, where user\nclick data give the questions and their correspond-\ning Wikipedia articles. The abstracts of these arti-\ncles are then extracted to create answer candidates. The assumption is made that if many queries lead to\nthe same article, it must contain the answer context;\nhowever, this assumption fails for some occasions,\nwhich makes this dataset more challenging. Since\nthe existence of answer contexts is not guaranteed\nin this task, it is called answer triggering instead of\nanswer selection. SELQA (Jurczyk et al., 2016) is a product of \ufb01ve\nannotation tasks through crowdsourcing. It consists\nof about 8K questions where a half of the questions\nare paraphrased from the other half, aiming to re-\nduce contextual similarities between questions and\nanswers. Each question is associated with a section\nin Wikipedia where the answer context is guaran-\nteed, and also with \ufb01ve sections selected from the\nentire Wikipedia where the selection is made by\nthe Lucene search engine. This second dataset does\nnot assume the existence of the answer context, so\ncan be used for the evaluation of answer triggering. SQUAD (Rajpurkar et al., 2016) presents 107K+\ncrowdsourced questions on 536 Wikipedia articles,\nwhere the answer contexts are guaranteed to exist\nwithin the provided paragraph. It contains annota-\ntion of answer phrases as well as the pointers to the\nsentences including the answer phrases; thus, it can\nbe used for both answer extraction and selection. This corpus also provides human accuracy on those\nquestions, setting up a reasonable upper bound for\nmachines. To avoid over\ufb01tting, the evaluation set\nis not publicly available although system outputs\ncan be evaluated by their provided script. INFOBOX QA (Morales et al., 2016) gives 15K+\nquestions based on the infoboxes from 150 articles\nin Wikipedia. Each question is crowdsourced and\nassociated with an infobox, where each line of the\ninfobox is considered an answer candidate. This\ncorpus emphasizes the gravity of infoboxes, which\nsummary arguably the most commonly asked infor-\nmation about those articles. Although the nature of\nthis corpus is different from the others, it can also\nbe used to evaluate answer selection. Analysis\nAll corpora provide datasets/splits for answer se-\nlection, whereas only ( WIKI QA, SQUAD) and\n(WIKI QA, SELQA) provide datasets for answer\nextraction and answer triggering, respectively. SQUAD is much larger in size although ques-\ntions in this corpus are often paraphrased multi-\nple times. On the contrary, SQUAD\u2019s average\ncandidates per question ( c/q) is the smallest be-\ncause SQUAD extracts answer candidates from\nparagraphs whereas the others extract them from\nsections or infoboxes that consist of bigger con-\ntexts. Although INFOBOX QA is larger than WIK-\nIQA or SELQA, the number of token types (t) in\nINFOBOX QA is smaller than those two, due to the\nrepetitive nature of infoboxes. All corpora show similar average answer candi-\ndate lengths (\u00b5c), except for INFOBOX QA where\neach line in the infobox is considered a candidate. SELQA and SQUAD show similar average ques-\ntion lengths (\u00b5q) because of the similarity between\ntheir annotation schemes. It is not surprising that\nWIKI QA\u2019s average question length is the smallest,\nconsidering their questions are taken from search\nqueries. INFOBOX QA\u2019s average question length is\nrelatively small, due to the restricted information\nthat can be asked from the infoboxes.INFOBOX QA\nand WIKI QA show the least question-answer word\noverlaps over questions and answers (\u2126q and \u2126a in\nTable 1), respectively. In terms of the F1-score for\noverlapping words (\u2126f ), SQUAD gives the least\nportion of overlaps between question-answer pairs\n\nWIKI QA SELQA SQUAD\n(\u03c1,\u03b3c,\u03b3p),t \u2265 0.3 ( 92 .00, 1,203, 96.86) (90.00, 7,446, 94.28) (100.00, 93,928, 95.61)\n(\u03c1,\u03b3c,\u03b3p),t \u2265 0.4 ( 94.00, 1,139, 91.71) (94.00, 7,133, 90.31) (100.00, 93,928, 95.61)\n(\u03c1,\u03b3c,\u03b3p),t \u2265 0.5 (100.00, 1,051, 84.62) (98.00, 6,870, 86.98) (100.00, 93,928, 95.61)\nk= (1,5,10,20) (4.39,12.47,16.59,22.39) (20.01,34.07,40.29,46.40) (19.90,35.08,40.96,46.74)\nTable 2: Statistics of the silver-standard dataset (\ufb01rst three rows) and the accuracies of answer retrieval in % (last row). \u03c1: robustness of the silver-standard in %, \u03b3c/p: #/% of retrieved silver-standard passages (coverage).\n\n2016. Science question answering using instructional ma-\nterials. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers). Association for Computa-\ntional Linguistics, Berlin, Germany, pages 467\u2013473. http://anthology.aclweb.org/P16-2076. C\u00b4\u0131cero Nogueira dos Santos, Ming Tan, Bing\nXiang, and Bowen Zhou. 2016. Attentive\npooling networks. CoRR abs/1602.03609. http://arxiv.org/abs/1602.03609. Dan Shen and Dietrich Klakow. 2006. Exploring cor-\nrelation of dependency relation paths for answer ex-\ntraction. In Proceedings of the 21st International\nConference on Computational Linguistics and the\n44th annual meeting of the Association for Compu-\ntational Linguistics. Association for Computational\nLinguistics, pages 889\u2013896. Md Arafat Sultan, Vittorio Castelli, and Radu Florian. 2016. A joint model for answer sentence ranking\nand answer extraction. Transactions of the Associa-\ntion for Computational Linguistics 4:113\u2013125. Di Wang and Eric Nyberg.",
            0.5438355419999823
          ]
        }
      ],
      "reference_answers": [
        "Yes No",
        "seven  7",
        "They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap."
      ]
    },
    "1707.00995.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.2694697996658622,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.4666666666666667,
          "recall": 0.6785714285714286,
          "f1": 0.4700485698866702
        },
        "rouge-2": {
          "precision": 0.38374125874125875,
          "recall": 0.5476190476190476,
          "f1": 0.37733644859813087
        },
        "rouge-L": {
          "precision": 0.437037037037037,
          "recall": 0.5993303571428572,
          "f1": 0.43146249325418234
        }
      },
      "per_question_metrics": [
        {
          "question": "What misbehavior is identified?",
          "generated_answer": "The misbehavior identified is that if the attention mechanism loses track of the objects in the picture and \"gets lost,\" the model still takes the image into account and somehow overrides the information brought by the text-based annotations, leading to a completely misled translation.",
          "reference_answer": "if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.7111111111111111,
              "recall": 0.5,
              "f1": 0.5871559633027522
            },
            "rouge-2": {
              "precision": 0.6136363636363636,
              "recall": 0.42857142857142855,
              "f1": 0.5046728971962617
            },
            "rouge-L": {
              "precision": 0.6888888888888889,
              "recall": 0.484375,
              "f1": 0.5688073394495412
            }
          },
          "bleu_score": 0.35011522088201674,
          "exact_match": 0.0
        },
        {
          "question": "Which attention mechanisms do they compare?",
          "generated_answer": "The text compares three models of the image attention mechanism: soft attention, hard stochastic attention, and another model that is not explicitly named within the given context.",
          "reference_answer": "Soft attention, Hard Stochastic attention, Local Attention",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.2222222222222222,
              "recall": 0.8571428571428571,
              "f1": 0.35294117647058826
            },
            "rouge-2": {
              "precision": 0.15384615384615385,
              "recall": 0.6666666666666666,
              "f1": 0.25
            },
            "rouge-L": {
              "precision": 0.18518518518518517,
              "recall": 0.7142857142857143,
              "f1": 0.2941176470588235
            }
          },
          "bleu_score": 0.18882437844970767,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "What misbehavior is identified?",
        "Which attention mechanisms do they compare?"
      ],
      "generated_answers": [
        {
          "answer": "The misbehavior identified is that if the attention mechanism loses track of the objects in the picture and \"gets lost,\" the model still takes the image into account and somehow overrides the information brought by the text-based annotations, leading to a completely misled translation.",
          "relevant_chunks": [
            "Figure 1: Die beiden Kinder spielen auf dem\nSpielplatz . Figure 2: Ein Junge sitzt auf und blickt aus einem\nMikroskop . 3.2 Hard Stochastic attention\nThis model is a stochastic and sampling-based\nprocess where, at every timestep t, we are making\na hard choice to attend only one annotation. This\ncorresponds to one spatial location in the image. Hard attention has previously been used in the\ncontext of object recognition (Mnih et al., 2014;\nBa et al., 2015) and later extended to image\ndescription generation (Xu et al., 2015). In the\ncontext of multimodal NMT, we can follow Xu\net al.\n\nFigure 5: Ein M \u00a8adchen mit einer Schwimmweste\nschwimmt im Wasser . Figure 6: Ein kleiner schwarzer Hund springt \u00a8uber\nHindernisse . 4 Image attention optimization\nThree optimizations can be added to the attention\nmechanism regarding the image modality. All\nlead to a better use of the image by the model and\nimproved the translation scores overall. At every decoding step t, we compute a gat-\ning scalar \u03b2t \u2208 [0,1] according to the previous\ndecoder state st\u22121:\n\u03b2t = \u03c3(W\u03b2st\u22121 + b\u03b2) (24)\nIt is then used to compute the time-dependent im-\nage context vector :\nit = \u03b2t\nL\u2211\nl=1\n\u03b1t,lal (25)\nXu et al. (2015) empirically found it to put more\nemphasis on the objects in the image descriptions\ngenerated with their model. We also double the output size of trainable\nparameters Ua, Wa and vT in equation 12 when\nit comes to compute the expected annotations\nover the image annotation sequence. More\n\nformally, given the image annotation sequence\nI = (a1,a2,..., aL),ai \u2208RD, the tree matrices\nare of size D \u00d72D, D \u00d72D and 2D \u00d71\nrespectively. We noticed a better coverage of the\nobjects in the image by the alpha weights. Lastly, we use a grounding attention inspired\nby Delbrouck and Dupont (2017). The mech-\nanism merge each spatial location ai in the\nannotation sequence I with the initial decoder\nstate s0 obtained in equation 5 with non-linearity\n:\nI\u2032=(f(a1 + s0),f(a2 + s0),...,f (aL + s0))\n(26)\nwhere f is tanh function. The new annota-\ntions go through a L2 normalization layer fol-\nlowed by two 1 \u00d71 convolutional layers (of size\nD \u2192512,512 \u21921 respectively) to obtain L\u00d71\nweights, one for each spatial location. We nor-\nmalize the weights with a softmax to obtain a\nsoft attention map \u03b1. Each annotation ai is then\nweighted according to its corresponding \u03b1i:\nI =(\u03b11a1,\u03b12a2,..., \u03b1LaL) (27)\nThis method can be seen as the removal of unnec-\nessary information in the image annotations ac-\ncording to the source sentence. This attention is\nused on top of the others - before decoding - and\nis referred as \u201dgrounded image\u201d in Table 1. 5 Experiments\nFor this experiments on Multimodal Machine\nTranslation, we used the Multi30K dataset (Elliott\net al., 2016) which is an extended version of the\nFlickr30K Entities. For each image, one of the\nEnglish descriptions was selected and manually\ntranslated into German by a professional transla-\ntor. As training and development data, 29,000 and\n1,014 triples are used respectively. A test set of\nsize 1000 is used for metrics evaluation. 5.1 Training and model details\nAll our models are build on top of the nematus\nframework (Sennrich et al., 2017). The encoder\nis a bidirectional RNN with GRU, one 1024D\nsingle-layer forward and one 1024D single-layer\nbackward RNN. Word embeddings for source and\ntarget language are of 620D and trained jointly\nwith the model. Word embeddings and other\nnon-recurrent matrices are initialized by sampling\nfrom a Gaussian N(0,0.012), recurrent matrices\nare random orthogonal and bias vectors are all\ninitialized to zero. To create the image annotations used by our\ndecoder, we used a ResNet-50 pre-trained on\nImageNet and extracted the features of size\n14 \u00d714 \u00d71024 at its res4f layer (He et al.,\n2016). In our experiments, our decoder operates\non the \ufb02attened 196 \u00d71024 (i.e L \u00d7D). We\nalso apply dropout with a probability of 0.5\non the embeddings, on the hidden states in the\nbidirectional RNN in the encoder as well as in the\ndecoder. In the decoder, we also apply dropout\non the text annotations hi, the image features\nai, on both modality context vector and on all\ncomponents of the deep output layer before the\nreadout operation. We apply dropout using one\nsame mask in all time steps (Gal and Ghahramani,\n2016). We also normalize and tokenize English and\nGerman descriptions using the Moses tokenizer\nscripts (Koehn et al., 2007). We use the byte pair\nencoding algorithm on the train set to convert\nspace-separated tokens into subwords (Sennrich\net al., 2016), reducing our vocabulary size to\n9226 and 14957 words for English and German\nrespectively. All variants of our attention model were trained\nwith ADADELTA (Zeiler, 2012), with mini-\nbatches of size 80 for our monomodal (text-only)\nNMT model and 40 for our multimodal NMT. We\napply early stopping for model selection based\non BLEU4 : training is halted if no improvement\non the development set is observed for more than\n20 epochs. We use the metrics BLEU4 (Papineni\net al., 2002), METEOR (Denkowski and Lavie,\n2014) and TER (Snover et al., 2006) to evaluate\nthe quality of our models\u2019 translations. 5.2 Quantitative results\nWe notice a nice overall progress over Calixto\net al. (2017) multimodal baseline, especially\nwhen using the stochastic attention. With im-\nprovements of +1.51 BLEU and -2.2 TER on both\nprecision-oriented metrics, the model shows a\nstrong similarity of the n-grams of our candidate\ntranslations with respect to the references. The\nmore recall-oriented metrics METEOR scores\n\nModel Test Scores\nBLEU\u2191 METEOR\u2191 TER\u2193\nMonomodal (text only)\nCaglayan et al. (2016) 32.50 49.2\nCalixto et al. (2017) 33.70 52.3 46.7\nNMT 34.11 \u2191+0.41 52.4 \u2191+0.1 46.2 \u2193-0.5\nMultimodal\nCaglayan et al. (2016) 27.82 45.0 -\nHuang et al. (2016) 36.50 54.1 -\nCalixto et al. (2017) 36.50 55.0 43.7\nSoft attention 37.10 \u2191+0.60 54.8 \u2193-0.2 42.8 \u2193-0.9\nLocal attention 37.55 \u2191+1.05 54.8 \u2193-0.2 42.4 \u2193-1.3\nStochastic attention 38.01 \u2191+1.51 55.4 \u2191+0.4 41.5 \u2193-2.2\nSoft attention + grounded image 37.62 \u2191+1.12 55.3 \u2191+0.3 41.8 \u2193-1.9\nStochastic attention + grounded image 38.17 \u2191+1.67 55.4 \u2191+0.4 41.5 \u2193-2.2\nTable 1: Results on the 1000 test triples of the Multi30K dataset. We pick Calixto et al. (2017) scores\nas baseline and report our results accordingly (green for improvement and red for deterioration). In each\nof our experiments, Soft attention is used for text. The comparison is hence with respect to the attention\nmechanism used for the image modality. are roughly the same across our models which is\nexpected because all attention mechanisms share\nthe same subsequent step at every time-step t,\ni.e. taking into account the attention weights of\nprevious time-step t\u22121 in order to compute the\nnew intermediate hidden state proposal and there-\nfore the new context vector it. Again, the largest\nimprovement is given by the hard stochastic\nattention mechanism (+0.4 METEOR): because it\nis modeled as a decision process according to the\nprevious choices, this may reinforce the idea of\nrecall. We also remark interesting improvements\nwhen using the grounded mechanism, especially\nfor the soft attention. The soft attention may\nbene\ufb01t more of the grounded image because of\nthe wide range of spatial locations it looks at,\nespecially compared to the stochastic attention. This motivates us to dig into more complex\ngrounding techniques in order to give the machine\na deeper understanding of the modalities. Note that even though our baseline NMT\nmodel is basically the same as Calixto et al. (2017), our experiments results are slightly better. This is probably due to the different use of dropout\nand subwords. We also compared our results to\nCaglayan et al. (2016) because our multimodal\nmodels are nearly identical with the major ex-\nception of the gating scalar (cfr. section 4). This\nmotivated some of our qualitative analysis and\nhesitation towards the current architecture in the\nnext section. 5.3 Qualitative results\nFor space-saving and ergonomic reasons, we\nonly discuss about the hard stochastic and soft\nattention, the latter being a generalization of the\nlocal attention. As we can see in Figure 7, the soft attention model\nis looking roughly at the same region of the image\nfor every decoding step t. Because the words\n\u201dhund\u201d(dog), \u201dwald\u201d(forest) or \u201dweg\u201d(way) in left\nimage are objects, they bene\ufb01t from a high gating\nscalar. As a matter of fact, the attention mech-\nanism has learned to detect the objects within a\nscene (at every time-step, whichever word we\nare decoding as shown in the right image) and\nthe gating scalar has learned to decide whether\nor not we have to look at the picture (or more\naccurately whether or not we are translating an\nobject). Without this scalar, the translation scores\nundergo a massive drop (as seen in Caglayan\net al. (2016)) which means that the attention\nmechanisms don\u2019t really understand the more\ncomplex relationships between objects, what is\nreally happening in the scene. Surprisingly, the\n\nFigure 7: Representative \ufb01gures of the soft-attention behavior discussed in \u00a75.3\ngating scalar happens to be really low in the\nstochastic attention mechanism: a signi\ufb01cant\namount of sentences don\u2019t have a summed gating\nscalar \u22650.10. The model totally discards the\nimage in the translation process. It is also worth to mention that we use a\nResNet trained on 1.28 million images for a\nclassi\ufb01cation tasks. The features used by the\nattention mechanism are strongly object-oriented\nand the machine could miss important information\nfor a multimodal translation task. We believe\nthat the robust architecture of both encoders\n{\u2190 \u2212\u03a8enc,\u2212 \u2192\u03a8enc}combined with a GRU layer and\nword-embeddings took care of the right trans-\nlation for relationships between objects and\ntime-dependencies. Yet, we noticed a common\nmisbehavior for all our multimodal models: if the\nattention loose track of the objects in the picture\nand \u201dgets lost\u201d, the model still takes it into account\nand somehow overrides the information brought\nby the text-based annotations. The translation\nis then totally mislead. We illustrate with an\nexample:\nRef: Ein Kind sitzt auf den Schultern einer\nFrau und klatscht . Mono: Ein Kind sitzt auf den Schultern einer\nFrau und schl\u00a8aft . Soft: Ein Kind , das sich auf der Schultern\neines Frau reitet , f\u00a8ahrt auf den\nSchultern . Hard: Ein Kind in der Haltung , w \u00a8ahrend er\nauf den Schultern einer Frau f\u00a8ahrt . The monomodal translation has a sentence-level\nBLEU of 82.16 whilst the soft attention and hard\nstochastic attention scores are of 16.82 and 34.45\nrespectively. Figure 8 shows the attention maps\nfor both mechanism. Nevertheless, one has to\nconcede that the use of images indubitably helps\nthe translation as shown in the score tabular. Figure 8: Wrong detection for both Soft attention\n(top) and Hard stochastic attention (bottom)\n6 Conclusion and future work\nWe have tried different attention mechanism and\ntweaks for the image modality. We showed im-\nprovements and encouraging results overall on the\nFlickr30K Entities dataset. Even though we iden-\nti\ufb01ed some \ufb02aws of the current attention mecha-\nnisms, we can conclude pretty safely that images\nare an helpful resource for the machine in a trans-\nlation task. We are looking forward to try out\nricher and more suitable features for multimodal\ntranslation (ie. dense captioning features). An-\nother interesting approach would be to use visu-\nally grounded word embeddings to capture visual\nnotions of semantic relatedness. 7 Acknowledgements\nThis work was partly supported by the Chist-Era\nproject IGLU with contribution from the Belgian\nFonds de la Recherche Scientique (FNRS), con-\ntract no.\n\nThe annotations\na1,a2,...,a L are spatial features (i.e. each anno-\ntation represents features for a speci\ufb01c region in\nthe image) . We follow the same protocol for our\nexperiments and describe it in section 5. 3 Attention-based Models\nWe evaluate three models of the image attention\nmechanism f\u2032\natt of equation 7. They have in com-\nmon the fact that at each time step t of the de-\ncoding phase, all approaches \ufb01rst take as input the\nannotation sequence I to derive a time-dependent\ncontext vector that contain relevant information\nin the image to help predict the current target\nword yt. Even though these models differ in how\nthe time-dependent context vector is derived, they\nshare the same subsequent steps. For each mech-\nanism, we propose two hand-picked illustrations\nshowing where the attention is placed in an image. 3.1 Soft attention\nSoft attention has \ufb01rstly been used for syntactic\nconstituency parsing by Vinyals et al. (2015) but\nhas been widely used for translation tasks ever\nsince. One should note that it slightly differs\nfrom Bahdanau et al. (2014) where their attention\ntakes as input the previous decoder hidden state\ninstead of the current (intermediate) one as shown\nin equation 7. This mechanism has also been\nsuccessfully investigated for the task of image\ndescription generation (Xu et al., 2015) where\na model generates an image\u2019s description in\nnatural language. It has been used in multimodal\ntranslation as well (Calixto et al., 2017), for which\nit constitutes a state-of-the-art. The idea of the soft attentional model is to\nconsider all the annotations when deriving the\ncontext vector it. It consists of a single feed-\nforward network used to compute an expected\nalignment et between modality annotation al and\nthe target word to be emitted at the current time\nstep t. The inputs are the modality annotations\nand the intermediate representation of REC1 s\u2032\nt:\net,l = vT tanh(Uas\u2032\nt + Waal) (12)\nThe vector et has length Land its l-th item con-\ntains a score of how much attention should be put\non the l-th annotation in order to output the best\nword at time t. We compute normalized scores to\ncreate an attention mask \u03b1t over annotations:\n\u03b1t,i = exp(et,i)\u2211L\nj=1 exp(et,j)\n(13)\nit =\nL\u2211\ni=1\n\u03b1t,iai (14)\nFinally, the modality time-dependent context vec-\ntor it is computed as a weighted sum over the an-\nnotation vectors (equation 14). In the above ex-\npressions, vT, Ua and Wa are trained parameters.",
            0.45201833299998384
          ]
        },
        {
          "answer": "The text compares three models of the image attention mechanism: soft attention, hard stochastic attention, and another model that is not explicitly named within the given context.",
          "relevant_chunks": [
            "The annotations\na1,a2,...,a L are spatial features (i.e. each anno-\ntation represents features for a speci\ufb01c region in\nthe image) . We follow the same protocol for our\nexperiments and describe it in section 5. 3 Attention-based Models\nWe evaluate three models of the image attention\nmechanism f\u2032\natt of equation 7. They have in com-\nmon the fact that at each time step t of the de-\ncoding phase, all approaches \ufb01rst take as input the\nannotation sequence I to derive a time-dependent\ncontext vector that contain relevant information\nin the image to help predict the current target\nword yt. Even though these models differ in how\nthe time-dependent context vector is derived, they\nshare the same subsequent steps. For each mech-\nanism, we propose two hand-picked illustrations\nshowing where the attention is placed in an image. 3.1 Soft attention\nSoft attention has \ufb01rstly been used for syntactic\nconstituency parsing by Vinyals et al. (2015) but\nhas been widely used for translation tasks ever\nsince. One should note that it slightly differs\nfrom Bahdanau et al. (2014) where their attention\ntakes as input the previous decoder hidden state\ninstead of the current (intermediate) one as shown\nin equation 7. This mechanism has also been\nsuccessfully investigated for the task of image\ndescription generation (Xu et al., 2015) where\na model generates an image\u2019s description in\nnatural language. It has been used in multimodal\ntranslation as well (Calixto et al., 2017), for which\nit constitutes a state-of-the-art. The idea of the soft attentional model is to\nconsider all the annotations when deriving the\ncontext vector it. It consists of a single feed-\nforward network used to compute an expected\nalignment et between modality annotation al and\nthe target word to be emitted at the current time\nstep t. The inputs are the modality annotations\nand the intermediate representation of REC1 s\u2032\nt:\net,l = vT tanh(Uas\u2032\nt + Waal) (12)\nThe vector et has length Land its l-th item con-\ntains a score of how much attention should be put\non the l-th annotation in order to output the best\nword at time t. We compute normalized scores to\ncreate an attention mask \u03b1t over annotations:\n\u03b1t,i = exp(et,i)\u2211L\nj=1 exp(et,j)\n(13)\nit =\nL\u2211\ni=1\n\u03b1t,iai (14)\nFinally, the modality time-dependent context vec-\ntor it is computed as a weighted sum over the an-\nnotation vectors (equation 14). In the above ex-\npressions, vT, Ua and Wa are trained parameters.\n\nFigure 1: Die beiden Kinder spielen auf dem\nSpielplatz . Figure 2: Ein Junge sitzt auf und blickt aus einem\nMikroskop . 3.2 Hard Stochastic attention\nThis model is a stochastic and sampling-based\nprocess where, at every timestep t, we are making\na hard choice to attend only one annotation. This\ncorresponds to one spatial location in the image. Hard attention has previously been used in the\ncontext of object recognition (Mnih et al., 2014;\nBa et al., 2015) and later extended to image\ndescription generation (Xu et al., 2015). In the\ncontext of multimodal NMT, we can follow Xu\net al.\n\n2015. Effective approaches to attention-\nbased neural machine translation. In Proceedings of\nthe 2015 Conference on Empirical Methods in Nat-\nural Language Processing. V olodymyr Mnih, Nicolas Heess, Alex Graves, and\nkoray kavukcuoglu. 2014. Recurrent models of\nvisual attention. In Z. Ghahramani, M. Welling,\nC. Cortes, N.D.",
            1.24703845800002
          ]
        }
      ],
      "reference_answers": [
        "if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations",
        "Soft attention, Hard Stochastic attention, Local Attention"
      ]
    },
    "1707.00110.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.019816839701347233,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.1579378068739771,
          "recall": 0.7083333333333333,
          "f1": 0.25194228634850163
        },
        "rouge-2": {
          "precision": 0.09086956521739131,
          "recall": 0.3857142857142857,
          "f1": 0.1446078431372549
        },
        "rouge-L": {
          "precision": 0.13666121112929625,
          "recall": 0.5416666666666666,
          "f1": 0.21420643729189787
        }
      },
      "per_question_metrics": [
        {
          "question": "Which baseline methods are used?",
          "generated_answer": "The baseline methods used in the context are:\n1. Standard parametrized attention (Bahdanau et al., 2014).\n2. Non-attention baseline models.\n3. Regular attention mechanism models.",
          "reference_answer": "standard parametrized attention and a non-attention baseline",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.23076923076923078,
              "recall": 0.75,
              "f1": 0.3529411764705882
            },
            "rouge-2": {
              "precision": 0.16,
              "recall": 0.5714285714285714,
              "f1": 0.25
            },
            "rouge-L": {
              "precision": 0.23076923076923078,
              "recall": 0.75,
              "f1": 0.3529411764705882
            }
          },
          "bleu_score": 0.036309066512747096,
          "exact_match": 0.0
        },
        {
          "question": "Which datasets are used in experiments?",
          "generated_answer": "The datasets used in the experiments are from the WMT\u201917 translation tasks, specifically for the following language pairs: English-Czech (en-cs, 52M examples), English-German (en-de, 5.9M examples), English-Finnish (en-fi, 2.6M examples), and English-Turkish (en-tr, 207,373 examples).",
          "reference_answer": "Sequence Copy Task and WMT'17",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.0851063829787234,
              "recall": 0.6666666666666666,
              "f1": 0.1509433962264151
            },
            "rouge-2": {
              "precision": 0.021739130434782608,
              "recall": 0.2,
              "f1": 0.0392156862745098
            },
            "rouge-L": {
              "precision": 0.0425531914893617,
              "recall": 0.3333333333333333,
              "f1": 0.07547169811320754
            }
          },
          "bleu_score": 0.003324612889947367,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "Which baseline methods are used?",
        "Which datasets are used in experiments?"
      ],
      "generated_answers": [
        {
          "answer": "The baseline methods used in the context are:\n1. Standard parametrized attention (Bahdanau et al., 2014).\n2. Non-attention baseline models.\n3. Regular attention mechanism models.",
          "relevant_chunks": [
            "step based on the current state of the decoder. Intuitively, this corresponds to looking at the source\nsequence after the output of every single target token. Inspired by how humans process sentences, we\nbelieve it may be unnecessary to look back at the\nentire original source sequence at each step.1 We thus\npropose an alternative attention mechanism (section 3)\nthat leads to smaller computational time complexity. Our method predictsKattention context vectors while\nreading the source, and learns to use a weighted av-\nerage of these vectors at each step of decoding. Thus,\nwe avoid looking back at the source sequence once\nit has been encoded. We show (section 4) that this\nspeeds up inference while performing on-par with the\nstandard mechanism on both toy and real-world WMT\ntranslation datasets. We also show that our mecha-\nnism leads to larger speedups as sequences get longer. Finally, by visualizing the attention scores (section\n5), we verify that the proposed technique learns mean-\ningful alignments, and that different attention context\nvectors specialize on different parts of the source. 2 Background\n2.1 Sequence-to-Sequence Model with Attention\nOur models are based on an encoder-decoder archi-\ntecture with attention mechanism (Bahdanau et al.,\n2014; Luong et al., 2015). An encoder function takes\nas input a sequence of source tokensx=(x1,...,xm)\nand produces a sequence of statess=(s1,...,sm) .The\ndecoder is an RNN that predicts the probability of a\ntarget sequencey=(y1,...,yT|s). The probability of\neach target tokenyi\u2208{1,...,|V|}is predicted based\non the recurrent state in the decoder RNN,hi, the pre-\nvious words,y<i, and a context vectorci. The context\nvector ci, also referred to as the attention vector, is\ncalculated as a weighted average of the source states. 1Eye-tracking and keystroke logging data from human\ntranslators show that translators generally do not reread previously\ntranslated source text words when producing target text (Carl\net al., 2011). arXiv:1707.00110v1  [cs.CL]  1 Jul 2017\n\nci=\n\u2211\nj\n\u03b1ijsj (1)\n\u03b1i=softmax(fatt(hi,s)) (2)\nHere, fatt(hi,s) is an attention function that\ncalculates an unnormalized alignment score between\nthe encoder statesj and the decoder statehi. V ariants\nof fatt used in Bahdanau et al. (2014) and Luong\net al. (2015) are:\nfatt(hi,sj)=\n{\nvT\natanh(Wa[hi,sj]), Bahdanau\nhT\ni Wasj Luong\nwhere Wa and va are model parameters learned to\npredict alignment. Let|S|and |T|denote the lengths\nof the source and target sequences respectively andD\ndenoate the state size of the encoder and decoder RNN. Such content-based attention mechanisms result in in-\nference times ofO(D2|S||T|)2, as each context vector\ndepends on the current decoder statehiand all encoder\nstates, and requires anO(D2) matrix multiplication. The decoder outputs a distribution over a\nvocabulary of fixed-size|V|:\nP(yi|y<i,x)= softmax(W[si;ci]+b) (3)\nThe model is trained end-to-end by minimizing the\nnegative log likelihood of the target words using\nstochastic gradient descent. 3 Memory-Based Attention Model\nOur proposed model is shown in Figure 1. During en-\ncoding, we compute an attention matrixC\u2208RK\u00d7D,\nwhere K is the number of attention vectors and a\nhyperparameter of our method, andDis the dimen-\nsionality of the top-most encoder state. This matrix\nis computed by predicting a score vector\u03b1t \u2208RK\nat each encoding time stept. C is then a linear\ncombination of the encoder states, weighted by\u03b1t:\nCk=\n|S|\u2211\nt=0\n\u03b1tkst (4)\n\u03b1t=softmax(W\u03b1st), (5)\nwhereW\u03b1 is a parameter matrix inRK\u00d7D. The computational time complexity for this\noperation isO(KD|S|). One can think of C as\ncompact fixed-length memory that the decoder\n2An exception is the dot-attention from Luong et al. (2015),\nwhich isO(D|S||T|), which we discuss further in Section 3. will perform attention over. In contrast, standard\napproaches use a variable-length set of encoder states\nfor attention. At each decoding step, we similarly\npredictKscores\u03b2\u2208RK. The final attention context\ncis a linear combination of the rows inCweighted\nby the scores. Intuitively, each decoder step predicts\nhow important each of theKattention vectors is. c=\nK\u2211\ni=0\n\u03b2iCi (6)\n\u03b2=softmax(W\u03b2h) (7)\nHere,his the current state of the decoder, andW\u03b2 is a\nlearned parameter matrix. Note that we do not access\nthe encoder states at each decoder step. We simply\ntake a linear combination of the attention matrixC\npre-computed during encoding - a much cheaper op-\neration that is independent of the length of the source\nsequence. The time complexity of this computation\nis O(KD|T|) as multiplication with theKattention\nmatrices needs to happen at each decoding step. Summing O(KD|S|) from encoding and\nO(KD|T|) from decoding, we have a total linear\ncomputational complexity ofO(KD(|S|+ |T|). As D is typically very large,512 or 1024 units in\nmost applications, we expect our model to be faster\nthan the standard attention mechanism running in\nO(D2|S||T|). For long sequences (as in summariza-\ntion, where \u2014S\u2014 is large), we also expect our model\nto be faster than the cheaper dot-based attention mech-\nanism, which needsO(D|S||T|) computation time\nand requires encoder and decoder states sizes to match. We also experimented with using a sigmoid\nfunction instead of the softmax to score the encoder\nand decoder attention scores, resulting in 4 possible\ncombinations. We call this choice thescoring function. A softmax scoring function calculates normalized\nscores, while the sigmoid scoring function results in\nunnormalized scores that can be understood as gates. 3.1 Model Interpretations\nOur memory-based attention model can be under-\nstood intuitively in two ways. We can interpret it as\n\u201dpredicting\u201d the set of attention contexts produced\nby a standard attention mechanism during encoding. To see this, assume we setK\u2248|T|. In this case, we\npredict all|T|attention contexts during the encoding\nstage and learn to choose the right one during\ndecoding. This is cheaper than computing contexts\none-by-one based on the decoder and encoder content. In fact, we could enforce this objective by first training\n\nFigure 1: Memory Attention model architecture.K attention vectors are predicted during encoding, and a\nlinear combination is chosen during decoding. In our example,K=3. a regular attention model and adding a regularization\nterm to force the memory matrixC to be close to\nthe T\u00d7Dvectors computed by the standard attention. We leave it to future work to explore such an objective. Alternatively, we can interpret our mechanism as\nfirst predicting a compactK\u00d7D memory matrix,\na representation of the source sequence, and then\nperforminglocation-basedattention on the memory\nby picking which row of the matrix to attend to. Standard location-based attention mechanism, by\ncontrast, predicts a location in the source sequence\nto focus on (Luong et al., 2015; Xu et al., 2015). 3.2 Position Encodings (PE)\nIn the above formulation, the predictions of attention\ncontexts are symmetric. That is,Ci is not forced to\nbe different fromCj\u0338=i. While we would hope for the\nmodel to learn to generate distinct attention contexts,\nwe now present an extension that pushes the model\ninto this direction. We addposition encodingsto the\nscore matrix that forces the first few context vector\nC1,C2,...to focus on the beginning of the sequence\nand the last few vectors...,CK\u22121,CK to focus on the\nend (thereby encouraging in-between vectors to focus\non the middle). Explicitly, we multiply the score vector\u03b1 with\nposition encodingsls\u2208RK:\nCPE =\n|S|\u2211\ns=0\n\u03b1PEhs (8)\n\u03b1PE\ns =softmax(W\u03b1hs\u25e6ls) (9)\nTo obtainls we first calculate a constant matrixL\nwhere we define each element as\nLks=(1\u2212k/K)(1\u2212s/S)+ k\nK\ns\nS, (10)\nadapting a formula from (Sukhbaatar et al., 2015). Here, k \u2208{1,2,...,K}is the context vector index\nand Sis the maximum sequence length across all\nsource sequences. The manifold is shown graphically\nin Figure 2. We can see that earlier encoder states are\nupweighted in the first context vectors, and later states\nare upweighted in later vectors. The symmetry of the\nmanifold and its stationary point having value 0.5 both\nfollow from Eq. 10. The elements of the matrix that\nfall beyond the sequence lengths are then masked out\nand the remaining elements are renormalized across\nthe timestep dimension. This results in the jagged\narray of position encodings{lks}. Figure 2: Surface for the position encodings. 4 Experiments\n4.1 Toy Copying Experiment\nDue to the reduction of computational time complex-\nity we expect our method to yield performance gains\nespecially for longer sequences and tasks where the\nsource can be compactly represented in a fixed-size\nmemory matrix. To investigate the trade-off between\nspeed and performance, we compare our technique\nto standard models with and without attention on a\nSequence Copy Task of varying length like in Graves\net al. (2014). We generated 4 training datasets of\n100,000 examples and a validation dataset of 1,000 ex-\namples. The vocabulary size was 20. For each dataset,\nthe sequences had lengths randomly chosen between 0\nto L, forL\u2208{10,50,100,200}unique to each dataset. 4.1.1 Training Setup\nAll models are implemented using TensorFlow\nbased on the seq2seq implementation of Britz et al. (2017)3 and trained on a single machine with a\nNvidia K40m GPU. We use a 2-layer 256-unit, a\nbidirectional LSTM (Hochreiter and Schmidhuber,\n1997) encoder, a 2-layer 256-unit LSTM decoder,\nand 256-dimensional embeddings. For the attention\nbaseline, we use the standard parametrized attention\n(Bahdanau et al., 2014). Dropout of 0.2 (0.8 keep\nprobability) is applied to the input of each cell and\nwe optimize using Adam (Kingma and Ba, 2014) at\na learning rate of 0.0001 and batch size 128. We train\nfor at most 200,000 steps (see Figure 3 for sample\nlearning curves). BLEU scores are calculated on\ntokenized data using themulti-bleu.perlscript in\nMoses.4 We decode using beam search with a beam\n3http://github.com/google/seq2seq\n4http://github.com/moses-smt/mosesdecoder\nLength Model BLEU Time (s)\n20 No Att 99.93 2.03\nK=1 99.52 2.12\nK=4 99.56 2.25\nK=16 99.56 2.21\nK=32 99.57 2.59\nK=64 99.75 2.86\nAtt 99.98 2.86\n50 No Att 97.37 3.90\nK=1 98.86 4.33\nK=4 99.95 4.48\nK=16 99.96 4.58\nK=32 99.96 5.35\nK=64 99.97 5.84\nAtt 99.94 6.46\n100 No Att 73.99 6.33\nK=1 87.42 7.32\nK=4 99.81 7.47\nK=16 99.97 7.50\nK=32 99.99 7.65\nK=64 100.00 7.77\nAtt 100.00 11.00\n200 No Att 32.64 9.10\nK=1 44.22 9.30\nK=4 98.54 9.49\nK=16 99.98 9.53\nK=32 100.00 9.59\nK=64 100.00 9.78\nAtt 100.00 14.28\nTable 1: BLEU scores and computation times with\nvaryingKand sequence length compared to baseline\nmodels with and without attention. size of 10 (Wiseman and Rush, 2016). 4.1.2 Results\nTable 1 shows the BLEU scores of our model on differ-\nent sequence lengths while varyingK. This is a study\nof the trade-off between computational time and rep-\nresentational power. A largeKallows us to compute\ncomplex source representations, while aKof 1 limits\nthe source representation to a single vector. We can\nsee that performance consistently increases withKup\nto a point that depends on the data length, with longer\nsequences requiring more complex representations. The results with and without position encodings are\nalmost identical on the toy data. Our technique learns\nto fit the data as well as the standard attention mecha-\nnism despite having less representational power. Both\nbeat the non-attention baseline by a significant margin. 0 50 100 150 200\nsteps (k)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5log perplexity\nK=1\nK=4\nK=16\nK=32\nK=64\nattention\nno attention\n(a) Comparison of varyingK for copying sequences of length\n200 on evaluation data, showing that largeK leads to faster\nconvergence and smallK performs similarly to the non-attentional\nbaseline. 0 50 100 150 200\nsteps (k)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5log perplexity\nsigmoid/sigmoid\nsigmoid/softmax\nsoftmax/sigmoid\nsoftmax/softmax\n(b) Comparison of sigmoid and softmax functions for choosing the\nencoder and decoder attention scores on evaluation data, showing\nthat choice of gating/normalization matters. Figure 3: Training Curves for the Toy Copy task\nThat we are able to represent the source sequence\nwith a fixed size matrix with fewer than|S|rows\nsuggests that traditional attention mechanisms\nmay be representing the source with redundancies\nand wasting computational resources. This makes\nintuitive sense for the toy task, which should require\na relatively simple representation. The last column shows that our technique signif-\nicantly speeds up the inference process. The gap in\ninference speed increases as sequences become longer. We measured inference time on the full validation\nset of 1,000 examples, not including data loading or\nmodel construction times. Figure 3a shows the learning curves for sequence\nlength 200. We see thatK=1 is unable to fit the data\ndistribution, whileK\u2208{32,64}fits the data almost as\nquickly as the attention-based model. Figure 3b shows\nthe effect of varying the encoder and decoder scoring\nfunctions between softmax and sigmoid. All combina-\ntions manage to fit the data, but some converge faster\nthan others. In section 5 we show that distinct align-\nments are learned by different function combinations. 4.2 Machine Translation\nNext, we explore if the memory-based attention\nmechanism is able to fit complex real-world datasets. For this purpose we use 4 large machine translation\ndatasets of WMT\u2019175 on the following language\npairs: English-Czech (en-cs, 52M examples), English-\nGerman (en-de, 5.9M examples), English-Finish\n(en-fi, 2.6M examples), and English-Turkish (en-tr,\n207,373 examples). We used the newly available pre-\n5statmt.org/wmt17/translation-task.html\nprocessed datasets for the WMT\u201917 task.6 Note that\nour scores may not be directly comparable to other\nwork that performs their own data pre-processing. We\nlearn shared vocabularies of 16,000 subword units\nusing the BPE algorithm (Sennrich et al., 2016). We use newstest2015 as a validation set, and report\nBLEU on newstest2016. 4.2.1 Training Setup\nWe use a similar setup to the Toy Copy task, but\nuse 512 RNN and embedding units, train using 8\ndistributed workers with 1 GPU each, and train for\nat most 1M steps. We save checkpoints every 30\nminutes during training, and choose the best based\non the validation BLEU score. 4.2.2 Results\nTable 2 compares our approach with and without\nposition encodings, and with varying values for\nhyperparameterK, to baseline models with regular\nattention mechanism. Learning curves are shown in\nFigure 4. We see that our memory attention model\nwith sufficiently highK performs on-par with, or\nslightly better, than the attention-based baseline model\ndespite its simpler nature. Across the board, models\nwith K = 64performed better than corresponding\nmodels withK= 32, suggesting that using a larger\nnumber of attention vectors can capture a richer under-\nstanding of source sequences. Position encodings also\nseem to consistently improve model performance. Table 3 shows that our model results in faster de-\ncoding time even on a complex dataset with a large\n6http://data.statmt.org/wmt17/translation-task/preprocessed\n\nK\nK\n(a) Training curves for en-fi\nK\nK (b) Training curves for en-tr\nFigure 4: Comparing training curves for en-fi and en-tr with sigmoid encoder scoring and softmax decoder\nscoring and position encoding. Note that en-tr curves converged very quickly. Model Dataset K en-cs en-de en-fi en-tr\nMemory Attention Test 32 19.37 28.82 15.87 -\n64 19.65 29.53 16.49 -\nV alid 32 19.20 26.20 15.90 12.94\n64 19.63 26.39 16.35 13.06\nMemory Attention + PETest 32 19.45 29.53 15.86 -\n64 20.36 30.61 17.03 -\nV alid 32 19.35 26.22 16.31 12.97\n64 19.73 27.31 16.91 13.25\nAttention Test - 19.19 30.99 17.34 -\nV alid - 18.61 28.13 17.16 13.76\nTable 2: BLEU scores on WMT\u201917 translation datasets from the memory attention models and regular attention\nbaselines. We picked the best out of the four scoring function combinations on the validation set.\n\n2017. Online and linear-time\nattention by enforcing monotonic alignments.CoRR\nabs/1704.00784. http://arxiv.org/abs/1704.00784.\n\nhttp://arxiv.org/abs/1510.03055. Minh-Thang Luong, Hieu Pham, and Christopher D. Man-\nning.",
            1.0660420410000029
          ]
        },
        {
          "answer": "The datasets used in the experiments are from the WMT\u201917 translation tasks, specifically for the following language pairs: English-Czech (en-cs, 52M examples), English-German (en-de, 5.9M examples), English-Finnish (en-fi, 2.6M examples), and English-Turkish (en-tr, 207,373 examples).",
          "relevant_chunks": [
            "step based on the current state of the decoder. Intuitively, this corresponds to looking at the source\nsequence after the output of every single target token. Inspired by how humans process sentences, we\nbelieve it may be unnecessary to look back at the\nentire original source sequence at each step.1 We thus\npropose an alternative attention mechanism (section 3)\nthat leads to smaller computational time complexity. Our method predictsKattention context vectors while\nreading the source, and learns to use a weighted av-\nerage of these vectors at each step of decoding. Thus,\nwe avoid looking back at the source sequence once\nit has been encoded. We show (section 4) that this\nspeeds up inference while performing on-par with the\nstandard mechanism on both toy and real-world WMT\ntranslation datasets. We also show that our mecha-\nnism leads to larger speedups as sequences get longer. Finally, by visualizing the attention scores (section\n5), we verify that the proposed technique learns mean-\ningful alignments, and that different attention context\nvectors specialize on different parts of the source. 2 Background\n2.1 Sequence-to-Sequence Model with Attention\nOur models are based on an encoder-decoder archi-\ntecture with attention mechanism (Bahdanau et al.,\n2014; Luong et al., 2015). An encoder function takes\nas input a sequence of source tokensx=(x1,...,xm)\nand produces a sequence of statess=(s1,...,sm) .The\ndecoder is an RNN that predicts the probability of a\ntarget sequencey=(y1,...,yT|s). The probability of\neach target tokenyi\u2208{1,...,|V|}is predicted based\non the recurrent state in the decoder RNN,hi, the pre-\nvious words,y<i, and a context vectorci. The context\nvector ci, also referred to as the attention vector, is\ncalculated as a weighted average of the source states. 1Eye-tracking and keystroke logging data from human\ntranslators show that translators generally do not reread previously\ntranslated source text words when producing target text (Carl\net al., 2011). arXiv:1707.00110v1  [cs.CL]  1 Jul 2017\n\nci=\n\u2211\nj\n\u03b1ijsj (1)\n\u03b1i=softmax(fatt(hi,s)) (2)\nHere, fatt(hi,s) is an attention function that\ncalculates an unnormalized alignment score between\nthe encoder statesj and the decoder statehi. V ariants\nof fatt used in Bahdanau et al. (2014) and Luong\net al. (2015) are:\nfatt(hi,sj)=\n{\nvT\natanh(Wa[hi,sj]), Bahdanau\nhT\ni Wasj Luong\nwhere Wa and va are model parameters learned to\npredict alignment. Let|S|and |T|denote the lengths\nof the source and target sequences respectively andD\ndenoate the state size of the encoder and decoder RNN. Such content-based attention mechanisms result in in-\nference times ofO(D2|S||T|)2, as each context vector\ndepends on the current decoder statehiand all encoder\nstates, and requires anO(D2) matrix multiplication. The decoder outputs a distribution over a\nvocabulary of fixed-size|V|:\nP(yi|y<i,x)= softmax(W[si;ci]+b) (3)\nThe model is trained end-to-end by minimizing the\nnegative log likelihood of the target words using\nstochastic gradient descent. 3 Memory-Based Attention Model\nOur proposed model is shown in Figure 1. During en-\ncoding, we compute an attention matrixC\u2208RK\u00d7D,\nwhere K is the number of attention vectors and a\nhyperparameter of our method, andDis the dimen-\nsionality of the top-most encoder state. This matrix\nis computed by predicting a score vector\u03b1t \u2208RK\nat each encoding time stept. C is then a linear\ncombination of the encoder states, weighted by\u03b1t:\nCk=\n|S|\u2211\nt=0\n\u03b1tkst (4)\n\u03b1t=softmax(W\u03b1st), (5)\nwhereW\u03b1 is a parameter matrix inRK\u00d7D. The computational time complexity for this\noperation isO(KD|S|). One can think of C as\ncompact fixed-length memory that the decoder\n2An exception is the dot-attention from Luong et al. (2015),\nwhich isO(D|S||T|), which we discuss further in Section 3. will perform attention over. In contrast, standard\napproaches use a variable-length set of encoder states\nfor attention. At each decoding step, we similarly\npredictKscores\u03b2\u2208RK. The final attention context\ncis a linear combination of the rows inCweighted\nby the scores. Intuitively, each decoder step predicts\nhow important each of theKattention vectors is. c=\nK\u2211\ni=0\n\u03b2iCi (6)\n\u03b2=softmax(W\u03b2h) (7)\nHere,his the current state of the decoder, andW\u03b2 is a\nlearned parameter matrix. Note that we do not access\nthe encoder states at each decoder step. We simply\ntake a linear combination of the attention matrixC\npre-computed during encoding - a much cheaper op-\neration that is independent of the length of the source\nsequence. The time complexity of this computation\nis O(KD|T|) as multiplication with theKattention\nmatrices needs to happen at each decoding step. Summing O(KD|S|) from encoding and\nO(KD|T|) from decoding, we have a total linear\ncomputational complexity ofO(KD(|S|+ |T|). As D is typically very large,512 or 1024 units in\nmost applications, we expect our model to be faster\nthan the standard attention mechanism running in\nO(D2|S||T|). For long sequences (as in summariza-\ntion, where \u2014S\u2014 is large), we also expect our model\nto be faster than the cheaper dot-based attention mech-\nanism, which needsO(D|S||T|) computation time\nand requires encoder and decoder states sizes to match. We also experimented with using a sigmoid\nfunction instead of the softmax to score the encoder\nand decoder attention scores, resulting in 4 possible\ncombinations. We call this choice thescoring function. A softmax scoring function calculates normalized\nscores, while the sigmoid scoring function results in\nunnormalized scores that can be understood as gates. 3.1 Model Interpretations\nOur memory-based attention model can be under-\nstood intuitively in two ways. We can interpret it as\n\u201dpredicting\u201d the set of attention contexts produced\nby a standard attention mechanism during encoding. To see this, assume we setK\u2248|T|. In this case, we\npredict all|T|attention contexts during the encoding\nstage and learn to choose the right one during\ndecoding. This is cheaper than computing contexts\none-by-one based on the decoder and encoder content. In fact, we could enforce this objective by first training\n\nFigure 1: Memory Attention model architecture.K attention vectors are predicted during encoding, and a\nlinear combination is chosen during decoding. In our example,K=3. a regular attention model and adding a regularization\nterm to force the memory matrixC to be close to\nthe T\u00d7Dvectors computed by the standard attention. We leave it to future work to explore such an objective. Alternatively, we can interpret our mechanism as\nfirst predicting a compactK\u00d7D memory matrix,\na representation of the source sequence, and then\nperforminglocation-basedattention on the memory\nby picking which row of the matrix to attend to. Standard location-based attention mechanism, by\ncontrast, predicts a location in the source sequence\nto focus on (Luong et al., 2015; Xu et al., 2015). 3.2 Position Encodings (PE)\nIn the above formulation, the predictions of attention\ncontexts are symmetric. That is,Ci is not forced to\nbe different fromCj\u0338=i. While we would hope for the\nmodel to learn to generate distinct attention contexts,\nwe now present an extension that pushes the model\ninto this direction. We addposition encodingsto the\nscore matrix that forces the first few context vector\nC1,C2,...to focus on the beginning of the sequence\nand the last few vectors...,CK\u22121,CK to focus on the\nend (thereby encouraging in-between vectors to focus\non the middle). Explicitly, we multiply the score vector\u03b1 with\nposition encodingsls\u2208RK:\nCPE =\n|S|\u2211\ns=0\n\u03b1PEhs (8)\n\u03b1PE\ns =softmax(W\u03b1hs\u25e6ls) (9)\nTo obtainls we first calculate a constant matrixL\nwhere we define each element as\nLks=(1\u2212k/K)(1\u2212s/S)+ k\nK\ns\nS, (10)\nadapting a formula from (Sukhbaatar et al., 2015). Here, k \u2208{1,2,...,K}is the context vector index\nand Sis the maximum sequence length across all\nsource sequences. The manifold is shown graphically\nin Figure 2. We can see that earlier encoder states are\nupweighted in the first context vectors, and later states\nare upweighted in later vectors. The symmetry of the\nmanifold and its stationary point having value 0.5 both\nfollow from Eq. 10. The elements of the matrix that\nfall beyond the sequence lengths are then masked out\nand the remaining elements are renormalized across\nthe timestep dimension. This results in the jagged\narray of position encodings{lks}. Figure 2: Surface for the position encodings. 4 Experiments\n4.1 Toy Copying Experiment\nDue to the reduction of computational time complex-\nity we expect our method to yield performance gains\nespecially for longer sequences and tasks where the\nsource can be compactly represented in a fixed-size\nmemory matrix. To investigate the trade-off between\nspeed and performance, we compare our technique\nto standard models with and without attention on a\nSequence Copy Task of varying length like in Graves\net al. (2014). We generated 4 training datasets of\n100,000 examples and a validation dataset of 1,000 ex-\namples. The vocabulary size was 20. For each dataset,\nthe sequences had lengths randomly chosen between 0\nto L, forL\u2208{10,50,100,200}unique to each dataset. 4.1.1 Training Setup\nAll models are implemented using TensorFlow\nbased on the seq2seq implementation of Britz et al. (2017)3 and trained on a single machine with a\nNvidia K40m GPU. We use a 2-layer 256-unit, a\nbidirectional LSTM (Hochreiter and Schmidhuber,\n1997) encoder, a 2-layer 256-unit LSTM decoder,\nand 256-dimensional embeddings. For the attention\nbaseline, we use the standard parametrized attention\n(Bahdanau et al., 2014). Dropout of 0.2 (0.8 keep\nprobability) is applied to the input of each cell and\nwe optimize using Adam (Kingma and Ba, 2014) at\na learning rate of 0.0001 and batch size 128. We train\nfor at most 200,000 steps (see Figure 3 for sample\nlearning curves). BLEU scores are calculated on\ntokenized data using themulti-bleu.perlscript in\nMoses.4 We decode using beam search with a beam\n3http://github.com/google/seq2seq\n4http://github.com/moses-smt/mosesdecoder\nLength Model BLEU Time (s)\n20 No Att 99.93 2.03\nK=1 99.52 2.12\nK=4 99.56 2.25\nK=16 99.56 2.21\nK=32 99.57 2.59\nK=64 99.75 2.86\nAtt 99.98 2.86\n50 No Att 97.37 3.90\nK=1 98.86 4.33\nK=4 99.95 4.48\nK=16 99.96 4.58\nK=32 99.96 5.35\nK=64 99.97 5.84\nAtt 99.94 6.46\n100 No Att 73.99 6.33\nK=1 87.42 7.32\nK=4 99.81 7.47\nK=16 99.97 7.50\nK=32 99.99 7.65\nK=64 100.00 7.77\nAtt 100.00 11.00\n200 No Att 32.64 9.10\nK=1 44.22 9.30\nK=4 98.54 9.49\nK=16 99.98 9.53\nK=32 100.00 9.59\nK=64 100.00 9.78\nAtt 100.00 14.28\nTable 1: BLEU scores and computation times with\nvaryingKand sequence length compared to baseline\nmodels with and without attention. size of 10 (Wiseman and Rush, 2016). 4.1.2 Results\nTable 1 shows the BLEU scores of our model on differ-\nent sequence lengths while varyingK. This is a study\nof the trade-off between computational time and rep-\nresentational power. A largeKallows us to compute\ncomplex source representations, while aKof 1 limits\nthe source representation to a single vector. We can\nsee that performance consistently increases withKup\nto a point that depends on the data length, with longer\nsequences requiring more complex representations. The results with and without position encodings are\nalmost identical on the toy data. Our technique learns\nto fit the data as well as the standard attention mecha-\nnism despite having less representational power. Both\nbeat the non-attention baseline by a significant margin. 0 50 100 150 200\nsteps (k)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5log perplexity\nK=1\nK=4\nK=16\nK=32\nK=64\nattention\nno attention\n(a) Comparison of varyingK for copying sequences of length\n200 on evaluation data, showing that largeK leads to faster\nconvergence and smallK performs similarly to the non-attentional\nbaseline. 0 50 100 150 200\nsteps (k)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5log perplexity\nsigmoid/sigmoid\nsigmoid/softmax\nsoftmax/sigmoid\nsoftmax/softmax\n(b) Comparison of sigmoid and softmax functions for choosing the\nencoder and decoder attention scores on evaluation data, showing\nthat choice of gating/normalization matters. Figure 3: Training Curves for the Toy Copy task\nThat we are able to represent the source sequence\nwith a fixed size matrix with fewer than|S|rows\nsuggests that traditional attention mechanisms\nmay be representing the source with redundancies\nand wasting computational resources. This makes\nintuitive sense for the toy task, which should require\na relatively simple representation. The last column shows that our technique signif-\nicantly speeds up the inference process. The gap in\ninference speed increases as sequences become longer. We measured inference time on the full validation\nset of 1,000 examples, not including data loading or\nmodel construction times. Figure 3a shows the learning curves for sequence\nlength 200. We see thatK=1 is unable to fit the data\ndistribution, whileK\u2208{32,64}fits the data almost as\nquickly as the attention-based model. Figure 3b shows\nthe effect of varying the encoder and decoder scoring\nfunctions between softmax and sigmoid. All combina-\ntions manage to fit the data, but some converge faster\nthan others. In section 5 we show that distinct align-\nments are learned by different function combinations. 4.2 Machine Translation\nNext, we explore if the memory-based attention\nmechanism is able to fit complex real-world datasets. For this purpose we use 4 large machine translation\ndatasets of WMT\u2019175 on the following language\npairs: English-Czech (en-cs, 52M examples), English-\nGerman (en-de, 5.9M examples), English-Finish\n(en-fi, 2.6M examples), and English-Turkish (en-tr,\n207,373 examples). We used the newly available pre-\n5statmt.org/wmt17/translation-task.html\nprocessed datasets for the WMT\u201917 task.6 Note that\nour scores may not be directly comparable to other\nwork that performs their own data pre-processing. We\nlearn shared vocabularies of 16,000 subword units\nusing the BPE algorithm (Sennrich et al., 2016). We use newstest2015 as a validation set, and report\nBLEU on newstest2016. 4.2.1 Training Setup\nWe use a similar setup to the Toy Copy task, but\nuse 512 RNN and embedding units, train using 8\ndistributed workers with 1 GPU each, and train for\nat most 1M steps. We save checkpoints every 30\nminutes during training, and choose the best based\non the validation BLEU score. 4.2.2 Results\nTable 2 compares our approach with and without\nposition encodings, and with varying values for\nhyperparameterK, to baseline models with regular\nattention mechanism. Learning curves are shown in\nFigure 4. We see that our memory attention model\nwith sufficiently highK performs on-par with, or\nslightly better, than the attention-based baseline model\ndespite its simpler nature. Across the board, models\nwith K = 64performed better than corresponding\nmodels withK= 32, suggesting that using a larger\nnumber of attention vectors can capture a richer under-\nstanding of source sequences. Position encodings also\nseem to consistently improve model performance. Table 3 shows that our model results in faster de-\ncoding time even on a complex dataset with a large\n6http://data.statmt.org/wmt17/translation-task/preprocessed\n\nK\nK\n(a) Training curves for en-fi\nK\nK (b) Training curves for en-tr\nFigure 4: Comparing training curves for en-fi and en-tr with sigmoid encoder scoring and softmax decoder\nscoring and position encoding. Note that en-tr curves converged very quickly. Model Dataset K en-cs en-de en-fi en-tr\nMemory Attention Test 32 19.37 28.82 15.87 -\n64 19.65 29.53 16.49 -\nV alid 32 19.20 26.20 15.90 12.94\n64 19.63 26.39 16.35 13.06\nMemory Attention + PETest 32 19.45 29.53 15.86 -\n64 20.36 30.61 17.03 -\nV alid 32 19.35 26.22 16.31 12.97\n64 19.73 27.31 16.91 13.25\nAttention Test - 19.19 30.99 17.34 -\nV alid - 18.61 28.13 17.16 13.76\nTable 2: BLEU scores on WMT\u201917 translation datasets from the memory attention models and regular attention\nbaselines. We picked the best out of the four scoring function combinations on the validation set.\n\nNote that\nen-tr does not have an official test set.\n\nBest test scores on each dataset are highlighted. Model Decoding Time (s)\nK=32 26.85\nK=64 27.13\nAttention 33.28\nTable 3: Decoding time, averaged across 10 runs, for\nthe en-de validation set (2169 examples) with average\nsequence length of 35. Results are similar for both\nPE and non-PE models. vocabulary of 16k. We measured decoding time over\nthe full validation set, not including time used for\nmodel setup and data loading, averaged across 10 runs. The average sequence length for examples in this data\nwas 35, and we expect more significant speedups\nfor tasks with longer sequences, as suggested by our\nexperiments on toy data. Note that in our NMT ex-\namples/experiments,K\u2248T, but we obtain computa-\ntional savings from the fact thatK\u226aD. We may be\nable to setK\u226aT, as in toy copying, and still get very\ngood performance in other tasks. For instance, in sum-\nmarization the source is complex but the representa-\ntion of the source required to perform the task is \u201dsim-\nple\u201d (i.e. all that is needed to generate the abstract). Figure 5 shows the effect of using sigmoid and\nsoftmax function in the encoders and decoders. We\nfound that softmax/softmax consistently performs\nbadly, while all other combinations perform about\nequally well. We report results for the best combi-\nnation only (as chosen on the validation set), but we\nfound this choice to only make a minor difference. K\nK\nK\nK\nFigure 5: Comparing training curves for en-fi for\ndifferent encoder/decoder scoring functions for our\nmodels atK=64. 5 Visualizing Attention\nA useful property of the standard attention mechanism\nis that it produces meaningful alignment between\nsource and target sequences. Often, the attention\nmechanism learns to progressively focus on the\nnext source token as it decodes the target. These\nvisualizations can be an important tool in debugging\nand evaluating seq2seq models and are often used for\nunknown token replacement. This raises the question of whether or not our\nproposed memory attention mechanism also learns\nto generate meaningful alignments. Due to limiting\nthe number of attention contexts to a number that\nis generally less than the sequence length, it is not\nimmediately obvious what each context would learn\nto focus on. Our hope was that the model would learn\nto focus on multiple alignments at the same time,\nwithin the same attention vector. For example, if the\nsource sequence is of length 40 and we haveK=10\nattention contexts, we would hope thatC1 roughly fo-\ncuses on tokens 1 to 4,C2 on tokens 5 to 8, and so on. Figures 6 and 7 show that this is indeed the case. To\ngenerate this visualization we multiply the attention\nscores\u03b1and \u03b2from the encoder and decoder. Figure\n8 shows a sample translation task visualization. Figure 6 suggests that our model learns distinct\nways to use its memory depending on the encoder and\ndecoder functions. Interestingly, using softmax nor-\nmalization results in attention maps typical of those de-\nrived from using standard attention, i.e. a relatively lin-\near mapping between source and target tokens. Mean-\nwhile, using sigmoid gating results in what seems to\nbe a distributed representation of the source sequences\nacross encoder time steps, with multiple contiguous at-\ntention contexts being accessed at each decoding step. 6 Related Work\nOur contributions build on previous work in making\nseq2seq models more computationally efficient. Luong et al. (2015) introduce various attention mech-\nanisms that are computationally simpler and perform\nas well or better than the original one presented in\nBahdanau et al. (2014). However, these typically still\nrequireO(D2) computation complexity, or lack the\nflexibility to look at the full source sequence. Efficient\nlocation-based attention (Xu et al., 2015) has also\nbeen explored in the image recognition domain. Wu et al. (2016) presents several enhancements to\nthe standard seq2seq architecture that allow more effi-\ncient computation on GPUs, such as only attending on\nthe bottom layer. Kalchbrenner et al. (2016) propose\na linear time architecture based on stacked convolu-\ntional neural networks. Gehring et al. (2016) also\npropose the use of convolutional encoders to speed up\nNMT. de Br\u00b4ebisson and Vincent (2016) propose a lin-\near attention mechanism based on covariance matrices\napplied to information retrieval. Raffel et al. (2017)\nenable online linear time attention calculation by en-\nforcing that the alignment between input and output\nsequence elements be monotonic. Previously, mono-\ntonic attention was proposed for morphological inflec-\ntion generation by Aharoni and Goldberg (2016). 7 Conclusion\nIn this work, we propose a novel memory-based\nattention mechanism that results in a linear compu-\ntational time ofO(KD(|S|+|T|)) during decoding\nin seq2seq models. Through a series of experiments,\nwe demonstrate that our technique leads to consistent\ninference speedups as sequences get longer, and\ncan fit complex data distributions such as those\nfound in Neural Machine Translation. We show\nthat our attention mechanism learns meaningful\nalignments despite being constrained to a fixed\nrepresentation after encoding. We encourage future\nwork that explores the optimal values ofK for\nvarious language tasks and examines whether or not\nit is possible to predictKbased on the task at hand. We also encourage evaluating our models on other\ntasks that must deal with long sequences but have\ncompact representations, such as summarization and\nquestion-answering, and further exploration of their\neffect on memory and training speed. Figure 6: Attention scores at each step of decoding for on a sample from the sequence length 100 toy copy\ndataset. Individual attention vectors are highlighted in blue. (y-axis: source tokens;x-axis: target tokens)\nK\nK\nK\nK\nFigure 7: Attention scores at each step of decoding forK= 4on a sample with sequence length 11. The\nsubfigure on the left color codes each individual attention vector. (y-axis: source;x-axis: target)\nC1\n C2\n C3\n C4\n C5\n C6\n C7\n C8\nC9\n C10\n C11\n C12\n C13\n C14\n C15\n C16\nC17\n C18\n C19\n C20\n C21\n C22\n C23\n C24\nC25\n C26\n C27\n C28\n C29\n C30\n C31\n C32\nFigure 8: Attention scores at each step of decoding for en-de WMT translation task using model with sigmoid\nscoring functions andK= 32. The left subfigure displays each individual attention vector separately while\nthe right subfigure displays the full combined attention. (y-axis: source;x-axis: target)\n\nReferences\nRoee Aharoni and Y oav Goldberg. 2016. Mor-\nphological inflection generation with hard\nmonotonic attention. CoRR abs/1611.01487. http://arxiv.org/abs/1611.01487. Dzmitry Bahdanau, Kyunghyun Cho, and Y oshua\nBengio. 2014. Neural machine translation by jointly\nlearning to align and translate.CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473. Denny Britz, Anna Goldie, Thang Luong, and Quoc\nLe. 2017. Massive Exploration of Neural Machine\nTranslation Architectures. CoRR abs/1703.03906. http://arxiv.org/abs/1703.03906. Michael Carl, Barbara Dragsted, and Arnt Lykke Jakob-\nsen.",
            1.0221312909999938
          ]
        }
      ],
      "reference_answers": [
        "standard parametrized attention and a non-attention baseline",
        "Sequence Copy Task and WMT'17"
      ]
    },
    "1706.09673.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.011084698091910018,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.07017543859649122,
          "recall": 0.057971014492753624,
          "f1": 0.0634920634920635
        },
        "rouge-2": {
          "precision": 0.018518518518518517,
          "recall": 0.015151515151515152,
          "f1": 0.016666666666666666
        },
        "rouge-L": {
          "precision": 0.07017543859649122,
          "recall": 0.057971014492753624,
          "f1": 0.0634920634920635
        }
      },
      "per_question_metrics": [
        {
          "question": "Which dataset do they use?",
          "generated_answer": "Based on the provided context, the specific dataset used is not mentioned.",
          "reference_answer": " Paraphrase Database (PPDB) ,  book corpus",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            }
          },
          "bleu_score": 0.01428363257865929,
          "exact_match": 0.0
        },
        {
          "question": "Do they evaluate their learned representations on downstream tasks?",
          "generated_answer": "Yes, they evaluate their learned representations on downstream tasks.",
          "reference_answer": "No No",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            }
          },
          "bleu_score": 0,
          "exact_match": 0.0
        },
        {
          "question": "How do they encourage understanding of literature as part of their objective function?",
          "generated_answer": "The provided context does not specifically address how understanding of literature is encouraged as part of their objective function.",
          "reference_answer": "They group the existing works in terms of the objective function they optimize - within-tweet relationships, inter-tweet relationships, autoencoder, and weak supervision.",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.21052631578947367,
              "recall": 0.17391304347826086,
              "f1": 0.1904761904761905
            },
            "rouge-2": {
              "precision": 0.05555555555555555,
              "recall": 0.045454545454545456,
              "f1": 0.049999999999999996
            },
            "rouge-L": {
              "precision": 0.21052631578947367,
              "recall": 0.17391304347826086,
              "f1": 0.1904761904761905
            }
          },
          "bleu_score": 0.018970461697070763,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "Which dataset do they use?",
        "Do they evaluate their learned representations on downstream tasks?",
        "How do they encourage understanding of literature as part of their objective function?"
      ],
      "generated_answers": [
        {
          "answer": "Based on the provided context, the specific dataset used is not mentioned.",
          "relevant_chunks": [
            "2003. Latent Dirichlet Allocation. Journal of Ma-\nchine Learning Research 3:993\u20131022. Tao Chen, Dongyuan Lu, Min-Yen Kan, and Peng Cui. 2013. Understanding and classifying image tweets. In ACM Multimedia Conference, MM \u201913 . pages\n781\u2013784. Bhuwan Dhingra, Zhong Zhou, Dylan Fitzpatrick,\nMichael Muehl, and William W. Cohen.\n\n2017. Improving tweet representations using temporal and\nuser context. In Advances in Information Retrieval\n- 39th European Conference on IR Research, ECIR. pages 575\u2013581. Juri Ganitkevitch, Benjamin Van Durme, and Chris\nCallison-Burch. 2013. PPDB: the paraphrase\ndatabase. In Proc. of conf. of the North American\nChapter of the Association of Computational Lin-\nguistics. pages 758\u2013764. Anastasia Giachanou and Fabio Crestani. 2016. Like\nit or not: A Survey of Twitter Sentiment Anal-\nysis Methods. ACM Computing Surveys (CSUR)\n49(2):28. Zellig S Harris.\n\n2015. Skip-Thought Vec-\ntors. In Proc. of the 2015 Intl. Conf. on Advances\nin Neural Information Processing Systems . pages\n3294\u20133302. Angeliki Lazaridou, Nghia The Pham, and Marco Ba-\nroni. 2015. Combining language and vision with a\nmultimodal skip-gram model. In Proc. of conf. of\nthe North American Chapter of the Association of\nComputational Linguistics. pages 153\u2013163. Quoc V . Le and Tomas Mikolov. 2014. Distributed\nRepresentations of Sentences and Documents. In\nProc. of the 31st Intl. Conf. on Machine Learning . pages 1188\u20131196. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013. Distributed Repre-\nsentations of Words and Phrases and Their Compo-\nsitionality. In Proc. of the 26th Intl. Conf. on Neural\nInformation Processing Systems. pages 3111\u20133119. Tamara Polajnar, Laura Rimell, and Stephen Clark. 2015. An exploration of discourse-based sentence\nspaces for compositional distributional semantics. In Workshop on Linking Models of Lexical, Sen-\ntential and Discourse-level Semantics (LSDSem) . page 1. Yafeng Ren, Yue Zhang, Meishan Zhang, and\nDonghong Ji. 2016. Context-sensitive twitter senti-\nment classi\ufb01cation using neural network. In Proc. of\nthe 13th AAAI Conference on Arti\ufb01cial Intelligence. pages 215\u2013221. Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,\nand Gr \u00b4egoire Mesnil. 2014. A Latent Semantic\nModel with Convolutional-Pooling Structure for In-\nformation Retrieval. In Proc. of the 23rd ACM Intl. Conf. on Conference on Information and Knowledge\nManagement. pages 101\u2013110. Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,\nand Rob Fergus. 2015. End-to-end memory net-\nworks. In NIPS. pages 2440\u20132448. Ilya Sutskever, Oriol Vinyals, and Quoc V . Le.",
            0.42260545900001034
          ]
        },
        {
          "answer": "Yes, they evaluate their learned representations on downstream tasks.",
          "relevant_chunks": [
            "2015. Skip-Thought Vec-\ntors. In Proc. of the 2015 Intl. Conf. on Advances\nin Neural Information Processing Systems . pages\n3294\u20133302. Angeliki Lazaridou, Nghia The Pham, and Marco Ba-\nroni. 2015. Combining language and vision with a\nmultimodal skip-gram model. In Proc. of conf. of\nthe North American Chapter of the Association of\nComputational Linguistics. pages 153\u2013163. Quoc V . Le and Tomas Mikolov. 2014. Distributed\nRepresentations of Sentences and Documents. In\nProc. of the 31st Intl. Conf. on Machine Learning . pages 1188\u20131196. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013. Distributed Repre-\nsentations of Words and Phrases and Their Compo-\nsitionality. In Proc. of the 26th Intl. Conf. on Neural\nInformation Processing Systems. pages 3111\u20133119. Tamara Polajnar, Laura Rimell, and Stephen Clark. 2015. An exploration of discourse-based sentence\nspaces for compositional distributional semantics. In Workshop on Linking Models of Lexical, Sen-\ntential and Discourse-level Semantics (LSDSem) . page 1. Yafeng Ren, Yue Zhang, Meishan Zhang, and\nDonghong Ji. 2016. Context-sensitive twitter senti-\nment classi\ufb01cation using neural network. In Proc. of\nthe 13th AAAI Conference on Arti\ufb01cial Intelligence. pages 215\u2013221. Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,\nand Gr \u00b4egoire Mesnil. 2014. A Latent Semantic\nModel with Convolutional-Pooling Structure for In-\nformation Retrieval. In Proc. of the 23rd ACM Intl. Conf. on Conference on Information and Knowledge\nManagement. pages 101\u2013110. Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,\nand Rob Fergus. 2015. End-to-end memory net-\nworks. In NIPS. pages 2440\u20132448. Ilya Sutskever, Oriol Vinyals, and Quoc V . Le.\n\nImproving Distributed Representations of Tweets - Present and Future\nGanesh J\nInformation Retrieval and Extraction Laboratory\nIIIT Hyderabad\nTelangana, India\nganesh.j@research.iiit.ac.in\nAbstract\nUnsupervised representation learning for\ntweets is an important research \ufb01eld which\nhelps in solving several business applica-\ntions such as sentiment analysis, hashtag\nprediction, paraphrase detection and mi-\ncroblog ranking. A good tweet represen-\ntation learning model must handle the id-\niosyncratic nature of tweets which poses\nseveral challenges such as short length, in-\nformal words, unusual grammar and mis-\nspellings. However, there is a lack of\nprior work which surveys the represen-\ntation learning models with a focus on\ntweets. In this work, we organize the mod-\nels based on its objective function which\naids the understanding of the literature. We also provide interesting future direc-\ntions, which we believe are fruitful in ad-\nvancing this \ufb01eld by building high-quality\ntweet representation learning models. 1 Introduction\nTwitter is a widely used microblogging platform,\nwhere users post and interact with messages,\n\u201ctweets\u201d. Understanding the semantic represen-\ntation of tweets can bene\ufb01t a plethora of applica-\ntions such as sentiment analysis (Ren et al., 2016;\nGiachanou and Crestani, 2016), hashtag predic-\ntion (Dhingra et al., 2016), paraphrase detec-\ntion (V osoughi et al., 2016) and microblog rank-\ning (Huang et al., 2013; Shen et al., 2014). How-\never, tweets are dif\ufb01cult to model as they pose\nseveral challenges such as short length, informal\nwords, unusual grammar and misspellings. Re-\ncently, researchers are focusing on leveraging un-\nsupervised representation learning methods based\non neural networks to solve this problem. Once\nthese representations are learned, we can use off-\nthe-shelf predictors taking the representation as in-\nput to solve the downstream task (Bengio, 2013a;\nBengio et al., 2013b). These methods enjoy sev-\neral advantages: (1) they are cheaper to train, as\nthey work with unlabelled data, (2) they reduce the\ndependence on domain level experts, and (3) they\nare highly effective across multiple applications,\nin practice. Despite this, there is a lack of prior work which\nsurveys the tweet-speci\ufb01c unsupervised represen-\ntation learning models. In this work, we attempt to\n\ufb01ll this gap by investigating the models in an or-\nganized fashion. Speci\ufb01cally, we group the mod-\nels based on the objective function it optimizes. We believe this work can aid the understanding\nof the existing literature. We conclude the pa-\nper by presenting interesting future research direc-\ntions, which we believe are fruitful in advancing\nthis \ufb01eld by building high-quality tweet represen-\ntation learning models. 2 Unsupervised Tweet Representation\nModels\nThere are various models spanning across differ-\nent model architectures and objective functions in\nthe literature to compute tweet representation in\nan unsupervised fashion. These models work in a\nsemi-supervised way - the representations gener-\nated by the model is fed to an off-the-shelf pre-\ndictor like Support Vector Machines (SVM) to\nsolve a particular downstream task. These mod-\nels span across a wide variety of neural network\nbased architectures including average of word vec-\ntors, convolutional-based, recurrent-based and so\non. We believe that the performance of these mod-\nels is highly dependent on the objective function\nit optimizes \u2013 predicting adjacent word (within-\ntweet relationships), adjacent tweet (inter-tweet\nrelationships), the tweet itself (autoencoder), mod-\narXiv:1706.09673v1  [cs.CL]  29 Jun 2017\n\nUnsupervised Tweet Representation Models\nModeling \nwithin-tweet \nrelationships\nModeling\ninter-tweet \nrelationships\nParagraph2Vec \n(ICML\u201914)\nConceptual SE \n(ACL\u201916)\nModeling from \nstructured \nresources\nModeling as an \nautoencoder\nModeling using \nweak \nsupervision\nSkip-thought \n(NIPS\u201915)\nFastSent\n(NAACL\u201916)\nSiamese CBOW \n(ACL\u201916)\nGanesh et al. (ECIR\u201917)\nCHARAGRAM \n(EMNLP\u201916)\nWieting et al. (ICLR\u201916)\nSDAE \n(NAACL\u201916)\nTweet2Vec \n(SIGIR\u201916) \nSSWE \n(TKDE\u201916)\nTwee2Vec \n(ACL\u201916)\nFigure 1: Unsupervised Tweet Representation Models Hierarchy based on Optimized Objective Function\neling from structured resources like paraphrase\ndatabases and weak supervision. In this section,\nwe provide the \ufb01rst of its kind survey of the recent\ntweet-speci\ufb01c unsupervised models in an orga-\nnized fashion to understand the literature. Specif-\nically, we categorize each model based on the op-\ntimized objective function as shown in Figure 1. Next, we study each category one by one. 2.1 Modeling within-tweet relationships\nMotivation: Every tweet is assumed to have a la-\ntent topic vector, which in\ufb02uences the distribution\nof the words in the tweet. For example, though\nthe appearance of the phrase catch the ball is fre-\nquent in the corpus, if we know that the topic of a\ntweet is about \u201ctechnology\u201d, we can expect words\nsuch as bug or exception after the word catch (ig-\nnoring the) instead of the word ball since catch\nthe bug/exceptionis more plausible under the topic\n\u201ctechnology\u201d. On the other hand, if the topic of\nthe tweet is about \u201csports\u201d, then we can expect\nball after catch. These intuitions indicate that the\nprediction of neighboring words for a given word\nstrongly relies on the tweet also. Models: (Le and Mikolov, 2014)\u2019s work is the \ufb01rst\nto exploit this idea to compute distributed docu-\nment representations that are good at predicting\nwords in the document. They propose two mod-\nels: PV-DM and PV-DBOW, that are extensions\nof Continuous Bag Of Words (CBOW) and Skip-\ngram model variants of the popular Word2Vec\nmodel (Mikolov et al., 2013) respectively \u2013 PV-\nDM inserts an additional document token (which\ncan be thought of as another word) which is shared\nacross all contexts generated from the same doc-\nument; PV-DBOW attempts to predict the sam-\npled words from the document given the document\nrepresentation. Although originally employed for\nparagraphs and documents, these models work\nbetter than the traditional models: BOW (Harris,\n1954) and LDA (Blei et al., 2003) for tweet classi-\n\ufb01cation and microblog retrieval tasks (Wang et al.,\n2016). The authors in (Wang et al., 2016) make the\nPV-DM and PV-DBOW modelsconcept-aware (a\nrich semantic signal from a tweet) by augmenting\ntwo features: attention over contextual words and\nconceptual tweet embedding, which jointly exploit\nconcept-level senses of tweets to compute better\nrepresentations. Both the discussed works have\nthe following characteristics: (1) they use a shal-\nlow architecture, which enables fast training, (2)\ncomputing representations for test tweets requires\ncomputing gradients, which is time-consuming for\nreal-time Twitter applications, and (3) most impor-\ntantly, they fail to exploit textual information from\nrelated tweets that can bear salient semantic sig-\nnals. 2.2 Modeling inter-tweet relationships\nMotivation: To capture rich tweet semantics,\nresearchers are attempting to exploit a type of\nsentence-level Distributional Hypothesis (Harris,\n1954; Polajnar et al., 2015). The idea is to infer the\ntweet representation from the content of adjacent\ntweets in a related stream like users\u2019 Twitter time-\nline, topical, retweet and conversational stream. This approach signi\ufb01cantly alleviates the context\ninsuf\ufb01ciency problem caused due to the ambigu-\nous and short nature of tweets (Ren et al., 2016;\nGanesh et al., 2017). Models: Skip-thought vectors (Kiros et al., 2015)\n(STV) is a widely popular sentence encoder,\nwhich is trained to predict adjacent sentences in\nthe book corpus (Zhu et al., 2015). Although the\ntesting is cheap as it involves a cheap forward\npropagation of the test sentence, STV is very slow\nto train thanks to its complicated model architec-\nture. To combat this computational inef\ufb01ciency,\nFastSent (Hill et al., 2016) propose a simple ad-\nditive (log-linear) sentence model, which predicts\nadjacent sentences (represented as BOW) taking\nthe BOW representation of some sentence in con-\ntext. This model can exploit the same signal, but\nat a much lower computational expense. Paral-\nlel to this work, Siamase CBOW (Kenter et al.,\n2016) develop a model which directly compares\nthe BOW representation of two sentence to bring\nthe embeddings of a sentence closer to its adja-\ncent sentence, away from a randomly occurring\nsentence in the corpus. For FastSent and Siamese\nCBOW, the test sentence representation is a sim-\nple average of word vectors obtained after train-\ning. Both of these models are general purpose\nsentence representation models trained on book\ncorpus, yet give a competitive performance over\nprevious models on the tweet semantic similarity\ncomputation task. (Ganesh et al., 2017)\u2019s model\nattempt to exploit these signals directly from Twit-\nter. With the help of attention technique and\nlearned user representation, this log-linear model\nis able to capture salient semantic information\nfrom chronologically adjacent tweets of a target\ntweet in users\u2019 Twitter timeline. 2.3 Modeling from structured resources\nMotivation: In recent times, building represen-\ntation models based on supervision from richly\nstructured resources such as Paraphrase Database\n(PPDB) (Ganitkevitch et al., 2013) (containing\nnoisy phrase pairs) has yielded high quality sen-\ntence representations. These methods work by\nmaximizing the similarity of the sentences in the\nlearned semantic space. Models: CHARAGRAM (Wieting et al., 2016a)\nembeds textual sequences by learning a character-\nbased compositional model that involves addition\nof the vectors of its character n-grams followed by\nan elementwise nonlinearity. This simpler archi-\ntecture trained on PPDB is able to beat models\nwith complex architectures like CNN, LSTM on\nSemEval 2015 Twitter textual similarity task by a\nlarge margin. This result emphasizes the impor-\ntance of character-level models that address differ-\nences due to spelling variation and word choice. The authors in their subsequent work (Wieting\net al., 2016b) conduct a comprehensive analysis\nof models spanning the range of complexity from\nword averaging to LSTMs for its ability to do\ntransfer and supervised learning after optimizing a\nmargin based loss on PPDB. For transfer learning,\nthey \ufb01nd models based on word averaging perform\nwell on both the in-domain and out-of-domain tex-\ntual similarity tasks, beating LSTM model by a\nlarge margin. On the other hand, the word averag-\ning models perform well for both sentence similar-\nity and textual entailment tasks, outperforming the\nLSTM. However, for sentiment classi\ufb01cation task,\nthey \ufb01nd LSTM (trained on PPDB) to beat the av-\neraging models to establish a new state of the art. The above results suggest that structured resources\nplay a vital role in computing general-purpose em-\nbeddings useful in downstream applications. 2.4 Modeling as an autoencoder\nMotivation: The autoencoder based approach\nlearns latent (or compressed) representation by re-\nconstructing its own input. Since textual data like\ntweets contain discrete input signals, sequence-\nto-sequence models (Sutskever et al., 2014) like\nSTV can be used to build the solution. The en-\ncoder model which encodes the input tweet can\ntypically be a CNN (Kim, 2014), recurrent models\nlike RNN, GRU, LSTM (Karpathy et al., 2015) or\nmemory networks (Sukhbaatar et al., 2015). The\ndecoder model which generates the output tweet\ncan typically be a recurrent model that predicts a\noutput token at every time step. Models: Sequential Denoising Autoencoders\n(SDAE) (Hill et al., 2016) is a LSTM-based\nsequence-to-sequence model, which is trained to\n\nrecover the original data from the corrupted ver-\nsion. SDAE produces robust representations by\nlearning to represent the data in terms of fea-\ntures that explain its important factors of varia-\ntion. Tweet2Vec (V osoughi et al., 2016) is a recent\nmodel which uses a character-level CNN-LSTM\nencoder-decoder architecture trained to construct\nthe input tweet directly. This model outper-\nforms competitive models that work on word-level\nlike PV-DM, PV-DBOW on semantic similarity\ncomputation and sentiment classi\ufb01cation tasks,\nthereby showing that the character-level nature of\nTweet2Vec is best-suited to deal with the noise and\nidiosyncrasies of tweets. Tweet2Vec controls the\ngeneralization error by using a data augmentation\ntechnique, wherein tweets are replicated and some\nof the words in the replicated tweets are replaced\nwith their synonyms. Both SDAE and Tweet2Vec\nhas the advantage that they don\u2019t need a coherent\ninter-sentence narrative (like STV), which is hard\nto obtain in Twitter. 2.5 Modeling using weak supervision\nMotivation: In a weakly supervised setup, we\ncreate labels for a tweet automatically and pre-\ndict them to learn potentially sophisticated mod-\nels than those obtained by unsupervised learning\nalone. Examples of labels include sentiment of\nthe overall tweet, words like hashtag present in\nthe tweet and so on. This technique can create a\nhuge labeled dataset especially for building data-\nhungry, sophisticated deep learning models. Models: (Tang et al., 2016) learns sentiment-\nspeci\ufb01c word embedding (SSWE), which encodes\nthe polarity information in the word representa-\ntions so that words with contrasting polarities and\nsimilar syntactic context (like good and bad) are\npushed away from each other in the semantic\nspace that it learns. SSWE utilizes the massive\ndistant-supervised tweets collected by positive and\nnegative emoticons to build a powerful tweet rep-\nresentation, which are shown to be useful in tasks\nsuch as sentiment classi\ufb01cation and word simi-\nlarity computation in sentiment lexicon. (Dhin-\ngra et al., 2016) observes that hashtags in tweets\ncan be considered as topics and hence tweets with\nsimilar hashtags must come closer to each other. Their model predicts the hashtags by using a Bi-\nGRU layer to embed the tweets from its charac-\nters. Due to subword modeling, such character-\nlevel models can approximate the representations\nfor rare words and new words (words not seen dur-\ning training) in the test tweets really well. This\nmodel outperforms the word-level baselines for\nhashtag prediction task, thereby concluding that\nexploring character-level models for tweets is a\nworthy research direction to pursue. Both these\nworks fail to study the model\u2019s generality (Weston\net al., 2014), i.e., the ability of the model to trans-\nfer the learned representations to diverse tasks. 3 Future Directions\nIn this section we present the future research di-\nrections which we believe can be worth pursuing\nto generate high quality tweet embeddings. \u2022 (Ren et al., 2016) propose a supervised\nneural network utilizing contextualized fea-\ntures from conversation, author and topic\nbased context about a target tweet to per-\nform well in classi\ufb01cation of tweet. Apart\nfrom (Ganesh et al., 2017)\u2019s work which uti-\nlizes author context, there is no other work\nwhich builds unsupervised tweet representa-\ntion model on Twitter-speci\ufb01c contexts such\nas conversation and topical streams. We be-\nlieve such a solution directly exploits seman-\ntic signals (or nuances) from Twitter, unlike\nSTV or Siamese CBOW which are trained on\nbooks corpus. \u2022 (dos Santos and Gatti, 2014) propose a super-\nvised, hybrid model exploiting both the char-\nacter and word level information for Twit-\nter sentiment analysis task. Since the set-\ntings when the character level model beats the\nword level model is not well understood yet,\nwe believe it would be interesting to explore\nsuch a hybrid compositional model to build\nunsupervised tweet representations. \u2022 Twitter provides a platform for the users to\ninteract with other users. To the best of our\nknowledge, there is no related work that com-\nputes unsupervised tweet representation by\nexploiting the user pro\ufb01le attributes like pro-\n\ufb01le picture, user biography and set of fol-\nlowers, and social interactions like retweet\ncontext (set of surrounding tweets in a users\nretweet stream) and favorite context (set of\nsurrounding tweets in a users favorite tweet\nstream). \u2022 DSSM (Huang et al., 2013; Shen et al., 2014)\npropose a family of deep models that are\ntrained to maximize the relevance of clicked\ndocuments given a query. Such a ranking loss\nfunction helps the model cater to a wide vari-\nety of applications1 such as web search rank-\ning, ad selection/relevance, question answer-\ning, knowledge inference and machine trans-\nlation. We observe such a loss function has\nnot been explored for building unsupervised\ntweet representations. We believe employing\na ranking loss directly on tweets using a large\nscale microblog dataset 2 can result in repre-\nsentations which can be useful to Twitter ap-\nplications beyond those studied in the tweet\nrepresentation learning literature. \u2022 Linguists assume that language is best un-\nderstood as a hierarchical tree of phrases,\nrather than a \ufb02at sequence of words or charac-\nters. It\u2019s dif\ufb01cult to get the syntactic trees for\ntweets as most of them are not grammatically\ncorrect. The average of word vectors model\nhas the most simplest compositional archi-\ntecture with no additional parameters, yet\ndisplays a strong performance outperforming\ncomplex architectures such as CNN, LSTM\nand so on for several downstream applica-\ntions (Wieting et al., 2016a,b). We believe\na theoretical understanding of why word av-\neraging models perform well can help in em-\nbracing these models by linguists. \u2022 Models in (Wieting et al., 2016a,b) learn\nfrom noisy phrase pairs of PPDB. Note that\nthe source of the underlying texts is com-\npletely different from Twitter. It can be inter-\nesting to see the effectiveness of such mod-\nels when directly trained on structural re-\nsources from Twitter like Twitter Paraphrase\nCorpus (Xu et al., 2014). The main challenge\nwith this approach is the small size of the an-\nnotated Twitter resources, which can encour-\nage models like (Arora et al., 2017) that work\nwell even when the training data is scarce or\nnonexistent. \u2022 Tweets mostly have an accompanying im-\nage which sometimes has visual correspon-\ndence with its textual content (Chen et al.,\n1https://www.microsoft.com/en-us/\nresearch/project/dssm/\n2http://trec.nist.gov/\n2013; Wang et al., 2014) (\u2018visual\u2019 tweet). To the best of our knowledge, there is no\nwork which explores the following question:\ncan we build multimodal representations for\ntweets accompanying correlated visual con-\ntent and compare with traditional bench-\nmarks?. We can leverage insights from mul-\ntimodal skip-gram model (Lazaridou et al.,\n2015) which builds multimodally-enhanced\nword vectors that perform well in the tradi-\ntional semantic benchmarks. However, it\u2019s\nhard to detect visual tweets and learning from\na non-visual tweet can degrade its tweet rep-\nresentation. It would be interesting to see if\na dispersion metric (Kiela et al., 2014) for\ntweets can be explored to overcome this prob-\nlem of building a nondegradable, improved\ntweet representation. \u2022 Interpreting the tweet representations to un-\nearth the encoded features responsible for\nits performance on a downstream task is an\nimportant, but a less studied research area. (Ganesh et al., 2016)\u2019s work is the \ufb01rst to\nopen the blackbox of vector embeddings for\ntweets. They propose elementary property\nprediction tasks which predicts the accuracy\nto which a given tweet representation en-\ncodes the elementary property (like slang\nwords, hashtags, mentions, etc). The main\ndrawback of the work is that they fail to cor-\nrelate their study with downstream applica-\ntions. We believe performing such a correla-\ntion study can clearly highlight the set of el-\nementary features behind the performance of\na particular representation model over other\nfor a given downstream task. 4 Conclusion\nIn this work we study the problem of learning\nunsupervised tweet representations. We believe\nour survey of the existing works based on the ob-\njective function can give vital perspectives to re-\nsearchers and aid their understanding of the \ufb01eld. We also believe the future research directions stud-\nied in this work can help in breaking the barriers in\nbuilding high quality, general purpose tweet repre-\nsentation models. References\nSanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A simple but tough-to-beat baseline for sentence em-\nbeddings. CoRR . Yoshua Bengio. 2013a. Deep Learning of Represen-\ntations: Looking Forward. In Proc. of the 1st Intl. Conf. on Statistical Language and Speech Process-\ning. pages 1\u201337. Yoshua Bengio, Aaron C. Courville, and Pascal Vin-\ncent. 2013b. Representation Learning: A Review\nand New Perspectives. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence 35(8):1798\u2013\n1828. David M. Blei, Andrew Y . Ng, and Michael I. Jordan.\n\n2013. Learning\nDeep Structured Semantic Models for Web Search\nusing Clickthrough Data. In Proc. of the 22nd ACM\nIntl. Conf. on Information and Knowledge Manage-\nment. pages 2333\u20132338. Andrej Karpathy, Justin Johnson, and Fei-Fei Li. 2015. Visualizing and understanding recurrent networks. CoRR abs/1506.02078. Tom Kenter, Alexey Borisov, and Maarten de Rijke.",
            0.4166559589999679
          ]
        },
        {
          "answer": "The provided context does not specifically address how understanding of literature is encouraged as part of their objective function.",
          "relevant_chunks": [
            "2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In IEEE Intl.\n\n1954. Distributional Structure. Word\n10(2-3):146\u2013162. Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning Distributed Representations of Sentences\nfrom Unlabelled Data. In Proc. of the Conf. of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies. pages 1367\u20131377. Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\nAlex Acero, and Larry P. Heck.\n\nImproving Distributed Representations of Tweets - Present and Future\nGanesh J\nInformation Retrieval and Extraction Laboratory\nIIIT Hyderabad\nTelangana, India\nganesh.j@research.iiit.ac.in\nAbstract\nUnsupervised representation learning for\ntweets is an important research \ufb01eld which\nhelps in solving several business applica-\ntions such as sentiment analysis, hashtag\nprediction, paraphrase detection and mi-\ncroblog ranking. A good tweet represen-\ntation learning model must handle the id-\niosyncratic nature of tweets which poses\nseveral challenges such as short length, in-\nformal words, unusual grammar and mis-\nspellings. However, there is a lack of\nprior work which surveys the represen-\ntation learning models with a focus on\ntweets. In this work, we organize the mod-\nels based on its objective function which\naids the understanding of the literature. We also provide interesting future direc-\ntions, which we believe are fruitful in ad-\nvancing this \ufb01eld by building high-quality\ntweet representation learning models. 1 Introduction\nTwitter is a widely used microblogging platform,\nwhere users post and interact with messages,\n\u201ctweets\u201d. Understanding the semantic represen-\ntation of tweets can bene\ufb01t a plethora of applica-\ntions such as sentiment analysis (Ren et al., 2016;\nGiachanou and Crestani, 2016), hashtag predic-\ntion (Dhingra et al., 2016), paraphrase detec-\ntion (V osoughi et al., 2016) and microblog rank-\ning (Huang et al., 2013; Shen et al., 2014). How-\never, tweets are dif\ufb01cult to model as they pose\nseveral challenges such as short length, informal\nwords, unusual grammar and misspellings. Re-\ncently, researchers are focusing on leveraging un-\nsupervised representation learning methods based\non neural networks to solve this problem. Once\nthese representations are learned, we can use off-\nthe-shelf predictors taking the representation as in-\nput to solve the downstream task (Bengio, 2013a;\nBengio et al., 2013b). These methods enjoy sev-\neral advantages: (1) they are cheaper to train, as\nthey work with unlabelled data, (2) they reduce the\ndependence on domain level experts, and (3) they\nare highly effective across multiple applications,\nin practice. Despite this, there is a lack of prior work which\nsurveys the tweet-speci\ufb01c unsupervised represen-\ntation learning models. In this work, we attempt to\n\ufb01ll this gap by investigating the models in an or-\nganized fashion. Speci\ufb01cally, we group the mod-\nels based on the objective function it optimizes. We believe this work can aid the understanding\nof the existing literature. We conclude the pa-\nper by presenting interesting future research direc-\ntions, which we believe are fruitful in advancing\nthis \ufb01eld by building high-quality tweet represen-\ntation learning models. 2 Unsupervised Tweet Representation\nModels\nThere are various models spanning across differ-\nent model architectures and objective functions in\nthe literature to compute tweet representation in\nan unsupervised fashion. These models work in a\nsemi-supervised way - the representations gener-\nated by the model is fed to an off-the-shelf pre-\ndictor like Support Vector Machines (SVM) to\nsolve a particular downstream task. These mod-\nels span across a wide variety of neural network\nbased architectures including average of word vec-\ntors, convolutional-based, recurrent-based and so\non. We believe that the performance of these mod-\nels is highly dependent on the objective function\nit optimizes \u2013 predicting adjacent word (within-\ntweet relationships), adjacent tweet (inter-tweet\nrelationships), the tweet itself (autoencoder), mod-\narXiv:1706.09673v1  [cs.CL]  29 Jun 2017\n\nUnsupervised Tweet Representation Models\nModeling \nwithin-tweet \nrelationships\nModeling\ninter-tweet \nrelationships\nParagraph2Vec \n(ICML\u201914)\nConceptual SE \n(ACL\u201916)\nModeling from \nstructured \nresources\nModeling as an \nautoencoder\nModeling using \nweak \nsupervision\nSkip-thought \n(NIPS\u201915)\nFastSent\n(NAACL\u201916)\nSiamese CBOW \n(ACL\u201916)\nGanesh et al. (ECIR\u201917)\nCHARAGRAM \n(EMNLP\u201916)\nWieting et al. (ICLR\u201916)\nSDAE \n(NAACL\u201916)\nTweet2Vec \n(SIGIR\u201916) \nSSWE \n(TKDE\u201916)\nTwee2Vec \n(ACL\u201916)\nFigure 1: Unsupervised Tweet Representation Models Hierarchy based on Optimized Objective Function\neling from structured resources like paraphrase\ndatabases and weak supervision. In this section,\nwe provide the \ufb01rst of its kind survey of the recent\ntweet-speci\ufb01c unsupervised models in an orga-\nnized fashion to understand the literature. Specif-\nically, we categorize each model based on the op-\ntimized objective function as shown in Figure 1. Next, we study each category one by one. 2.1 Modeling within-tweet relationships\nMotivation: Every tweet is assumed to have a la-\ntent topic vector, which in\ufb02uences the distribution\nof the words in the tweet. For example, though\nthe appearance of the phrase catch the ball is fre-\nquent in the corpus, if we know that the topic of a\ntweet is about \u201ctechnology\u201d, we can expect words\nsuch as bug or exception after the word catch (ig-\nnoring the) instead of the word ball since catch\nthe bug/exceptionis more plausible under the topic\n\u201ctechnology\u201d. On the other hand, if the topic of\nthe tweet is about \u201csports\u201d, then we can expect\nball after catch. These intuitions indicate that the\nprediction of neighboring words for a given word\nstrongly relies on the tweet also. Models: (Le and Mikolov, 2014)\u2019s work is the \ufb01rst\nto exploit this idea to compute distributed docu-\nment representations that are good at predicting\nwords in the document. They propose two mod-\nels: PV-DM and PV-DBOW, that are extensions\nof Continuous Bag Of Words (CBOW) and Skip-\ngram model variants of the popular Word2Vec\nmodel (Mikolov et al., 2013) respectively \u2013 PV-\nDM inserts an additional document token (which\ncan be thought of as another word) which is shared\nacross all contexts generated from the same doc-\nument; PV-DBOW attempts to predict the sam-\npled words from the document given the document\nrepresentation. Although originally employed for\nparagraphs and documents, these models work\nbetter than the traditional models: BOW (Harris,\n1954) and LDA (Blei et al., 2003) for tweet classi-\n\ufb01cation and microblog retrieval tasks (Wang et al.,\n2016). The authors in (Wang et al., 2016) make the\nPV-DM and PV-DBOW modelsconcept-aware (a\nrich semantic signal from a tweet) by augmenting\ntwo features: attention over contextual words and\nconceptual tweet embedding, which jointly exploit\nconcept-level senses of tweets to compute better\nrepresentations. Both the discussed works have\nthe following characteristics: (1) they use a shal-\nlow architecture, which enables fast training, (2)\ncomputing representations for test tweets requires\ncomputing gradients, which is time-consuming for\nreal-time Twitter applications, and (3) most impor-\ntantly, they fail to exploit textual information from\nrelated tweets that can bear salient semantic sig-\nnals. 2.2 Modeling inter-tweet relationships\nMotivation: To capture rich tweet semantics,\nresearchers are attempting to exploit a type of\nsentence-level Distributional Hypothesis (Harris,\n1954; Polajnar et al., 2015). The idea is to infer the\ntweet representation from the content of adjacent\ntweets in a related stream like users\u2019 Twitter time-\nline, topical, retweet and conversational stream. This approach signi\ufb01cantly alleviates the context\ninsuf\ufb01ciency problem caused due to the ambigu-\nous and short nature of tweets (Ren et al., 2016;\nGanesh et al., 2017). Models: Skip-thought vectors (Kiros et al., 2015)\n(STV) is a widely popular sentence encoder,\nwhich is trained to predict adjacent sentences in\nthe book corpus (Zhu et al., 2015). Although the\ntesting is cheap as it involves a cheap forward\npropagation of the test sentence, STV is very slow\nto train thanks to its complicated model architec-\nture. To combat this computational inef\ufb01ciency,\nFastSent (Hill et al., 2016) propose a simple ad-\nditive (log-linear) sentence model, which predicts\nadjacent sentences (represented as BOW) taking\nthe BOW representation of some sentence in con-\ntext. This model can exploit the same signal, but\nat a much lower computational expense. Paral-\nlel to this work, Siamase CBOW (Kenter et al.,\n2016) develop a model which directly compares\nthe BOW representation of two sentence to bring\nthe embeddings of a sentence closer to its adja-\ncent sentence, away from a randomly occurring\nsentence in the corpus. For FastSent and Siamese\nCBOW, the test sentence representation is a sim-\nple average of word vectors obtained after train-\ning. Both of these models are general purpose\nsentence representation models trained on book\ncorpus, yet give a competitive performance over\nprevious models on the tweet semantic similarity\ncomputation task. (Ganesh et al., 2017)\u2019s model\nattempt to exploit these signals directly from Twit-\nter. With the help of attention technique and\nlearned user representation, this log-linear model\nis able to capture salient semantic information\nfrom chronologically adjacent tweets of a target\ntweet in users\u2019 Twitter timeline. 2.3 Modeling from structured resources\nMotivation: In recent times, building represen-\ntation models based on supervision from richly\nstructured resources such as Paraphrase Database\n(PPDB) (Ganitkevitch et al., 2013) (containing\nnoisy phrase pairs) has yielded high quality sen-\ntence representations. These methods work by\nmaximizing the similarity of the sentences in the\nlearned semantic space. Models: CHARAGRAM (Wieting et al., 2016a)\nembeds textual sequences by learning a character-\nbased compositional model that involves addition\nof the vectors of its character n-grams followed by\nan elementwise nonlinearity. This simpler archi-\ntecture trained on PPDB is able to beat models\nwith complex architectures like CNN, LSTM on\nSemEval 2015 Twitter textual similarity task by a\nlarge margin. This result emphasizes the impor-\ntance of character-level models that address differ-\nences due to spelling variation and word choice. The authors in their subsequent work (Wieting\net al., 2016b) conduct a comprehensive analysis\nof models spanning the range of complexity from\nword averaging to LSTMs for its ability to do\ntransfer and supervised learning after optimizing a\nmargin based loss on PPDB. For transfer learning,\nthey \ufb01nd models based on word averaging perform\nwell on both the in-domain and out-of-domain tex-\ntual similarity tasks, beating LSTM model by a\nlarge margin. On the other hand, the word averag-\ning models perform well for both sentence similar-\nity and textual entailment tasks, outperforming the\nLSTM. However, for sentiment classi\ufb01cation task,\nthey \ufb01nd LSTM (trained on PPDB) to beat the av-\neraging models to establish a new state of the art. The above results suggest that structured resources\nplay a vital role in computing general-purpose em-\nbeddings useful in downstream applications. 2.4 Modeling as an autoencoder\nMotivation: The autoencoder based approach\nlearns latent (or compressed) representation by re-\nconstructing its own input. Since textual data like\ntweets contain discrete input signals, sequence-\nto-sequence models (Sutskever et al., 2014) like\nSTV can be used to build the solution. The en-\ncoder model which encodes the input tweet can\ntypically be a CNN (Kim, 2014), recurrent models\nlike RNN, GRU, LSTM (Karpathy et al., 2015) or\nmemory networks (Sukhbaatar et al., 2015). The\ndecoder model which generates the output tweet\ncan typically be a recurrent model that predicts a\noutput token at every time step. Models: Sequential Denoising Autoencoders\n(SDAE) (Hill et al., 2016) is a LSTM-based\nsequence-to-sequence model, which is trained to\n\nrecover the original data from the corrupted ver-\nsion. SDAE produces robust representations by\nlearning to represent the data in terms of fea-\ntures that explain its important factors of varia-\ntion. Tweet2Vec (V osoughi et al., 2016) is a recent\nmodel which uses a character-level CNN-LSTM\nencoder-decoder architecture trained to construct\nthe input tweet directly. This model outper-\nforms competitive models that work on word-level\nlike PV-DM, PV-DBOW on semantic similarity\ncomputation and sentiment classi\ufb01cation tasks,\nthereby showing that the character-level nature of\nTweet2Vec is best-suited to deal with the noise and\nidiosyncrasies of tweets. Tweet2Vec controls the\ngeneralization error by using a data augmentation\ntechnique, wherein tweets are replicated and some\nof the words in the replicated tweets are replaced\nwith their synonyms. Both SDAE and Tweet2Vec\nhas the advantage that they don\u2019t need a coherent\ninter-sentence narrative (like STV), which is hard\nto obtain in Twitter. 2.5 Modeling using weak supervision\nMotivation: In a weakly supervised setup, we\ncreate labels for a tweet automatically and pre-\ndict them to learn potentially sophisticated mod-\nels than those obtained by unsupervised learning\nalone. Examples of labels include sentiment of\nthe overall tweet, words like hashtag present in\nthe tweet and so on. This technique can create a\nhuge labeled dataset especially for building data-\nhungry, sophisticated deep learning models. Models: (Tang et al., 2016) learns sentiment-\nspeci\ufb01c word embedding (SSWE), which encodes\nthe polarity information in the word representa-\ntions so that words with contrasting polarities and\nsimilar syntactic context (like good and bad) are\npushed away from each other in the semantic\nspace that it learns. SSWE utilizes the massive\ndistant-supervised tweets collected by positive and\nnegative emoticons to build a powerful tweet rep-\nresentation, which are shown to be useful in tasks\nsuch as sentiment classi\ufb01cation and word simi-\nlarity computation in sentiment lexicon. (Dhin-\ngra et al., 2016) observes that hashtags in tweets\ncan be considered as topics and hence tweets with\nsimilar hashtags must come closer to each other. Their model predicts the hashtags by using a Bi-\nGRU layer to embed the tweets from its charac-\nters. Due to subword modeling, such character-\nlevel models can approximate the representations\nfor rare words and new words (words not seen dur-\ning training) in the test tweets really well. This\nmodel outperforms the word-level baselines for\nhashtag prediction task, thereby concluding that\nexploring character-level models for tweets is a\nworthy research direction to pursue. Both these\nworks fail to study the model\u2019s generality (Weston\net al., 2014), i.e., the ability of the model to trans-\nfer the learned representations to diverse tasks. 3 Future Directions\nIn this section we present the future research di-\nrections which we believe can be worth pursuing\nto generate high quality tweet embeddings. \u2022 (Ren et al., 2016) propose a supervised\nneural network utilizing contextualized fea-\ntures from conversation, author and topic\nbased context about a target tweet to per-\nform well in classi\ufb01cation of tweet. Apart\nfrom (Ganesh et al., 2017)\u2019s work which uti-\nlizes author context, there is no other work\nwhich builds unsupervised tweet representa-\ntion model on Twitter-speci\ufb01c contexts such\nas conversation and topical streams. We be-\nlieve such a solution directly exploits seman-\ntic signals (or nuances) from Twitter, unlike\nSTV or Siamese CBOW which are trained on\nbooks corpus. \u2022 (dos Santos and Gatti, 2014) propose a super-\nvised, hybrid model exploiting both the char-\nacter and word level information for Twit-\nter sentiment analysis task. Since the set-\ntings when the character level model beats the\nword level model is not well understood yet,\nwe believe it would be interesting to explore\nsuch a hybrid compositional model to build\nunsupervised tweet representations. \u2022 Twitter provides a platform for the users to\ninteract with other users. To the best of our\nknowledge, there is no related work that com-\nputes unsupervised tweet representation by\nexploiting the user pro\ufb01le attributes like pro-\n\ufb01le picture, user biography and set of fol-\nlowers, and social interactions like retweet\ncontext (set of surrounding tweets in a users\nretweet stream) and favorite context (set of\nsurrounding tweets in a users favorite tweet\nstream). \u2022 DSSM (Huang et al., 2013; Shen et al., 2014)\npropose a family of deep models that are\ntrained to maximize the relevance of clicked\ndocuments given a query. Such a ranking loss\nfunction helps the model cater to a wide vari-\nety of applications1 such as web search rank-\ning, ad selection/relevance, question answer-\ning, knowledge inference and machine trans-\nlation. We observe such a loss function has\nnot been explored for building unsupervised\ntweet representations. We believe employing\na ranking loss directly on tweets using a large\nscale microblog dataset 2 can result in repre-\nsentations which can be useful to Twitter ap-\nplications beyond those studied in the tweet\nrepresentation learning literature. \u2022 Linguists assume that language is best un-\nderstood as a hierarchical tree of phrases,\nrather than a \ufb02at sequence of words or charac-\nters. It\u2019s dif\ufb01cult to get the syntactic trees for\ntweets as most of them are not grammatically\ncorrect. The average of word vectors model\nhas the most simplest compositional archi-\ntecture with no additional parameters, yet\ndisplays a strong performance outperforming\ncomplex architectures such as CNN, LSTM\nand so on for several downstream applica-\ntions (Wieting et al., 2016a,b). We believe\na theoretical understanding of why word av-\neraging models perform well can help in em-\nbracing these models by linguists. \u2022 Models in (Wieting et al., 2016a,b) learn\nfrom noisy phrase pairs of PPDB. Note that\nthe source of the underlying texts is com-\npletely different from Twitter. It can be inter-\nesting to see the effectiveness of such mod-\nels when directly trained on structural re-\nsources from Twitter like Twitter Paraphrase\nCorpus (Xu et al., 2014). The main challenge\nwith this approach is the small size of the an-\nnotated Twitter resources, which can encour-\nage models like (Arora et al., 2017) that work\nwell even when the training data is scarce or\nnonexistent. \u2022 Tweets mostly have an accompanying im-\nage which sometimes has visual correspon-\ndence with its textual content (Chen et al.,\n1https://www.microsoft.com/en-us/\nresearch/project/dssm/\n2http://trec.nist.gov/\n2013; Wang et al., 2014) (\u2018visual\u2019 tweet). To the best of our knowledge, there is no\nwork which explores the following question:\ncan we build multimodal representations for\ntweets accompanying correlated visual con-\ntent and compare with traditional bench-\nmarks?. We can leverage insights from mul-\ntimodal skip-gram model (Lazaridou et al.,\n2015) which builds multimodally-enhanced\nword vectors that perform well in the tradi-\ntional semantic benchmarks. However, it\u2019s\nhard to detect visual tweets and learning from\na non-visual tweet can degrade its tweet rep-\nresentation. It would be interesting to see if\na dispersion metric (Kiela et al., 2014) for\ntweets can be explored to overcome this prob-\nlem of building a nondegradable, improved\ntweet representation. \u2022 Interpreting the tweet representations to un-\nearth the encoded features responsible for\nits performance on a downstream task is an\nimportant, but a less studied research area. (Ganesh et al., 2016)\u2019s work is the \ufb01rst to\nopen the blackbox of vector embeddings for\ntweets. They propose elementary property\nprediction tasks which predicts the accuracy\nto which a given tweet representation en-\ncodes the elementary property (like slang\nwords, hashtags, mentions, etc). The main\ndrawback of the work is that they fail to cor-\nrelate their study with downstream applica-\ntions. We believe performing such a correla-\ntion study can clearly highlight the set of el-\nementary features behind the performance of\na particular representation model over other\nfor a given downstream task. 4 Conclusion\nIn this work we study the problem of learning\nunsupervised tweet representations. We believe\nour survey of the existing works based on the ob-\njective function can give vital perspectives to re-\nsearchers and aid their understanding of the \ufb01eld. We also believe the future research directions stud-\nied in this work can help in breaking the barriers in\nbuilding high quality, general purpose tweet repre-\nsentation models. References\nSanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A simple but tough-to-beat baseline for sentence em-\nbeddings. CoRR . Yoshua Bengio. 2013a. Deep Learning of Represen-\ntations: Looking Forward. In Proc. of the 1st Intl. Conf. on Statistical Language and Speech Process-\ning. pages 1\u201337. Yoshua Bengio, Aaron C. Courville, and Pascal Vin-\ncent. 2013b. Representation Learning: A Review\nand New Perspectives. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence 35(8):1798\u2013\n1828. David M. Blei, Andrew Y . Ng, and Michael I. Jordan.",
            0.905355625000027
          ]
        }
      ],
      "reference_answers": [
        " Paraphrase Database (PPDB) ,  book corpus",
        "No No",
        "They group the existing works in terms of the objective function they optimize - within-tweet relationships, inter-tweet relationships, autoencoder, and weak supervision."
      ]
    },
    "1706.08198.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.20141536122491943,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.3680555555555556,
          "recall": 0.5119047619047619,
          "f1": 0.38205128205128214
        },
        "rouge-2": {
          "precision": 0.23529411764705882,
          "recall": 0.2,
          "f1": 0.2162162162162162
        },
        "rouge-L": {
          "precision": 0.3402777777777778,
          "recall": 0.4880952380952381,
          "f1": 0.3564102564102565
        }
      },
      "per_question_metrics": [
        {
          "question": "Is pre-training effective in their evaluation?",
          "generated_answer": "Yes, pre-training is effective in their evaluation.",
          "reference_answer": "Yes Yes",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.125,
              "recall": 0.5,
              "f1": 0.2
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.125,
              "recall": 0.5,
              "f1": 0.2
            }
          },
          "bleu_score": 0.023980296761827107,
          "exact_match": 0.0
        },
        {
          "question": "What parallel corpus did they use?",
          "generated_answer": "They used two parallel corpora: the Asian Scientific Paper Excerpt Corpus (ASPEC) and the NTCIR PatentMT Parallel Corpus.",
          "reference_answer": "Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0, NTCIR PatentMT Parallel Corpus BIBREF1 Asian Scientific Paper Excerpt Corpus, NTCIR PatentMT Parallel Corpus ",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.6111111111111112,
              "recall": 0.5238095238095238,
              "f1": 0.5641025641025642
            },
            "rouge-2": {
              "precision": 0.47058823529411764,
              "recall": 0.4,
              "f1": 0.4324324324324324
            },
            "rouge-L": {
              "precision": 0.5555555555555556,
              "recall": 0.47619047619047616,
              "f1": 0.5128205128205129
            }
          },
          "bleu_score": 0.37885042568801175,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "Is pre-training effective in their evaluation?",
        "What parallel corpus did they use?"
      ],
      "generated_answers": [
        {
          "answer": "Yes, pre-training is effective in their evaluation.",
          "relevant_chunks": [
            "Regarding the training data of\nASPEC, we used only the \ufb01rst 1 million sentences\nsorted by sentence-alignment similarity. Japanese\nsentences were segmented by the morphological\nanalyzer MeCab (version 0.996, IPADIC), and En-\nglish sentences were tokenized by tokenizer.perl\nof Moses. Table 1 shows the numbers of the sen-\ntences in each corpus. Note that sentences with\nmore than 40 words were excluded from the train-\ning data. 5.2 Models\nWe used the attention-based NMT (Bahdanau\net al., 2015) as a baseline-NMT, the encoder-\ndecoder-reconstructor (Tu et al., 2017) and the\nencoder-decoder-reconstructor that jointly trained\nforward translation and back-translation without\npre-training. The RNN used in the experi-\nments had 512 hidden units, 512 embedding units,\n30,000 vocabulary size and 64 batch size. We\nused Adagrad (initial learning rate 0.01) for op-\ntimizing model parameters. We trained our model\non GeForce GTX TITAN X GPU. Note that we\nset the hyper-parameter \u03bb = 1 on the encoder-\ndecoder-reconstructor same as Tu et al. (2017). 5.3 Results\nTables 2 and 3 show the translation accuracy\nin BLEU scores, the p-value of the signi\ufb01cance\ntest by bootstrap resampling (Koehn, 2004) and\ntraining time in hours until convergence. The\nencoder-decoder-reconstructor (Tu et al., 2017)\nrequires slightly longer time to train than the\nbaseline NMT, but we emphasize that decoding\ntime remains the same with the encoder-decoder-\nreconstructor and baseline-NMT. The results show\nthat the encoder-decoder-reconstructor (Tu et al.,\n2017) signi\ufb01cantly improves translation accuracy\nby 1.01 points on ASPEC and 1.37 points on NT-\nCIR in English-Japanese translation ( p <0.05). However, it does not signi\ufb01cantly improve trans-\nlation accuracy in Japanese-English translation. In\naddition, it is proved that the encoder-decoder-\nreconstructor without pre-training worsens rather\n\nBaseline-NMT\n Encoder-Decoder-Reconstructor\nFigure 3: The attention layer in Example 1 : Improvement in under-translation. Baseline-NMT\n Encoder-Decoder-Reconstructor\nFigure 4: The attention layer in Example 2 : Improvement in over-translation. than improves translation accuracy. Table 4 shows examples of outputs of English-\nJapanese translations. In Example 1, \u201c \u4e71 \u6d41\u7c98\u6027\n\u3068\u6570\u5024\u7c98\u6027\u306e\u5927\u5c0f\u95a2\u4fc2\u306b\u3088\u308a,\u201d (on the ba-\nsis of the relation between turbulent viscosity and\nnumerical viscosity in size) is missing in the out-\nput of baseline-NMT, but \u201c \u4e71 \u6d41\u7c98\u6027 \u3068\u6570\u5024\n\u7684 \u7c98\u6027 \u306e\u95a2\u4fc2 \u3092\u57fa \u306b\u201d (on the basis of the\nrelation between turbulent viscosity and numeri-\ncal viscosity) is present in the output of encoder-\ndecoder-reconstructor. In Example 2, \u201c \u65b0\u751f\u5150\u201d\n(newborn infant) and \u201c30\u6b73\u4ee5\u4e0a\u306e\u201d (of 30 - year -\nold or more) are repeated in the output of baseline-\nNMT, but they appear only once in the output of\nencoder-decoder-reconstructor. In addition, Figures 3 and 4 show the atten-\ntion layer on baseline-NMT and encoder-decoder-\nreconstructor in each example. In Figure 3, al-\nthough the attention layer of baseline NMT attends\ninput word \u201cturbulent\u201d, the decoder does not out-\nput \u201c\u4e71\u6d41\u201d (turbulent) but \u201c\u691c\u8a0e\u201d (examined) at\nthe 13th word. Thus, under-translation may be\nresulted from the hidden layer or the embedding\nlayer instead of the attention layer. In Figure 4, it\nis found that the attention layer of baseline-NMT\nrepeatedly attends input words \u201cnewborn infant\u201d\nand \u201c30 - year - old or more\u201d. Consequently,\nthe decoder repeatedly outputs \u201c \u65b0\u751f\u5150\u201d (new-\nborn infant) and \u201c30\u6b73\u4ee5\u4e0a\u306e\u201d (of 30 - year - old\nor more). On the other hand, the attention layer\n\nCorpus Model English-Japanese Japanese-English\n(i) (ii) (iii) (i) (ii) (iii)\nBaseline-NMT 1,141 378 1,045 951 494 1,085\nASPEC +Reconstructor 988 336 1,042 836 418 1,014\n+Reconstructor (Jointly-Training) 1,292 446 1,147 1,106 525 1,821\nBaseline-NMT 2,122 1,015 1,106 2,521 1,073 1,630\nNTCIR +Reconstructor 1,958 922 963 2,187 987 1,422\n+Reconstructor (Jointly-Training) 1,978 916 1,078 2,475 1,107 1,610\nTable 5: Numbers of redundant and unknown word tokens. of encoder-decoder-reconstructor almost correctly\nattends input words. Table 5 shows a comparison of the number of\nword occurrences for each corpus and model. The\ncolumns show (i) the number of words that ap-\npear more frequently than the counterparts in the\nreference, and (ii) the number of words that ap-\npear more than once but are not included in the\nreference. Note that these numbers do not in-\nclude unknown words, so (iii) shows the num-\nber of unknown words. In all the cases, the\nnumber of occurrence of redundant words is re-\nduced in encoder-decoder-reconstructor. Thus,\nwe con\ufb01rmed that encoder-decoder-reconstructor\nachieves reduction of repeating and missing words\nwhile maintaining the quality of translation. 6 Conclusion\nIn this paper, we evaluated the encoder-decoder-\nreconstructor on English-Japanese and Japanese-\nEnglish translation tasks. In addition, we evaluate\nthe effectiveness of pre-training by comparing it\nwith a jointly-trained model of forward translation\nand back-translation. Experimental results show\nthat the encoder-decoder-reconstructor offers sig-\nni\ufb01cant improvement in BLEU scores and allevi-\nates the problem of repeating and missing words\nin the translation on English-Japanese translation\ntask, and the encoder-decoder-reconstructor can\nnot be trained well without pre-training, so it\nproves that we have to train the forward transla-\ntion model in a manner similar to the conventional\nattention-based NMT as pre-training. References\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio.\n\n\ud835\udc66! Figure 1: Attention-based NMT. Encoder\nDecoder\nReconstructor\nContext\nV ector\nInverse\nContext\nV ector Figure 2: Encoder-Decoder-Reconstructor. English translation tasks using encode-decoder-\nreconstructor. Our experiments show that their\nmethod offers signi\ufb01cant improvement in BLEU\nscores and alleviates the problem of repeating\nand missing words in the translation on English-\nJapanese translation task, though the difference\nis not signi\ufb01cant on Japanese-English translation\ntask. In addition, we jointly train a model of for-\nward translation and back-translation without pre-\ntraining, and then evaluate this model. As a re-\nsult, the encoder-decoder-reconstructor can not be\ntrained well without pre-training, so it proves that\nwe have to train the forward translation model in a\nmanner similar to the conventional attention-based\nNMT as pre-training. The main contributions of this paper are as fol-\nlows:\n\u2022Experimental results show that encode-\ndecoder-reconstructor framework achieves\nsigni\ufb01cant improvements in BLEU scores\n(1.0-1.4) for English-Japanese translation\ntask. \u2022Experimental results show that encode-\ndecoder-reconstructor framework has to train\nthe forward translation model in a manner\nsimilar to the conventional attention-based\nNMT as pre-training. 2 Related Works\nSeveral studies have addressed the NMT-speci\ufb01c\nproblem of missing or repeating words. Niehues\net al. (2016) optimized NMT by adding the out-\nputs of PBSMT to the input of NMT. Mi et al.\n\n(2016) and Feng et al. (2016) introduced a dis-\ntributed version of coverage vector taken from PB-\nSMT to consider which words have been already\ntranslated. All these methods, including ours, em-\nploy information of the source sentence to im-\nprove the quality of translation, but our method\nuses back-translation to ensure that there is no in-\nconsistency. Unlike other methods, once learned,\nour method is identical to the conventional NMT\nmodel, so it does not need any additional parame-\nters such as coverage vector or a PBSMT system\nfor testing. The attention mechanism proposed by Meng\net al. (2016) considers not only the hidden states\nof the encoder but also the hidden states of the\ndecoder so that over-translation can be relaxed. In addition, the attention mechanism proposed by\nFeng et al. (2016) computes a context vector by\nconsidering the previous context vector to pre-\nvent over-translation. These works indirectly re-\nduce repeating and missing words, while we di-\nrectly penalize translation mismatch by consider-\ning back-translation. The encoder-decoder-reconstructor framework\nfor NMT proposed by Tu et al. (2017) optimizes\nNMT by reconstructor using back-translation. They consider likelihood of both of forward trans-\nlation and back-translation, and then this frame-\nwork offers signi\ufb01cant improvement in BLEU\nscores and alleviates the problem of repeating and\nmissing words in the translation on a Chinese-\nEnglish translation task. 3 Neural Machine Translation\nHere, we describe the attention-based NMT pro-\nposed by Bahdanau et al. (2015) as shown in Fig-\nure 1. The input sequence ( x = [x1,x2,\u00b7\u00b7\u00b7 ,x|x|]) is\nconverted into a \ufb01xed-length vector by the encoder\nusing an RNN. At each time stept, the hidden state\n\nht of the encoder is presented as\nht = [\u2212 \u2192ht\u22a4: \u2190 \u2212ht\u22a4]\u22a4 (1)\nusing a bidirectional RNN. The forward state \u2212 \u2192ht\nand the backward state \u2190 \u2212ht are computed by\n\u2212 \u2192ht = r(xt,ht\u22121) (2)\nand \u2190 \u2212ht = r\u2032(xt,ht+1) (3)\nwhere rand r\u2032are nonlinear functions. The hidden\nstates (h1,h2,\u00b7\u00b7\u00b7 ,h|x|) are converted into a \ufb01xed-\nlength vector vas\nv= q([h1,h2,\u00b7\u00b7\u00b7 ,h|x|]) (4)\nwhere qis a nonlinear function. The \ufb01xed-length vector v generated by the en-\ncoder is converted into the target sequence ( y =\n[y1,y2,\u00b7\u00b7\u00b7 ,y|y|]) by the decoder using an RNN. At each time step i, the conditional probability of\nthe output word \u02c6yi is computed by\np(\u02c6yi|y<i,x) =f(si,yi\u22121,ci) (5)\nwhere f is a nonlinear function. The hidden state\nsi of the decoder is presented as\nsi = g(si\u22121,yi\u22121,ci) (6)\nusing the hidden state si\u22121 and the target word\nyi\u22121 at the previous time step and the context vec-\ntor ci. The context vector ci is a weighted sum of each\nhidden state hj of the encoder. It is presented as\nci =\n|x|\u2211\nj=1\n\u03b1ijhj (7)\nand its weight \u03b1ij is a normalized probability dis-\ntribution. It is computed by\n\u03b1ij = exp(eij)\n\u2211|x|\nk=1 exp(eik)\n(8)\nand\neij = v\u22a4\na tanh(Wasi\u22121 + Uahj) (9)\nwhere va is a weight vector and Wa and Ua are\nweight matrices. ASPEC NTCIR\ntrain 827,188 1,169,201\ndev 1,504 2,741\ntest 1,556 2,300\nTable 1: Numbers of parallel sentences. The objective function is de\ufb01ned by\nL(\u03b8) = 1\nN\n\u2211N\nn=1\n|y|\u2211\ni=1\nlog p(\u02c6y(n)\ni |y(n)\n<i ,x(n),\u03b8)\n(10)\nwhere N is the number of data and \u03b8 is a model\nparameter. Incidentally, as a nonlinear function, the hyper-\nbolic tangent function or the recti\ufb01ed linear unit\nare generally used. 4 Encoder-Decoder-Reconstructor\n4.1 Architecture\nNext, we describe the encoder-decoder-\nreconstructor framework for NMT proposed\nby Tu et al. (2017) as shown in Figure 2. The\nencoder-decoder-reconstructor consists of two\ncomponents: the standard encoder-decoder as\nan attention-based NMT proposed by Bahdanau\net al. (2015) and the reconstructor which back-\ntranslates from the hidden states of decoder to the\nsource sentence. In their method, the hidden state of the decoder\nis back-translated into the source sequence (x) by\nthe reconstructor for the back-translation. At each\ntime step i, the conditional probability of the out-\nput word \u02c6xi is computed by\np(\u02c6xi|x<i,\u02c6y) =f\u2032(s\u2032\ni,xi\u22121,c\u2032\ni) (11)\nwhere f\u2032is a nonlinear function. The hidden state\ns\u2032\ni of the reconstructor is presented as\ns\u2032\ni = g\u2032(s\u2032\ni\u22121,xi\u22121,c\u2032\ni) (12)\nusing the hidden state s\u2032\ni\u22121 and the source word\nxi\u22121 at the previous time step and the new context\nvector (inverse context vector) c\u2032\ni. The inverse context vector c\u2032\ni is a weighted sum\nof each hidden state sj of the decoder (on forward\ntranslation). It is presented as\nc\u2032\ni =\n|y|\u2211\nj=1\n\u03b1\u2032\nijsj (13)\n\nEnglish-Japanese\nCorpus Model BLEU p-value Hours\nBaseline-NMT 29.75 - 99\nASPEC +Reconstructor 30.76 0.00 149\n+Reconstructor (Jointly-Training) 26.04 - 174\nBaseline-NMT 30.03 - 116\nNTCIR +Reconstructor 31.40 0.00 166\n+Reconstructor (Jointly-Training) 29.04 - 252\nTable 2: English-Japanese translation results. Japanese-English\nCorpus Model BLEU p-value Hours\nBaseline-NMT 21.91 - 87\nASPEC +Reconstructor 22.27 0.10 127\n+Reconstructor (Jointly-Training) 16.29 - 187\nBaseline-NMT 29.48 - 180\nNTCIR +Reconstructor 29.73 0.11 244\n+Reconstructor (Jointly-Training) 28.95 - 300\nTable 3: Japanese-English translation results. and its weight \u03b1\u2032\nij is a normalized probability dis-\ntribution.",
            0.806449291999968
          ]
        },
        {
          "answer": "They used two parallel corpora: the Asian Scientific Paper Excerpt Corpus (ASPEC) and the NTCIR PatentMT Parallel Corpus.",
          "relevant_chunks": [
            "It is computed by\n\u03b1\u2032\nij =\nexp(e\u2032\nij)\n\u2211|y|\nk=1 exp(e\u2032\nik)\n(14)\nand\ne\u2032\nij = v\u2032\na\u22a4tanh(W\u2032\nas\u2032\ni\u22121 + U\u2032\nasj) (15)\nwhere v\u2032\na is a weight vector and W\u2032\na and U\u2032\na are\nweight matrices. The objective function is de\ufb01ned by\nL(\u03b8,\u03b3) = 1\nN\n\u2211N\nn=1\n{ |y|\u2211\ni=1\nlog p(\u02c6y(n)\ni |y(n)\n<i ,x(n),\u03b8)\n+ \u03bb\n|x|\u2211\ni=1\nlog p(\u02c6x(n)\ni |x(n)\n<i ,s(n),\u03b3)\n}\n(16)\nwhere N is the number of data, \u03b8and \u03b3are model\nparameters and \u03bbis a hyper-parameter which can\nconsider the weight between forward translation\nand back-translation. This objective function consists of two parts:\nforward measures translation \ufb02uency, and back-\nward measures translation adequacy. Thus, the\ncombined objective function is more consistent\nwith the goal of enhancing overall translation\nquality, and can more effectively guide the param-\neter training for making better translation. 4.2 Training\nThe encoder-decoder-reconstructor is trained with\nlikelihood of both the encoder-decoder and the re-\nconstructor on a set of training datasets. Tu et al. (2017) trained a back-translation model from the\nhidden state of the decoder into the source se-\nquence by reconstructor to enforce agreement be-\ntween source and target sentences using Equa-\ntion 16 after training the forward translation in a\nmanner similar to the conventional attention-based\nNMT using Equation 10. In addition, we experiment to jointly train a\nmodel of forward translation and back-translation\nwithout pre-training. It may learn a globally opti-\nmal model compared to locally optimal model pre-\ntrained using the forward translation. 4.3 Testing\nTu et al. (2017) used a beam search to predict tar-\nget sentences that approximately maximizes both\nof forward translation and back-translation on test-\ning. In this paper, however, we do not use a beam\nsearch for simplicity and effectiveness. 5 Experiments\nWe evaluated the encoder-decoder-reconstructor\nframework for NMT on English-Japanese and\nJapanese-English translation tasks. Example 1: Improvement in under-translation. Input the conditions under which the effect of turbulent viscosity is cor-\nrectly evaluated were examined on the basis of the relation between turbulent\nviscosity and numerical viscosity in size . Baseline-NMT \u4e71\u6d41\u7c98\u6027 \u306e\u5f71\u97ff\u3092\u6b63\u78ba\u306b\u8a55\u4fa1\u3059\u308b\u6761\u4ef6\u3092\u691c\u8a0e\u3057\u305f\u3002\n+Reconstructor \u4e71\u6d41\u7c98\u6027 \u306e\u5f71\u97ff\u3092\u6b63\u78ba\u306b\u8a55\u4fa1\u3059\u308b\u6761\u4ef6\u3092, \u4e71 \u4e71 \u4e71\u6d41 \u6d41 \u6d41\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\u3068 \u3068 \u3068\u6570 \u6570 \u6570\u5024 \u5024 \u5024 \u7684 \u7684 \u7684\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\n\u306e \u306e \u306e\u95a2 \u95a2 \u95a2\u4fc2 \u4fc2 \u4fc2\u3092 \u3092 \u3092\u57fa \u57fa \u57fa\u306b \u306b \u306b\u8abf\u3079\u305f \u3002\n+Reconstructor \u4e71\u6d41\u7c98\u6027 \u306e\u5f71\u97ff\u3092\u8003\u616e\u3057\u305f \u6761\u4ef6\u3092, \u4e71 \u4e71 \u4e71\u6d41 \u6d41 \u6d41\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\u3068 \u3068 \u3068\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\u306e \u306e \u306e\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\u3068 \u3068 \u3068\u306e \u306e \u306e\n(Jointly-Training) \u95a2 \u95a2 \u95a2\u4fc2 \u4fc2 \u4fc2\u3092 \u3092 \u3092\u3082 \u3082 \u3082\u3068 \u3068 \u3068\u306b \u306b \u306b\u691c\u8a0e\u3057\u305f \u3002\nReference \u4e71 \u4e71 \u4e71\u6d41 \u6d41 \u6d41\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\u3068 \u3068 \u3068\u6570 \u6570 \u6570\u5024 \u5024 \u5024\u7c98 \u7c98 \u7c98\u6027 \u6027 \u6027\u306e \u306e \u306e\u5927 \u5927 \u5927\u5c0f \u5c0f \u5c0f\u95a2 \u95a2 \u95a2\u4fc2 \u4fc2 \u4fc2\u306b \u306b \u306b\u3088 \u3088 \u3088\u308a \u308a \u308a, \u4e71\u6d41\u7c98\u6027\u306e\u52b9\u679c\u304c\u6b63\u3057\u304f\n\u8a55\u4fa1\u3055\u308c\u308b \u6761\u4ef6\u3092\u691c\u8a0e\u3057\u305f \u3002\nExample 2: Improvement in over-translation. Input activity was high in cells of the young , especiallynewborn infant , and was very\nslight in cells of 30 - year - old or more . Baseline-NMT \u6d3b\u52d5\u6027\u306f\u82e5\u9f62, \u7279\u306b\u65b0 \u65b0 \u65b0\u751f \u751f \u751f\u5150 \u5150 \u5150\u65b0 \u65b0 \u65b0\u751f \u751f \u751f\u5150 \u5150 \u5150\u3067\u306f30 \u6b73 \u6b73 \u6b73\u4ee5 \u4ee5 \u4ee5\u4e0a \u4e0a \u4e0a\u306e \u306e \u306e\u7d30\u80de\u3067\u9ad8\u304f,\n30 \u6b73 \u6b73 \u6b73\u4ee5 \u4ee5 \u4ee5\u4e0a \u4e0a \u4e0a\u306e \u306e \u306e\u7d30\u80de\u3067\u306f\u308f\u305a\u304b\u3067\u3042\u3063\u305f \u3002\n+Reconstructor \u305d\u306e\u6d3b\u6027\u306f\u82e5\u9f62, \u7279\u306b\u65b0 \u65b0 \u65b0\u751f \u751f \u751f\u5150 \u5150 \u5150\u306f\u7d30\u80de\u304c\u9ad8\u304f, 30 \u6b73 \u6b73 \u6b73\u4ee5 \u4ee5 \u4ee5\u4e0a \u4e0a \u4e0a\u306e \u306e \u306e\u7d30\u80de \u3067\u306f\n\u308f\u305a\u304b\u3067\u3042\u3063\u305f \u3002\n+Reconstructor \u82e5\u9f62\u306e\u65b0 \u65b0 \u65b0\u751f \u751f \u751f\u5150 \u5150 \u5150\u3067\u306f\u6d3b\u6027\u306f\u9ad8\u304b\u3063\u305f\u304c , 30 \u6b73 \u6b73 \u6b73\u4ee5 \u4ee5 \u4ee5\u4e0a \u4e0a \u4e0a\u306e \u306e \u306e\u5834\u5408\u306b\u306f \u6975\u3081\u3066\n(Jointly-Training) \u8efd\u5ea6\u3067\u3042\u3063\u305f \u3002\nReference \u6d3b\u6027\u306f\u82e5\u3044\u500b\u4f53, \u7279\u306b\u65b0 \u65b0 \u65b0\u751f \u751f \u751f\u5150 \u5150 \u5150\u306e\u7d30\u80de\u3067\u9ad8\u304f, 30 \u6b73 \u6b73 \u6b73\u4ee5 \u4ee5 \u4ee5\u4e0a \u4e0a \u4e0a\u306e \u306e \u306e\u3082\u306e\u3067\u306f\n\u3054\u304f\u308f\u305a\u304b\u3067\u3042\u3063\u305f \u3002\nTable 4: Examples of outputs of English-Japanese translation. 5.1 Datasets\nWe used two parallel corpora: Asian Scien-\nti\ufb01c Paper Excerpt Corpus (ASPEC) (Nakazawa\net al., 2016) and NTCIR PatentMT Parallel Corpus\n(Goto et al., 2013).\n\nRegarding the training data of\nASPEC, we used only the \ufb01rst 1 million sentences\nsorted by sentence-alignment similarity. Japanese\nsentences were segmented by the morphological\nanalyzer MeCab (version 0.996, IPADIC), and En-\nglish sentences were tokenized by tokenizer.perl\nof Moses. Table 1 shows the numbers of the sen-\ntences in each corpus. Note that sentences with\nmore than 40 words were excluded from the train-\ning data. 5.2 Models\nWe used the attention-based NMT (Bahdanau\net al., 2015) as a baseline-NMT, the encoder-\ndecoder-reconstructor (Tu et al., 2017) and the\nencoder-decoder-reconstructor that jointly trained\nforward translation and back-translation without\npre-training. The RNN used in the experi-\nments had 512 hidden units, 512 embedding units,\n30,000 vocabulary size and 64 batch size. We\nused Adagrad (initial learning rate 0.01) for op-\ntimizing model parameters. We trained our model\non GeForce GTX TITAN X GPU. Note that we\nset the hyper-parameter \u03bb = 1 on the encoder-\ndecoder-reconstructor same as Tu et al. (2017). 5.3 Results\nTables 2 and 3 show the translation accuracy\nin BLEU scores, the p-value of the signi\ufb01cance\ntest by bootstrap resampling (Koehn, 2004) and\ntraining time in hours until convergence. The\nencoder-decoder-reconstructor (Tu et al., 2017)\nrequires slightly longer time to train than the\nbaseline NMT, but we emphasize that decoding\ntime remains the same with the encoder-decoder-\nreconstructor and baseline-NMT. The results show\nthat the encoder-decoder-reconstructor (Tu et al.,\n2017) signi\ufb01cantly improves translation accuracy\nby 1.01 points on ASPEC and 1.37 points on NT-\nCIR in English-Japanese translation ( p <0.05). However, it does not signi\ufb01cantly improve trans-\nlation accuracy in Japanese-English translation. In\naddition, it is proved that the encoder-decoder-\nreconstructor without pre-training worsens rather\n\nBaseline-NMT\n Encoder-Decoder-Reconstructor\nFigure 3: The attention layer in Example 1 : Improvement in under-translation. Baseline-NMT\n Encoder-Decoder-Reconstructor\nFigure 4: The attention layer in Example 2 : Improvement in over-translation. than improves translation accuracy. Table 4 shows examples of outputs of English-\nJapanese translations. In Example 1, \u201c \u4e71 \u6d41\u7c98\u6027\n\u3068\u6570\u5024\u7c98\u6027\u306e\u5927\u5c0f\u95a2\u4fc2\u306b\u3088\u308a,\u201d (on the ba-\nsis of the relation between turbulent viscosity and\nnumerical viscosity in size) is missing in the out-\nput of baseline-NMT, but \u201c \u4e71 \u6d41\u7c98\u6027 \u3068\u6570\u5024\n\u7684 \u7c98\u6027 \u306e\u95a2\u4fc2 \u3092\u57fa \u306b\u201d (on the basis of the\nrelation between turbulent viscosity and numeri-\ncal viscosity) is present in the output of encoder-\ndecoder-reconstructor. In Example 2, \u201c \u65b0\u751f\u5150\u201d\n(newborn infant) and \u201c30\u6b73\u4ee5\u4e0a\u306e\u201d (of 30 - year -\nold or more) are repeated in the output of baseline-\nNMT, but they appear only once in the output of\nencoder-decoder-reconstructor. In addition, Figures 3 and 4 show the atten-\ntion layer on baseline-NMT and encoder-decoder-\nreconstructor in each example. In Figure 3, al-\nthough the attention layer of baseline NMT attends\ninput word \u201cturbulent\u201d, the decoder does not out-\nput \u201c\u4e71\u6d41\u201d (turbulent) but \u201c\u691c\u8a0e\u201d (examined) at\nthe 13th word. Thus, under-translation may be\nresulted from the hidden layer or the embedding\nlayer instead of the attention layer. In Figure 4, it\nis found that the attention layer of baseline-NMT\nrepeatedly attends input words \u201cnewborn infant\u201d\nand \u201c30 - year - old or more\u201d. Consequently,\nthe decoder repeatedly outputs \u201c \u65b0\u751f\u5150\u201d (new-\nborn infant) and \u201c30\u6b73\u4ee5\u4e0a\u306e\u201d (of 30 - year - old\nor more). On the other hand, the attention layer\n\nCorpus Model English-Japanese Japanese-English\n(i) (ii) (iii) (i) (ii) (iii)\nBaseline-NMT 1,141 378 1,045 951 494 1,085\nASPEC +Reconstructor 988 336 1,042 836 418 1,014\n+Reconstructor (Jointly-Training) 1,292 446 1,147 1,106 525 1,821\nBaseline-NMT 2,122 1,015 1,106 2,521 1,073 1,630\nNTCIR +Reconstructor 1,958 922 963 2,187 987 1,422\n+Reconstructor (Jointly-Training) 1,978 916 1,078 2,475 1,107 1,610\nTable 5: Numbers of redundant and unknown word tokens. of encoder-decoder-reconstructor almost correctly\nattends input words. Table 5 shows a comparison of the number of\nword occurrences for each corpus and model. The\ncolumns show (i) the number of words that ap-\npear more frequently than the counterparts in the\nreference, and (ii) the number of words that ap-\npear more than once but are not included in the\nreference. Note that these numbers do not in-\nclude unknown words, so (iii) shows the num-\nber of unknown words. In all the cases, the\nnumber of occurrence of redundant words is re-\nduced in encoder-decoder-reconstructor. Thus,\nwe con\ufb01rmed that encoder-decoder-reconstructor\nachieves reduction of repeating and missing words\nwhile maintaining the quality of translation. 6 Conclusion\nIn this paper, we evaluated the encoder-decoder-\nreconstructor on English-Japanese and Japanese-\nEnglish translation tasks. In addition, we evaluate\nthe effectiveness of pre-training by comparing it\nwith a jointly-trained model of forward translation\nand back-translation. Experimental results show\nthat the encoder-decoder-reconstructor offers sig-\nni\ufb01cant improvement in BLEU scores and allevi-\nates the problem of repeating and missing words\nin the translation on English-Japanese translation\ntask, and the encoder-decoder-reconstructor can\nnot be trained well without pre-training, so it\nproves that we have to train the forward transla-\ntion model in a manner similar to the conventional\nattention-based NMT as pre-training. References\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio.\n\n(2016) and Feng et al. (2016) introduced a dis-\ntributed version of coverage vector taken from PB-\nSMT to consider which words have been already\ntranslated. All these methods, including ours, em-\nploy information of the source sentence to im-\nprove the quality of translation, but our method\nuses back-translation to ensure that there is no in-\nconsistency. Unlike other methods, once learned,\nour method is identical to the conventional NMT\nmodel, so it does not need any additional parame-\nters such as coverage vector or a PBSMT system\nfor testing. The attention mechanism proposed by Meng\net al. (2016) considers not only the hidden states\nof the encoder but also the hidden states of the\ndecoder so that over-translation can be relaxed. In addition, the attention mechanism proposed by\nFeng et al. (2016) computes a context vector by\nconsidering the previous context vector to pre-\nvent over-translation. These works indirectly re-\nduce repeating and missing words, while we di-\nrectly penalize translation mismatch by consider-\ning back-translation. The encoder-decoder-reconstructor framework\nfor NMT proposed by Tu et al. (2017) optimizes\nNMT by reconstructor using back-translation. They consider likelihood of both of forward trans-\nlation and back-translation, and then this frame-\nwork offers signi\ufb01cant improvement in BLEU\nscores and alleviates the problem of repeating and\nmissing words in the translation on a Chinese-\nEnglish translation task. 3 Neural Machine Translation\nHere, we describe the attention-based NMT pro-\nposed by Bahdanau et al. (2015) as shown in Fig-\nure 1. The input sequence ( x = [x1,x2,\u00b7\u00b7\u00b7 ,x|x|]) is\nconverted into a \ufb01xed-length vector by the encoder\nusing an RNN. At each time stept, the hidden state\n\nht of the encoder is presented as\nht = [\u2212 \u2192ht\u22a4: \u2190 \u2212ht\u22a4]\u22a4 (1)\nusing a bidirectional RNN. The forward state \u2212 \u2192ht\nand the backward state \u2190 \u2212ht are computed by\n\u2212 \u2192ht = r(xt,ht\u22121) (2)\nand \u2190 \u2212ht = r\u2032(xt,ht+1) (3)\nwhere rand r\u2032are nonlinear functions. The hidden\nstates (h1,h2,\u00b7\u00b7\u00b7 ,h|x|) are converted into a \ufb01xed-\nlength vector vas\nv= q([h1,h2,\u00b7\u00b7\u00b7 ,h|x|]) (4)\nwhere qis a nonlinear function. The \ufb01xed-length vector v generated by the en-\ncoder is converted into the target sequence ( y =\n[y1,y2,\u00b7\u00b7\u00b7 ,y|y|]) by the decoder using an RNN. At each time step i, the conditional probability of\nthe output word \u02c6yi is computed by\np(\u02c6yi|y<i,x) =f(si,yi\u22121,ci) (5)\nwhere f is a nonlinear function. The hidden state\nsi of the decoder is presented as\nsi = g(si\u22121,yi\u22121,ci) (6)\nusing the hidden state si\u22121 and the target word\nyi\u22121 at the previous time step and the context vec-\ntor ci. The context vector ci is a weighted sum of each\nhidden state hj of the encoder. It is presented as\nci =\n|x|\u2211\nj=1\n\u03b1ijhj (7)\nand its weight \u03b1ij is a normalized probability dis-\ntribution. It is computed by\n\u03b1ij = exp(eij)\n\u2211|x|\nk=1 exp(eik)\n(8)\nand\neij = v\u22a4\na tanh(Wasi\u22121 + Uahj) (9)\nwhere va is a weight vector and Wa and Ua are\nweight matrices. ASPEC NTCIR\ntrain 827,188 1,169,201\ndev 1,504 2,741\ntest 1,556 2,300\nTable 1: Numbers of parallel sentences. The objective function is de\ufb01ned by\nL(\u03b8) = 1\nN\n\u2211N\nn=1\n|y|\u2211\ni=1\nlog p(\u02c6y(n)\ni |y(n)\n<i ,x(n),\u03b8)\n(10)\nwhere N is the number of data and \u03b8 is a model\nparameter. Incidentally, as a nonlinear function, the hyper-\nbolic tangent function or the recti\ufb01ed linear unit\nare generally used. 4 Encoder-Decoder-Reconstructor\n4.1 Architecture\nNext, we describe the encoder-decoder-\nreconstructor framework for NMT proposed\nby Tu et al. (2017) as shown in Figure 2. The\nencoder-decoder-reconstructor consists of two\ncomponents: the standard encoder-decoder as\nan attention-based NMT proposed by Bahdanau\net al. (2015) and the reconstructor which back-\ntranslates from the hidden states of decoder to the\nsource sentence. In their method, the hidden state of the decoder\nis back-translated into the source sequence (x) by\nthe reconstructor for the back-translation. At each\ntime step i, the conditional probability of the out-\nput word \u02c6xi is computed by\np(\u02c6xi|x<i,\u02c6y) =f\u2032(s\u2032\ni,xi\u22121,c\u2032\ni) (11)\nwhere f\u2032is a nonlinear function. The hidden state\ns\u2032\ni of the reconstructor is presented as\ns\u2032\ni = g\u2032(s\u2032\ni\u22121,xi\u22121,c\u2032\ni) (12)\nusing the hidden state s\u2032\ni\u22121 and the source word\nxi\u22121 at the previous time step and the new context\nvector (inverse context vector) c\u2032\ni. The inverse context vector c\u2032\ni is a weighted sum\nof each hidden state sj of the decoder (on forward\ntranslation). It is presented as\nc\u2032\ni =\n|y|\u2211\nj=1\n\u03b1\u2032\nijsj (13)\n\nEnglish-Japanese\nCorpus Model BLEU p-value Hours\nBaseline-NMT 29.75 - 99\nASPEC +Reconstructor 30.76 0.00 149\n+Reconstructor (Jointly-Training) 26.04 - 174\nBaseline-NMT 30.03 - 116\nNTCIR +Reconstructor 31.40 0.00 166\n+Reconstructor (Jointly-Training) 29.04 - 252\nTable 2: English-Japanese translation results. Japanese-English\nCorpus Model BLEU p-value Hours\nBaseline-NMT 21.91 - 87\nASPEC +Reconstructor 22.27 0.10 127\n+Reconstructor (Jointly-Training) 16.29 - 187\nBaseline-NMT 29.48 - 180\nNTCIR +Reconstructor 29.73 0.11 244\n+Reconstructor (Jointly-Training) 28.95 - 300\nTable 3: Japanese-English translation results. and its weight \u03b1\u2032\nij is a normalized probability dis-\ntribution.",
            0.8841584169999805
          ]
        }
      ],
      "reference_answers": [
        "Yes Yes",
        "Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0, NTCIR PatentMT Parallel Corpus BIBREF1 Asian Scientific Paper Excerpt Corpus, NTCIR PatentMT Parallel Corpus "
      ]
    }
  },
  "average_metrics": {
    "collection": "semantic_embedding",
    "average_exact_match": 0.0,
    "average_bleu_score": 0.08142318333855263,
    "average_rouge_scores": {
      "rouge-1": {
        "precision": 0.24494601824766096,
        "recall": 0.41680638663202063,
        "f1": 0.26811343743359245
      },
      "rouge-2": {
        "precision": 0.10694679040459043,
        "recall": 0.22109365730055383,
        "f1": 0.12889939867413625
      },
      "rouge-L": {
        "precision": 0.22218235581871057,
        "recall": 0.35924359352235263,
        "f1": 0.23994432596710555
      }
    }
  }
}