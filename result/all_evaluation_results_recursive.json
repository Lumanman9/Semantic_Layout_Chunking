{
  "individual_document_results": {
    "Literature_paper.pdf": {
      "error": "No valid question-answer pairs to evaluate",
      "average_exact_match": 0.0,
      "average_bleu_score": 0.0,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        },
        "rouge-2": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        },
        "rouge-L": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        }
      },
      "per_question_metrics": []
    },
    "1706.09147.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.038708405353591234,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.11307703509538372,
          "recall": 0.537037037037037,
          "f1": 0.18153980752405952
        },
        "rouge-2": {
          "precision": 0.05092592592592592,
          "recall": 0.3235294117647059,
          "f1": 0.088
        },
        "rouge-L": {
          "precision": 0.08679190330566477,
          "recall": 0.4074074074074074,
          "f1": 0.1389326334208224
        }
      },
      "per_question_metrics": [
        {
          "question": "What is the new initialization method proposed in this paper?",
          "generated_answer": "The new initialization method proposed in the paper is explicitly initializing the embeddings with vectors pre-trained over a large collection of unlabeled data. This approach is based on the Skip-Gram with Negative-Sampling (SGNS) algorithm by Mikolov et al. (2013), which simultaneously trains both word and entity vectors using word2vecf, allowing for training word and context embeddings using arbitrary definitions of \"word\" and \"context\" by providing a dataset of word-context pairs, rather than a textual corpus. In this usage, a context is defined as an entity, and the dataset of (word, entity) pairs is compiled from words that appeared in the Wikipedia article describing the entity.",
          "reference_answer": "They initialize their word and entity embeddings with vectors pre-trained over a large corpus of unlabeled data.",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.14678899082568808,
              "recall": 0.8888888888888888,
              "f1": 0.2519685039370079
            },
            "rouge-2": {
              "precision": 0.10185185185185185,
              "recall": 0.6470588235294118,
              "f1": 0.176
            },
            "rouge-L": {
              "precision": 0.11009174311926606,
              "recall": 0.6666666666666666,
              "f1": 0.18897637795275593
            }
          },
          "bleu_score": 0.07298553970052364,
          "exact_match": 0.0
        },
        {
          "question": "How was a quality control performed so that the text is noisy but the annotations are accurate?",
          "generated_answer": "The quality control for ensuring the text is noisy but the annotations are accurate is not explicitly described in the provided context. The context mainly discusses the performance of the model, the datasets used, the challenges of dealing with noisy text, and some error analysis. It does not detail specific quality control measures taken to ensure the accuracy of annotations amidst noisy text.",
          "reference_answer": "The authors believe that the Wikilinks corpus  contains ground truth annotations while being noisy. They discard mentions that cannot have ground-truth verified by comparison with Wikipedia.",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.07936507936507936,
              "recall": 0.18518518518518517,
              "f1": 0.1111111111111111
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.06349206349206349,
              "recall": 0.14814814814814814,
              "f1": 0.08888888888888888
            }
          },
          "bleu_score": 0.004431271006658827,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "What is the new initialization method proposed in this paper?",
        "How was a quality control performed so that the text is noisy but the annotations are accurate?"
      ],
      "generated_answers": [
        {
          "answer": "The new initialization method proposed in the paper is explicitly initializing the embeddings with vectors pre-trained over a large collection of unlabeled data. This approach is based on the Skip-Gram with Negative-Sampling (SGNS) algorithm by Mikolov et al. (2013), which simultaneously trains both word and entity vectors using word2vecf, allowing for training word and context embeddings using arbitrary definitions of \"word\" and \"context\" by providing a dataset of word-context pairs, rather than a textual corpus. In this usage, a context is defined as an entity, and the dataset of (word, entity) pairs is compiled from words that appeared in the Wikipedia article describing the entity.",
          "relevant_chunks": [
            "slowly converge with marginal gains, and there-\nfore trained all models for 8 epochs with k = 5\nfor corrupt-sampling.\n4.3 Embedding Initialization\nTraining our model implicitly embeds the vocabu-\nlary of words and collection of entities in a com-\nmon space. However, we found that explicitly\ninitializing these embeddings with vectors pre-\ntrained over a large collection of unlabeled data\nsigni\ufb01cantly improved performance (see Section\n5.3). To this end, we implemented an approach\nbased on the Skip-Gram with Negative-Sampling\n(SGNS) algorithm by Mikolov et al. (2013) that si-\nmultaneously trains both word and entity vectors.\nWe used word2vecf4 (Levy and Goldberg,\n2014a), which allows one to train word and con-\ntext embeddings using arbitrary de\ufb01nitions of\n\u201dword\u201d and \u201dcontext\u201d by providing a dataset of\nword-context pairs (w, c), rather than a textual\ncorpus. In our usage, we de\ufb01ne a context as an en-\ntity e. To compile a dataset of(w, e) pairs, we con-\nsider every word w that appeared in the Wikipedia\narticle describing entity e. We limit our vocabular-\nies to words that appeared at least 20 times in the\ncorpus and entities that contain at least 20 words\nin their articles. We ran the process for 10 epochs\nand produced vectors of 300 dimensions; other hy-\nperparameters were set to their defaults.\nLevy and Goldberg (2014b) showed that SGNS\nimplicitly factorizes the word-context PMI matrix.\nOur approach is doing the same for the word-entity\nPMI matrix, which is highly related to the word-\nentity TFIDF matrix used in Explicit Semantic\nAnalysis (Gabrilovich and Markovitch, 2007).\n5 Evaluation\nIn this section, we describe our experimental setup\nand compare our model to the state of the art\non two datasets: our new WikilinksNED dataset,\nas well as the commonly-used CoNLL-Y AGO\ndataset (Hoffart et al., 2011). We also examine the\neffect of different corrupt-sampling schemes, and\nof initializing our model with pre-trained word and\nentity embeddings.\nIn all experiments, our model was trained with\n\ufb01xed-size left and right contexts (20 words in each\nside). We used a special padding symbol when\nthe actual context was shorter than the window.\nFurther, we \ufb01ltered stopwords using NLTK\u2019s stop-\nword list prior to selecting the window in order to\n\nbility, number of candidates for the given mention\nand maximum conditional probability of the entity\nin the document. The string based features include\nedit distance between mention and entity title and\ntwo boolean features indicating whether the entity\ntitle starts or ends with the mention and vice versa.\nThe GBRT model parameters where set to the val-\nues reported as optimal by Yamada6.\nCandidate Generation For comparability with\nexisting methods we used two publicly available\ncandidates datasets: (1) PPRforNED - Pershina at\nel. (2015); (2) Y AGO - Hoffart at el. (2011).\nBaselines As a baseline we took the standard\nMost Probable Sense (MPS) prediction, which se-\nlects the entity that was seen most with the given\nmention during training. We also compare to the\nfollowing papers - Francis-Landau et al. (2016),\nYamada at el. (2016), and Chisholm et al. (2015),\nas they are all strong local approaches and a good\nsource for comparison.\nResults Table 2 displays the micro and macro\nP@1 scores on CoNLL-Y AGO test-b for the dif-\nferent training steps. We \ufb01nd that when using only\nthe training set of CoNLL-Y AGO our model is\nunder-trained and that the domain adaptation sig-\nni\ufb01cant boosts performance. We \ufb01nd that incorpo-\nrating extra statistical and string features yields a\nsmall extra improvement in performance.\nThe \ufb01nal micro and macro P@1 scores on\nCoNLL-Y AGO test-b are displayed in table 3. On\nthis dataset our model achieves comparable re-\nsults, however it does not outperform the state-\nof-the-art, probably because of the relatively small\ntraining set and our reliance on domain adaptation.\n5.3 Effects of initialized embeddings and\ncorrupt-sampling schemes\nWe performed a study of the effects of using\npre-initialized embeddings for our model, and of\nusing either All-Entity or Near-Misses corrupt-\nsampling. The evaluation was done on a10% sam-\nple of the evaluation set of the WikilinksNED cor-\npus and can be seen in Table 4.\nWe have found that using pre-initialized embed-\ndings results in signi\ufb01cant performance gains, due\nto the better starting point. We have also found\nthat using Near-Misses, our model achieves sig-\nni\ufb01cantly improved performance. We attribute this\n6Learning rate of 0.02; maximal tree depth of 4; 10, 000\ntrees.\n\ntive context model for entity resolution. Transac-\ntions of the Association of Computational Linguis-\ntics 3:503\u2013515. http://aclweb.org/anthology/Q15-\n1036.\nOmer Levy and Yoav Goldberg. 2014a. Dependency-\nbased word embeddings. In Proceedings of the 52nd\nAnnual Meeting of the Association for Computa-\ntional Linguistics (Volume 2: Short Papers). Associ-\nation for Computational Linguistics, pages 302\u2013308.\nhttps://doi.org/10.3115/v1/P14-2050.\nOmer Levy and Yoav Goldberg. 2014b. Neural word\nembedding as implicit matrix factorization. In Ad-\nvances in Neural Information Processing Systems\n27: Annual Conference on Neural Information\nProcessing Systems 2014, December 8-13 2014,\nMontreal, Quebec, Canada . pages 2177\u20132185.\nhttp://papers.nips.cc/paper/5477-neural-word-\nembedding-as-implicit-matrix-factorization.\nOmer Levy, Steffen Remus, Chris Biemann, and Ido\nDagan. 2015. Do supervised distributional methods\nreally learn lexical inference relations? In Proceed-\nings of the 2015 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies . Associa-\ntion for Computational Linguistics, pages 970\u2013976.\nhttps://doi.org/10.3115/v1/N15-1098.\nEdgar Meij, Wouter Weerkamp, and Maarten de Ri-\njke. 2012. Adding semantics to microblog posts.\nIn Proceedings of the Fifth ACM International Con-\nference on Web Search and Data Mining . ACM,\nNew York, NY , USA, WSDM \u201912, pages 563\u2013572.\nhttps://doi.org/10.1145/2124295.2124364.\nRada Mihalcea and Andras Csomai. 2007. Wik-\nify!: Linking documents to encyclopedic\nknowledge. In Proceedings of the Sixteenth\nACM Conference on Conference on Informa-\ntion and Knowledge Management . ACM, New\nYork, NY , USA, CIKM \u201907, pages 233\u2013242.\nhttps://doi.org/10.1145/1321440.1321475.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013. Distributed\nrepresentations of words and phrases and their com-\npositionality. In Advances in Neural Information\nProcessing Systems 26: 27th Annual Conference\non Neural Information Processing Systems 2013.\nProceedings of a meeting held December 5-8,\n2013, Lake Tahoe, Nevada, United States. . pages\n3111\u20133119. http://papers.nips.cc/paper/5021-\ndistributed-representations-of-words-and-phrases-\nand-their-compositionality.\nLili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu,\nLu Zhang, and Zhi Jin. 2016. How transfer-\nable are neural networks in nlp applications? In\nProceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics, pages 479\u2013489.",
            0.8013190409999993
          ]
        },
        {
          "answer": "The quality control for ensuring the text is noisy but the annotations are accurate is not explicitly described in the provided context. The context mainly discusses the performance of the model, the datasets used, the challenges of dealing with noisy text, and some error analysis. It does not detail specific quality control measures taken to ensure the accuracy of annotations amidst noisy text.",
          "relevant_chunks": [
            "duce the impact of noise in the text.\nOur experiments show that our model signi\ufb01-\ncantly outperforms existing state-of-the-art NED\nalgorithms on WikilinksNED, suggesting that\nRNNs with attention are able to model short and\nnoisy context better than current approaches. In\naddition, we evaluate our algorithm on CoNLL-\nY AGO (Hoffart et al., 2011), a dataset of annotated\nnews articles. We use a simple domain adapta-\ntion technique since CoNLL-Y AGO lacks a large\nenough training set for our model, and achieve\ncomparable results to other state-of-the-art meth-\nods. These experiments highlight the difference\nbetween the two datasets, indicating that our NED\nbenchmark is substantially more challenging.\nCode and data used for our experiments\ncan be found at https://github.com/\nyotam-happy/NEDforNoisyText\n2 Related Work\nLocal vs Global NED Early work on Named\nEntity Disambiguation, such as Bunescu and\nPas \u00b8ca (2006) and Mihalcea and Csomai (2007) fo-\ncused on local approaches where each mention is\ndisambiguated separately using hand-crafted fea-\ntures. While local approaches provide a hard-to-\nbeat baseline (Ratinov et al., 2011), recent work\nhas largely focused on global approaches. These\ndisambiguate all mentions within a document si-\nmultaneously by considering the coherency of en-\ntity assignments within a document. For exam-\nple the local component of the GLOW algorithm\n(Ratinov et al., 2011) was used as part of the re-\nlational inference system suggested by Cheng and\nRoth (2013). Similarly, Globerson et al. (2016)\nachieved state-of-the-art results by extending the\nlocal-based selective-context model of Lazic et al.\n(2015) with an attention-like coherence model.\nGlobal models can tap into highly-\ndiscriminative semantic signals (e.g. coreference\nand entity relatedness) that are unavailable to local\nmethods, and have signi\ufb01cantly outperformed the\nlocal approach on standard datasets (Guo and Bar-\nbosa, 2014; Pershina et al., 2015; Globerson et al.,\n2016). However, global approaches are dif\ufb01cult\nto apply in domains where only short and noisy\ntext is available, as often occurs in social media,\nquestions and answers, and other short web docu-\nments. For example, Huang et al. (2014) collected\nmany tweets from the same author in order to\n\nman reader. The second prominent type of \u2019almost\ncorrect\u2019 errors where differentiating between ad-\njectives that are used to describe properties of a na-\ntion. Particularity, mentions such as \u2019Germanic\u2019,\n\u2019Chinese\u2019and \u2019Dutch\u2019were falsely assigned to en-\ntities that describe language instead of people, and\nvice versa. We observed this type of mistake in\n8.4% of the errors (11/130).\nAnother interesting type of errors where in\ncases where the correct entity had insuf\ufb01cient\ntraining. We de\ufb01ned insuf\ufb01cient training errors as\nerrors where the correct entity appeared less than\n10 times in the training data. We saw that the\nmodel followed the MPS in 75% of these cases,\nshowing that our model tends to follow the base-\nline in such cases. Further, the amount of gen-\neralization error in insuf\ufb01cient-training conditions\nwas also signi\ufb01cant (35.7%), as our model tended\nto select more general entities.\n7 Conclusions\nOur results indicate that the expressibility of\nattention-RNNs indeed allows us to extract use-\nful features from noisy context, when suf\ufb01cient\namounts of training examples are available. This\nallows our model to signi\ufb01cantly out-perform ex-\nisting state-of-the-art models. We \ufb01nd that both\nusing pre-initialized embedding vocabularies, and\nthe corrupt-sampling method employed are very\nimportant for properly training our model.\nHowever, the gap between results of all systems\ntested on both CoNLL-Y AGO and WikilinksNED\nindicates that mentions with noisy context are in-\ndeed a challenging test. We believe this to be\nan important real-world scenario, that represents\na distinct test-case that \ufb01lls a gap between existing\nnews-based datasets and the much noisier Twitter\ndata (Ritter et al., 2011) that has received increas-\ning attention. We \ufb01nd recurrent neural models are\na promising direction for this task.\nFinally, our error analysis shows a number of\npossible improvements that should be addressed.\nSince we use the training set for candidate genera-\ntion, non-nonsensical candidates (i.e. disambigua-\ntion pages) cause our model to err and should be\nremoved from the candidate set. In addition, we\nobserve that lack of suf\ufb01cient training for long-\n\ntail entities is still a problem, even when a large\ntraining set is available. We believe this, and some\nsubtle semantic cases (book/movie) can be at least\npartially addressed by considering semantic prop-\n\ndataset contains various kinds of noise, especially\ndue to incoherent contexts. The contextual noise\npresents an interesting test-case that supplements\nexisting datasets that are sourced from mostly co-\nherent and well-formed text.\nTo get a sense of textual noise we have set up\na small experiment where we measure the similar-\nity between entities mentioned in WikilinksNED\nand their surrounding context, and compare the\nresults to CoNLL-Y AGO. We use state-of-the-art\nword and entity embeddings obtained from Ya-\nmada et al. (2016) and compute cosine similarity\nbetween embeddings of the correct entity assign-\nment and the mean of context words. We com-\npare results from all mentions in CoNLL-Y AGO\nto a sample of 50000 web fragments taken from\nWikilinksNED, using a window of words of size\n40 around entity mentions. We \ufb01nd that similar-\nity between context and correct entity is indeed\nlower for web mentions (0.163) than for CoNLL-\nY AGO mentions (0.188), and \ufb01nd this result to be\nstatistically signi\ufb01cant with very high probability\n(p < 10\u22125) . This result indicates that web frag-\nments in WikilinksNED are indeed noisier com-\npared to CoNLL-Y AGO documents.\nWe prepare our dataset from the local-context\nversion of Wikilinks 1, and resolve ground-truth\nlinks using a Wikipedia dump from April 2016 2.\nWe use the page and redirect tables for resolution,\nand keep the database pageid column as a unique\nidenti\ufb01er for Wikipedia entities. We discard men-\ntions where the ground-truth could not be resolved\n(only 3% of mentions).\nWe collect all pairs of mention m and entity e\nappearing in the dataset, and compute the number\nof times m refers to e (#(m, e)), as well as the\nconditional probability of e given m: P(e|m) =\n#(m, e)/ \u2211\ne\u2032 #(m, e\u2032). Examining these distri-\nbutions reveals many mentions belong to two ex-\ntremes \u2013 either they have very little ambiguity,\nor they appear in the dataset only a handful of\ntimes and refer to different entities only a couple\nof times each. We deem the former to be less in-\nteresting for the purpose of NED, and suspect the\nlatter to be noise with high probability. To \ufb01lter\nthese cases, we keep only mentions for which at\nleast two different entities have 10 mentions each",
            0.4685404589999962
          ]
        }
      ],
      "reference_answers": [
        "They initialize their word and entity embeddings with vectors pre-trained over a large corpus of unlabeled data.",
        "The authors believe that the Wikilinks corpus  contains ground truth annotations while being noisy. They discard mentions that cannot have ground-truth verified by comparison with Wikipedia."
      ]
    },
    "Risk_Prediction_of_Digital_Human_Resource_Manageme.pdf": {
      "error": "No valid question-answer pairs to evaluate",
      "average_exact_match": 0.0,
      "average_bleu_score": 0.0,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        },
        "rouge-2": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        },
        "rouge-L": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        }
      },
      "per_question_metrics": []
    },
    "1707.02377.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.02300253760758204,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.1405493289054933,
          "recall": 0.6010101010101009,
          "f1": 0.2089371980676328
        },
        "rouge-2": {
          "precision": 0.046153846153846156,
          "recall": 0.10344827586206896,
          "f1": 0.06382978723404255
        },
        "rouge-L": {
          "precision": 0.07682994326829944,
          "recall": 0.3797979797979798,
          "f1": 0.1146911663216011
        }
      },
      "per_question_metrics": [
        {
          "question": "Which language models do they compare against?",
          "generated_answer": "The document compares against the following language models:\n\n1. Bag-of-Words (BoW)\n2. Denoising Autoencoders (DEA)\n3. Word2Vec + IDF\n4. Doc2Vec\n5. Skip-thought Vectors\n6. RNNLM (Recurrent Neural Network Language Model)\n7. LSTM-based methods (specifically mentioned for the semantic relatedness task)",
          "reference_answer": "RNNLM BIBREF11",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.022727272727272728,
              "recall": 0.5,
              "f1": 0.04347826086956522
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.022727272727272728,
              "recall": 0.5,
              "f1": 0.04347826086956522
            }
          },
          "bleu_score": 0.00366753025392797,
          "exact_match": 0.0
        },
        {
          "question": "Is their approach similar to making an averaged weighted sum of word vectors, where weights reflect word frequencies?",
          "generated_answer": "No, their approach, as described, does not directly involve making an averaged weighted sum of word vectors where weights reflect word frequencies. Instead, it represents each document as an average of the embeddings of words randomly sampled from the document, with a corruption mechanism that randomly removes a significant portion of words. This method differs from simply weighting word vectors by their frequencies in the document.",
          "reference_answer": "Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained.",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.30303030303030304,
              "recall": 0.6666666666666666,
              "f1": 0.41666666666666663
            },
            "rouge-2": {
              "precision": 0.13846153846153847,
              "recall": 0.3103448275862069,
              "f1": 0.19148936170212766
            },
            "rouge-L": {
              "precision": 0.16666666666666666,
              "recall": 0.36666666666666664,
              "f1": 0.22916666666666663
            }
          },
          "bleu_score": 0.06165337672411484,
          "exact_match": 0.0
        },
        {
          "question": "How do they determine which words are informative?",
          "generated_answer": "They determine which words are informative by using a data-dependent regularization method introduced in Doc2VecC, which suppresses the embeddings of frequent but uninformative words, such as symbols and stop words. This approach contrasts with other methods where least frequent words might have embeddings close to zero despite being indicative of sentiment. Doc2VecC specifically manages to clamp down the representation of words that frequently appear in the training set but are considered uninformative.",
          "reference_answer": "Informative are those that will not be suppressed by regularization performed.",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.0958904109589041,
              "recall": 0.6363636363636364,
              "f1": 0.16666666666666663
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.0410958904109589,
              "recall": 0.2727272727272727,
              "f1": 0.07142857142857142
            }
          },
          "bleu_score": 0.0036867058447033006,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "Which language models do they compare against?",
        "Is their approach similar to making an averaged weighted sum of word vectors, where weights reflect word frequencies?",
        "How do they determine which words are informative?"
      ],
      "generated_answers": [
        {
          "answer": "The document compares against the following language models:\n\n1. Bag-of-Words (BoW)\n2. Denoising Autoencoders (DEA)\n3. Word2Vec + IDF\n4. Doc2Vec\n5. Skip-thought Vectors\n6. RNNLM (Recurrent Neural Network Language Model)\n7. LSTM-based methods (specifically mentioned for the semantic relatedness task)",
          "relevant_chunks": [
            "Method Pearson\u2019s\u03b3 Spearman\u2019s\u03c1 MSE\nIllinois-LH 0.7993 0.7538 0.3692\nUNAL-NLP 0.8070 0.7489 0.3550\nMeaning Factory 0.8268 0.7721 0.3224\nECNU 0.8279 0.7689 0.3250\nMean vectors (Word2Vec + avg) 0.7577 0.6738 0.4557\nDT-RNN (Socher et al., 2014) 0.7923 0.7319 0.3822\nSDT-RNN (Socher et al., 2014) 0.7900 0.7304 0.3848\nLSTM (Tai et al., 2015) 0.8528 0.7911 0.2831\nBidirectional LSTM (Tai et al., 2015) 0.8567 0.7966 0.2736\nDependency Tree-LSTM (Tai et al., 2015) 0.8676 0.8083 0.2532\ncombine-skip (Kiros et al., 2015) 0.8584 0.7916 0.2687\nDoc2VecC 0.8381 0.7621 0.3053\nREFERENCES\nBlei, D. M., Ng, A. Y ., and Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine\nLearning research, 3(Jan):993\u20131022.\nChen, M., Weinberger, K. Q., Sha, F., and Bengio, Y . (2014). Marginalized denoising auto-encoders\nfor nonlinear representations. In ICML, pages 1476\u20131484.\nChen, M., Xu, Z., Weinberger, K., and Sha, F. (2012). Marginalized denoising autoencoders for\ndomain adaptation. arXiv preprint arXiv:1206.4683.\nCroft, B. and Lafferty, J. (2013).Language modeling for information retrieval, volume 13. Springer\nScience & Business Media.\nDai, A. M. and Le, Q. V . (2015). Semi-supervised sequence learning. In Advances in Neural\nInformation Processing Systems, pages 3079\u20133087.\nDai, A. M., Olah, C., and Le, Q. V . (2015). Document embedding with paragraph vectors. arXiv\npreprint arXiv:1507.07998.\nDeerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. (1990). Indexing\nby latent semantic analysis. Journal of the American society for information science, 41(6):391.\nFan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and Lin, C.-J. (2008). Liblinear: A library for\nlarge linear classi\ufb01cation. JMLR, 9(Aug):1871\u20131874.\nGlorot, X., Bordes, A., and Bengio, Y . (2011). Domain adaptation for large-scale sentiment classi-\n\ufb01cation: A deep learning approach. In ICML, pages 513\u2013520.\nGrefenstette, E., Dinu, G., Zhang, Y .-Z., Sadrzadeh, M., and Baroni, M. (2013). Multi-step regres-\nsion learning for compositional distributional semantics. arXiv preprint arXiv:1301.6939.\n\n4.3 W ORD ANALOGY\nIn table 3, we demonstrated that the corruption model introduced in Doc2VecC dampens the embed-\ndings of words which are common and non-discriminative (stop words). In this experiment, we are\ngoing to quantatively compare the word embeddings generated by Doc2VecC to the ones generated\nby Word2Vec, or Paragraph Vectors on the word analogy task introduced by Mikolov et al. (2013a).\nThe dataset contains \ufb01ve types of semantic questions, and nine types of syntactic questions, with a\ntotal of 8,869 semantic and 10,675 syntactic questions. The questions are answered through simple\nlinear algebraic operations on the word embeddings generated by different methods. Please refer to\nthe original paper for more details on the evaluation protocol.\n2As reported in the original paper, training of the skip-thought vector model on the book corpus dataset\ntakes around 2 weeks on GPU.\n7\n\nPublished as a conference paper at ICLR 2017\n1M 2M 4M 8M 15M\n0\n20\n40\n60\n3.8 6.1 8.3 9.1\n13.3\n18.7\n26.4\n32.7\n36.1 38.9\n20.3\n28.1\n36.4\n42.5\n46.7\nNumber of paragraphs used for learning\nAccuracy (%)\nParagraphVectorsWord2VecDoc2VecC\n(a) h=50\n1M 2M 4M 8M 15M\n0\n20\n40\n60\n5.1 7.5\n10.9 10.2 10.2\n23.6\n34.7\n42.4\n48.2 50.7\n24.3\n34.1\n44.1\n52.6\n58.2\nNumber of paragraphs used for learning\nParagraphVectorsWord2VecDoc2VecC\n(b) h=100\nFigure 2: Accuracy on subset of the Semantic-Syntactic Word Relationship test set. Only questions\ncontaining words from the most frequent 30k words are included in the test.\nSemantic questions Word2Vec Doc2VecC Syntactic questions Word2Vec Doc2VecC\ncapital-common-countries 73.59 81.82 gram1-adjective-to-adverb 19.25 20.32\ncapital-world 67.94 77.96 gram2-opposite 14.07 25.54\ncurrency 17.14 12.86 gram3-comparative 60.21 74.47\ncity-in-state 34.49 42.86 gram4-superlative 52.87 55.40\nfamily 68.71 64.62 gram5-present-participle 56.34 65.81\ngram6-nationality-adjective 88.71 91.03\ngram7-past-tense 47.05 51.86\ngram8-plural 50.28 61.27\ngram9-plural-verbs 25.38 39.69\nTable 4: Top 1 accuracy on the 5 type of semantics and 9 types of syntactic questions.\nWe trained the word embeddings of different methods using the English news dataset released under\nthe ACL workshop on statistical machine translation. The training set includes close to 15M para-\n\net al., 2013) and denoising autoencoders (Chen et al., 2014).\n4 E XPERIMENTS\nWe evaluate Doc2VecC on a sentiment analysis task, a document classi\ufb01cation task and a semantic\nrelatedness task, along with several document representation learning algorithms. All experiments\ncan be reproduced using the code available at https://github.com/mchen24/iclr2017\n4.1 B ASELINES\nWe compare against the following document representation baselines: bag-of-words (BoW); De-\nnoising Autoencoders (DEA) (Vincent et al., 2008), a representation learned from reconstruct-\ning original document x using corrupted one \u02dcx. SDAs have been shown to be the state-of-the-art\nfor sentiment analysis tasks (Glorot et al., 2011). We used Kullback-Liebler divergence as the re-\nconstruction error and an af\ufb01ne encoder. To scale up the algorithm to large vocabulary, we only\ntake into account the non-zero elements of x in the reconstruction error and employed negative\nsampling for the remainings; Word2Vec (Mikolov et al., 2013a)+IDF, a representation generated\nthrough weighted average of word vectors learned using Word2Vec; Doc2Vec (Le and Mikolov,\n2014); Skip-thought Vectors(Kiros et al., 2015), a generic, distributed sentence encoder that ex-\ntends the Word2Vec skip-gram model to sentence level. It has been shown to produce highly generic\nsentence representations that apply to various natural language processing tasks. We also include\nRNNLM (Mikolov et al., 2010), a recurrent neural network based language model in the compar-\nison. In the semantic relatedness task, we further compare to LSTM-based methods (Tai et al.,\n2015) that have been reported on this dataset.\n5\n\nPublished as a conference paper at ICLR 2017\nTable 1: Classi\ufb01cation error of a linear classi\ufb01er trained on various document representations on the\nImdb dataset.\nModel Error rate % (include test) Error rate % (exclude test)\nBag-of-Words (BOW) 12.53 12.59\nRNN-LM 13.59 13.59\nDenoising Autoencoders (DEA) 11.58 12.54\nWord2Vec + A VG 12.11 12.69\nWord2Vec + IDF 11.28 11.92\nParagraph Vectors 10.81 12.10\nSkip-thought Vectors - 17.42\nDoc2VecC 10.48 11.70\n4.2 S ENTIMENT ANALYSIS\nFor sentiment analysis, we use the IMDB movie review dataset. It contains 100,000 movies reviews\ncategorized as either positive or negative. It comes with prede\ufb01ned train/test split (Maas et al.,",
            0.4157049579999921
          ]
        },
        {
          "answer": "No, their approach, as described, does not directly involve making an averaged weighted sum of word vectors where weights reflect word frequencies. Instead, it represents each document as an average of the embeddings of words randomly sampled from the document, with a corruption mechanism that randomly removes a significant portion of words. This method differs from simply weighting word vectors by their frequencies in the document.",
          "relevant_chunks": [
            "ct \u2208Rv\u00d71: BoW of the local context wt\u2212k,\u00b7\u00b7\u00b7 ,wt\u22121,wt+1,\u00b7\u00b7\u00b7 ,wt+k at the target position t.\nct\nj = 1 iff word jappears within the sliding window of the target;\nU \u2208Rh\u00d7v: the projection matrix from the input space to a hidden space of size h. We use uw to\ndenote the column in U for word w, i.e., the \u201cinput\u201c vector of wordw;\nV\u22a4 \u2208Rv\u00d7h: the projection matrix from the hidden space to output. Similarly, we use vw to\ndenote the column in V for word w, i.e., the \u201coutput\u201c vector of wordw.\nWord2Vec. Word2Vec proposed a neural network architecture of an input layer, a projection layer\nparameterized by the matrix U and an output layer by V\u22a4. It de\ufb01nes the probability of observing\nthe target word wt in a document Dgiven its local context ct as\nP(wt|ct) = exp(v\u22a4\nwt Uct)\u2211\nw\u2032\u2208V exp(v\u22a4\nw\u2032 Uct)\nThe word vectors are then learned to maximize the log likelihood of observing the target word at\neach position of the document. Various techniques (Mitchell and Lapata, 2010; Zanzotto et al.,\n2010; Yessenalina and Cardie, 2011; Grefenstette et al., 2013; Socher et al., 2013; Kusner et al.,\n2\n\nPublished as a conference paper at ICLR 2017\n2015) have been studied to generate vector representations of documents from word embeddings,\namong which the simplest approach is to use weighted average of word embeddings. Similarly,\nour method forms document representation by averaging word embeddings of all the words in the\ndocument. Differently, as our model encodes the compositionality of words in the learned word\nembeddings, heuristic weighting at test time is not required.\nParagraph Vectors. Paragraph Vectors, on the other hands, explicitly learns a document vector\nwith the word embeddings. It introduces another projection matrix D \u2208Rh\u00d7n. Each column of D\nacts as a memory of the global topic of the corresponding document. It then de\ufb01nes the probability\nof observing the target word wt in a document Dgiven its local context ct as\nP(wt|ct,d) = exp(v\u22a4\nwt (Uct + d))\u2211\nw\u2032\u2208V exp(v\u22a4\nw\u2032 (Uct + d))\nwhere d \u2208D is the vector representation of the document. As we can see from this formula, the\ncomplexity of Paragraph Vectors grows with not only the size of the vocabulary, but also the size of\n\nw\u2032 (Uct + d))\nwhere d \u2208D is the vector representation of the document. As we can see from this formula, the\ncomplexity of Paragraph Vectors grows with not only the size of the vocabulary, but also the size of\nthe training corpus. While we can reasonably limit the size of a vocabulary to be within a million\nfor most datasets, the size of a training corpus can easily go to billions. What is more concerning is\nthat, in order to come up with the vector representations of unseen documents, we need to perform\nan expensive inference by appending more columns to D and gradient descent on D while \ufb01xing\nother parameters of the learned model.\n3 M ETHOD\nSeveral works (Mikolov and Dean, 2013; Mikolov et al., 2013b) showcased that syntactic and se-\nmantic regularities of phrases and sentences are reasonably well preserved by adding or subtract-\ning word embeddings learned through Word2Vec. It prompts us to explore the option of simply\nrepresenting a document as an average of word embeddings. Figure 1 illustrates the new model\narchitecture.\nwt#1 Wt+1 Wt+2 wp wq wr\nwt\nopening for the performance praised brazil\nceremony\nword<vectors\ndocument< vector\nAverage/Concatenate\nFigure 1: A new framework for learning document vectors.\nSimilar to Word2Vec or Paragraph Vectors, Doc2VecC consists of an input layer, a projection layer\nas well as an output layer to predict the target word, \u201cceremony\u201d in this example. The embeddings of\nneighboring words (\u201copening\u201d, \u201cfor\u201d, \u201cthe\u201d) provide local context while the vector representation of\nthe entire document (shown in grey) serves as the global context. In contrast to Paragraph Vectors,\nwhich directly learns a unique vector for each document, Doc2VecC represents each document as\nan average of the embeddings of words randomly sampled from the document (\u201cperformance\u201d at\nposition p, \u201cpraised\u201d at position q, and \u201cbrazil\u201d at position r).\nHuang et al. (2012) also proposed the idea of using average of word embeddings to represent the\nglobal context of a document. Different from their work, we choose to corrupt the original document\nby randomly removing signi\ufb01cant portion of words, and represent the document using only the\nembeddings of the words remained. This corruption mechanism offers us great speedup during\ntraining as it signi\ufb01cantly reduces the number of parameters to update in back propagation. At the\n\nnew framework implicitly introduces a data-dependent regularization, which favors rare or informa-\ntive words and suppresses words that are common but not discriminative; 4. Vector representation\nof a document can be generated by simply averaging the learned word embeddings of all the words\nin the document, which signi\ufb01cantly boost test ef\ufb01ciency; 5. The vector representation generated by\nDoc2VecC matches or beats the state-of-the-art for sentiment analysis, document classi\ufb01cation as\nwell as semantic relatedness tasks.\n2 R ELATED WORKS AND NOTATIONS\nText representation learning has been extensively studied. Popular representations range from the\nsimplest BoW and its term-frequency based variants (Salton and Buckley, 1988), language model\nbased methods (Croft and Lafferty, 2013; Mikolov et al., 2010; Kim et al., 2015), topic mod-\nels (Deerwester et al., 1990; Blei et al., 2003), Denoising Autoencoders and its variants (Vincent\net al., 2008; Chen et al., 2012), and distributed vector representations (Mesnil et al., 2014; Le and\nMikolov, 2014; Kiros et al., 2015). Another prominent line of work includes learning task-speci\ufb01c\ndocument representation with deep neural networks, such as CNN (Zhang and LeCun, 2015) or\nLSTM based approaches (Tai et al., 2015; Dai and Le, 2015).\nIn this section, we brie\ufb02y introduce Word2Vec and Paragraph Vectors, the two approaches that are\nmost similar to ours. There are two well-know model architectures used for both methods, referred\nto as Continuous Bag-of-Words (CBoW) and Skipgram models (Mikolov et al., 2013a). In this\nwork, we focus on CBoW. Extending to Skipgram is straightforward. Here are the notations we are\ngoing to use throughout the paper:\nD= {D1,\u00b7\u00b7\u00b7 ,Dn}: a training corpus of size n, in which each document Di contains a variable-\nlength sequence of words w1\ni ,\u00b7\u00b7\u00b7 ,wTi\ni ;\nV: the vocabulary used in the training corpus, of sizes v;\nx \u2208Rv\u00d71: BoW of a document, where xj = 1 iff word jdoes appear in the document.\nct \u2208Rv\u00d71: BoW of the local context wt\u2212k,\u00b7\u00b7\u00b7 ,wt\u22121,wt+1,\u00b7\u00b7\u00b7 ,wt+k at the target position t.\nct\nj = 1 iff word jappears within the sliding window of the target;",
            1.0931993749999833
          ]
        },
        {
          "answer": "They determine which words are informative by using a data-dependent regularization method introduced in Doc2VecC, which suppresses the embeddings of frequent but uninformative words, such as symbols and stop words. This approach contrasts with other methods where least frequent words might have embeddings close to zero despite being indicative of sentiment. Doc2VecC specifically manages to clamp down the representation of words that frequently appear in the training set but are considered uninformative.",
          "relevant_chunks": [
            "4.3 W ORD ANALOGY\nIn table 3, we demonstrated that the corruption model introduced in Doc2VecC dampens the embed-\ndings of words which are common and non-discriminative (stop words). In this experiment, we are\ngoing to quantatively compare the word embeddings generated by Doc2VecC to the ones generated\nby Word2Vec, or Paragraph Vectors on the word analogy task introduced by Mikolov et al. (2013a).\nThe dataset contains \ufb01ve types of semantic questions, and nine types of syntactic questions, with a\ntotal of 8,869 semantic and 10,675 syntactic questions. The questions are answered through simple\nlinear algebraic operations on the word embeddings generated by different methods. Please refer to\nthe original paper for more details on the evaluation protocol.\n2As reported in the original paper, training of the skip-thought vector model on the book corpus dataset\ntakes around 2 weeks on GPU.\n7\n\nPublished as a conference paper at ICLR 2017\n1M 2M 4M 8M 15M\n0\n20\n40\n60\n3.8 6.1 8.3 9.1\n13.3\n18.7\n26.4\n32.7\n36.1 38.9\n20.3\n28.1\n36.4\n42.5\n46.7\nNumber of paragraphs used for learning\nAccuracy (%)\nParagraphVectorsWord2VecDoc2VecC\n(a) h=50\n1M 2M 4M 8M 15M\n0\n20\n40\n60\n5.1 7.5\n10.9 10.2 10.2\n23.6\n34.7\n42.4\n48.2 50.7\n24.3\n34.1\n44.1\n52.6\n58.2\nNumber of paragraphs used for learning\nParagraphVectorsWord2VecDoc2VecC\n(b) h=100\nFigure 2: Accuracy on subset of the Semantic-Syntactic Word Relationship test set. Only questions\ncontaining words from the most frequent 30k words are included in the test.\nSemantic questions Word2Vec Doc2VecC Syntactic questions Word2Vec Doc2VecC\ncapital-common-countries 73.59 81.82 gram1-adjective-to-adverb 19.25 20.32\ncapital-world 67.94 77.96 gram2-opposite 14.07 25.54\ncurrency 17.14 12.86 gram3-comparative 60.21 74.47\ncity-in-state 34.49 42.86 gram4-superlative 52.87 55.40\nfamily 68.71 64.62 gram5-present-participle 56.34 65.81\ngram6-nationality-adjective 88.71 91.03\ngram7-past-tense 47.05 51.86\ngram8-plural 50.28 61.27\ngram9-plural-verbs 25.38 39.69\nTable 4: Top 1 accuracy on the 5 type of semantics and 9 types of syntactic questions.\nWe trained the word embeddings of different methods using the English news dataset released under\nthe ACL workshop on statistical machine translation. The training set includes close to 15M para-\n\nthat(275240) time(48205) endeavor(100) here(21118) way(31302) own(13456)\nrepresentation. Paragraph Vectors, on the other hand, requires another round of inference to produce\nthe vector representation of unseen test documents. It takes Paragraph Vectors 4 minutes and 17\nseconds to infer the vector representations for the 25,000 test documents, in comparison to 7 seconds\nfor the other methods. As we did not re-train the Skip-thought vector models on this dataset, the\ntraining time2 reported in the table is the time it takes to generate the embeddings for the 25,000\ntraining documents. Due to repeated high-dimensional matrix operations required for encoding long\nparagraphs, it takes fairly long time to generate the representations for these documents. Similarly\nfor testing. The experiments were conducted on a desktop with Intel i7 2.2Ghz cpu.\nData dependent regularization. As explained in Section 3.1, the corruption introduced in\nDoc2VecC acts as a data-dependent regularization that suppresses the embeddings of frequent but\nuninformative words. Here we conduct an experiment to exam the effect. We used a cutoff of 100\nin this experiment. Table 3 lists the words having the smallest l2 norm of embeddings found by\ndifferent algorithms. The number inside the parenthesis after each word is the number of times this\nword appears in the learning set. In word2Vec or Paragraph Vectors, the least frequent words have\nembeddings that are close to zero, despite some of them being indicative of sentiment such as deba-\ncle, bliss and shabby. In contrast, Doc2VecC manages to clamp down the representation of words\nfrequently appear in the training set, but are uninformative, such as symbols and stop words.\nSubsampling frequent words. Note that for all the numbers reported, we applied the trick of\nsubsampling of frequent words introduced in (Mikolov and Dean, 2013) to counter the imbalance\nbetween frequent and rare words. It is critical to the performance of simple Word2Vec+A VG as the\nsole remedy to diminish the contribution of common words in the \ufb01nal document representation. If\nwe were to remove this step, the error rate of Word2Vec+A VG will increases from12.1% to 13.2%.\nDoc2VecC on the other hand naturally exerts a stronger regularization toward embeddings of words\nthat are frequent but uninformative, therefore does not rely on this trick.\n4.3 W ORD ANALOGY\nIn table 3, we demonstrated that the corruption model introduced in Doc2VecC dampens the embed-\ndings of words which are common and non-discriminative (stop words). In this experiment, we are\n\nWe trained the word embeddings of different methods using the English news dataset released under\nthe ACL workshop on statistical machine translation. The training set includes close to 15M para-\ngraphs with 355M tokens. We compare the performance of word embeddings trained by different\nmethods with increasing embedding dimensionality as well as increasing training data.\nWe observe similar trends as in Mikolov et al. (2013a). Increasing embedding dimensionality as\nwell as training data size improves performance of the word embeddings on this task. However, the\nimprovement is diminishing. Doc2VecC produces word embeddings which performs signi\ufb01cantly\nbetter than the ones generated by Word2Vec. We observe close to 20% uplift when we train on the\nfull training corpus. Paragraph vectors on the other hand performs surprisingly bad on this dataset.\nOur hypothesis is that due to the large capacity of the model architecture, Paragraph Vectors relies\nmostly on the unique document vectors to capture the information in a text document instead of\nlearning the word semantic or syntactic similarities. This also explains why the PV-DBOW Le and\nMikolov (2014) model architecture proposed in the original work, which completely removes word\nembedding layers, performs comparable to the distributed memory version.\nIn table 5, we list a detailed comparison of the performance of word embeddings generated by\nWord2Vec and Doc2VecC on the 14 subtasks, when trained on the full dataset with embedding of\nsize 100. We can see that Doc2VecC signi\ufb01cantly outperforms the word embeddings produced by\nWord2Vec across almost all the subtasks.\n4.4 D OCUMENT CLASSIFICATION\nFor the document classi\ufb01cation task, we use a subset of the wikipedia dump, which contains over\n300,000 wikipedia pages in 100 categories. The 100 categories includes categories under sports,\n8\n\nPublished as a conference paper at ICLR 2017\nTable 5: Classi\ufb01cation error (%) of a linear classi\ufb01er trained on various document representations\non the Wikipedia dataset.\nModel BOW DEA Word2Vec + A VG Word2Vec + IDF ParagraphVectors Doc2VecC\nh= 100 36.03 32.30 33.2 33.16 35.78 31.92\nh= 200 36.03 31.36 32.46 32.48 34.92 30.84\nh= 500 36.03 31.10 32.02 32.13 33.93 30.43\nh= 1000 36.03 31.13 31.78 32.06 33.02 30.24\n(a) Doc2Vec\n (b) Doc2VecC\nFigure 3: Visualization of document vectors on Wikipedia dataset using t-SNE.\nentertainment, literature, and politics etc. Examples of categories include American drama \ufb01lms,\nDirectorial debut \ufb01lms, Major League Baseball pitchers and Sydney Swans players. Body texts",
            0.5094708340000125
          ]
        }
      ],
      "reference_answers": [
        "RNNLM BIBREF11",
        "Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained.",
        "Informative are those that will not be suppressed by regularization performed."
      ]
    },
    "1801.02243.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.05635263667569492,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.08664525625585441,
          "recall": 0.4523809523809524,
          "f1": 0.14473684210526316
        },
        "rouge-2": {
          "precision": 0.06521739130434782,
          "recall": 0.3333333333333333,
          "f1": 0.10909090909090909
        },
        "rouge-L": {
          "precision": 0.0835006021678041,
          "recall": 0.42857142857142855,
          "f1": 0.1391812865497076
        }
      },
      "per_question_metrics": [
        {
          "question": "Do the authors give any examples of major events which draw the public's attention and the impact they have on stock price?",
          "generated_answer": "No, the authors do not give specific examples of major events which draw the public's attention and the impact they have on stock price in the provided context.",
          "reference_answer": "Yes",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            }
          },
          "bleu_score": 0,
          "exact_match": 0.0
        },
        {
          "question": "Which tweets are used to output the daily sentiment signal?",
          "generated_answer": "The daily sentiment signal is output from tweets that have been preprocessed to remove irregular symbols, URLs, emojis, hashtags, mentions of users, tabs, extra spaces, tweets with consecutive question marks, and non-English tweets. After preprocessing, the sentiment of each tweet is evaluated using the Stanford CoreNLP software, which assigns a sentiment score ranging from 0 (negative) to 4 (very positive). For tweets containing multiple sentences, the average of the sentiment scores of all sentences is used as the sentiment score of the tweet. These scores are then normalized between 0 to 1 to construct and normalize features based on the sentiment score for further analysis.",
          "reference_answer": "Tesla and Ford are investigated on how Twitter sentiment could impact the stock price",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.04716981132075472,
              "recall": 0.35714285714285715,
              "f1": 0.08333333333333334
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.03773584905660377,
              "recall": 0.2857142857142857,
              "f1": 0.06666666666666667
            }
          },
          "bleu_score": 0.0021708234367587642,
          "exact_match": 0.0
        },
        {
          "question": "What is the baseline machine learning prediction approach?",
          "generated_answer": "The baseline machine learning prediction approach applies linear logistic regression to a set of stock technical signals to predict the following day's stock return sign (+/-). This baseline is the same as the machine learning model except that only the technical indicator was used as the feature.",
          "reference_answer": "linear logistic regression to a set of stock technical signals",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.2127659574468085,
              "recall": 1.0,
              "f1": 0.3508771929824561
            },
            "rouge-2": {
              "precision": 0.1956521739130435,
              "recall": 1.0,
              "f1": 0.32727272727272727
            },
            "rouge-L": {
              "precision": 0.2127659574468085,
              "recall": 1.0,
              "f1": 0.3508771929824561
            }
          },
          "bleu_score": 0.166887086590326,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "Do the authors give any examples of major events which draw the public's attention and the impact they have on stock price?",
        "Which tweets are used to output the daily sentiment signal?",
        "What is the baseline machine learning prediction approach?"
      ],
      "generated_answers": [
        {
          "answer": "No, the authors do not give specific examples of major events which draw the public's attention and the impact they have on stock price in the provided context.",
          "relevant_chunks": [
            "The contribution of this paper is two-fold: First, the pre-\ndicting power of Twitter sentiment is evaluated. Our results\nshow sentiment is more suitable to construct alpha signals\nrather than total return signals and shows predicting power\nespecially when the Twitter volume is high. Second, we\nproposed a trading strategy based on reinforcement learning\n(Q-learning) that takes the sentiment features as part of its\nstates.\nThe paper is constructed as follows: In the second section,\nscraping Tweets from Twitter website and preprocessing the\ndata are described in details. In the third section, assigning\nsentiment scores to the text data is discussed. In the fourth\nsection, feature engineering and prediction based on the\nsentiment score is discussed. In the \ufb01fth section, how the\nreinforcement learning is applied to generate the optimal\ntrading strategy is described.\nII. T WITTER DATA SCRAPING AND PREPROCESSING\nThere are two options of getting the Tweets. First, Twitter\nprovides an API to download the Tweets. However, rate limit\nand history limit make it not an option for this paper. Second,\nscrapping Tweets directly from Twitter website. Using the\nsecond option, the daily Tweets for stocks of interest from\n2015 January to 2017 June were downloaded.\nThe predicting power of Twitter sentiment varies from\nstock to stock. For stocks that are mostly driven by the com-\npany fundamentals and hold by the institutional investors,\nthe predicting power of the Twitter sentiment is limited.\nFor stocks that are priced by the public expectation on the\ncompany\u2019s future growth, Twitter sentiment describes the\ncon\ufb01dence and expectation level of the investors.\nFor this reason, two companies from the same industry,\nTesla and Ford are investigated on how Twitter sentiment\ncould impact the stock price. Tesla is an electronic car\ncompany that shows consecutive negative operating cash \ufb02ow\nand net income but carries very high expectation from the\npublic. Ford, is a traditional auto maker whose stock prices\nhas been stabilized to represent the company fundamentals.\nTo investigate how different key words impact the predict-\ning power of the sentiment score, two Tweet sets, a ticker\nset and a product set, are prepared for each stock. The \ufb01rst\nset of Tweets are searched strictly according to the stock\nticker. The second set of Tweets are searched according to\nthe company\u2019s products and news. The keywords for the\nsecond dataset are de\ufb01ned according to the top twenty related\n\nTrading the Twitter Sentiment with Reinforcement Learning\nCatherine Xiao\ncatherine.xiao1@gmail.com\nWanfeng Chen\nwanfengc@gmail.com\nAbstract\u2014 This paper is to explore the possibility to use\nalternative data and arti\ufb01cial intelligence techniques to trade\nstocks. The ef\ufb01cacy of the daily Twitter sentiment on predicting\nthe stock return is examined using machine learning methods.\nReinforcement learning(Q-learning) is applied to generate the\noptimal trading policy based on the sentiment signal. The\npredicting power of the sentiment signal is more signi\ufb01cant\nif the stock price is driven by the expectation on the company\ngrowth and when the company has a major event that draws\nthe public attention. The optimal trading strategy based on\nreinforcement learning outperforms the trading strategy based\non the machine learning prediction.\nI. INTRODUCTION\nIn a world where traditional \ufb01nancial information is ubiq-\nuitous and the \ufb01nancial models are largely homogeneous,\n\ufb01nding hidden information that has not been priced in from\nalternative data is critical. The recent development in Natural\nLanguage Processing provides such opportunities to look\ninto text data in addition to numerical data. When the\nmarket sets the stock price, it is not uncommon that the\nexpectation of the company growth outweighs the company\nfundamentals. Twitter, a online news and social network\nwhere the users post and interact with messages to express\nviews about certain topics, contains valuable information on\nthe public mood and sentiment. A collection of research [1]\n[6] have shown that there is a positive correlation between\nthe \u201dpublic mood\u201d and the \u201dmarket mood\u201d. Other research[2]\nalso shows that signi\ufb01cant correlation exists between the\nTwitter sentiment and the abnormal return during the peaks\nof the Twitter volume during a major event.\nOnce a signal that has predicting power on the stock\nmarket return is constructed, a trading strategy to express the\nview of the signal is needed. Traditionally, the quantitative \ufb01-\nnance industry relies on backtest, a process where the trading\nstrategies are tuned during the simulations or optimizations.\nReinforcement learning provides a way to \ufb01nd the optimal\npolicy by maximizing the expected future utility. There are\nrecent attempts from the Arti\ufb01cial Intelligence community\nto apply reinforcement learning to asset allocation [5], algo-\nrithmic trading[3][7], and portfolio management[4].\nThe contribution of this paper is two-fold: First, the pre-\ndicting power of Twitter sentiment is evaluated. Our results\nshow sentiment is more suitable to construct alpha signals\nrather than total return signals and shows predicting power\n\nset of Tweets are searched strictly according to the stock\nticker. The second set of Tweets are searched according to\nthe company\u2019s products and news. The keywords for the\nsecond dataset are de\ufb01ned according to the top twenty related\nkeywords of the stock ticker according to Google Trend, a\nweb facility shows how often a certain word is searched\nrelative to Google\u2019s total search volume. For example, \u201dElon\nMusk\u201d is among the set of keywords that retrieve the second\ntweets set for Tesla.\nTweets contain irregular symbols, url and emoji etc which\nhas to be preprocessed so that the NLP algorithm can extract\nthe relevant information ef\ufb01ciently. Examples of preprocess-\ning are described as below:\n\u2022 Filter out tweets that contains http or .com Motiva-\ntion: They\u2019re usually ads e.g. #Fhotoroom #iPhone\narXiv:1801.02243v1  [cs.AI]  7 Jan 2018\n\nhttps://www.fhotoroom.com/fhotos/\n\u2022 Remove #hashtag, @user, tabs and extra spaces\n\u2022 Filter out tweets that contain consecutive two question\nmarks (?). The reason is the coding of these tweets is\nusually not recognizable.\n\u2022 Filtering out none-English tweets using Googles\nLangdetect package.\nIII. S ENTIMENT SCORE\nTo translate each tweet into a sentiment score, the Stanford\ncoreNLP software was used. Stanford CoreNLP is designed\nto make linguistic analysis accessible to the general public. It\nprovides named Entity Recognition, co-reference and basic\ndependencies and many other text understanding applica-\ntions. An example that illustrate the basic functionality of\nStanford coreNLP is shown in Figure.1\nFig. 1. Demo of the functionalities provided by Stanford CoreNLP\ncoreNLP can compute a sentiment score for each sentence\nwith value ranging from 0 to 4 , where 0 stands for negative,\nand 4 stands for very positive. For tweets with multiple\nsentences, the average of the sentiment scores of all sentences\nis used as the sentiment score of the Tweets.\nThe number of Tweets varies everyday from a couple of\nhundreds to over ten thousands, depends on if the company\nhas a major event that attracts the public attention. The\nsentiment scores are normalized between 0 to 1, and features\nbased on the sentiment score is constructed and normalized.\nFigure 2 shows the relationship between Tesla stock return\nand stock sentiment score. According the distribution of the\nsentiment score, the sentiment on Tesla is slightly skewed\ntowards positive during the testing period. The price has\nbeen increased signi\ufb01cantly during the testing period, which",
            0.49746845799998596
          ]
        },
        {
          "answer": "The daily sentiment signal is output from tweets that have been preprocessed to remove irregular symbols, URLs, emojis, hashtags, mentions of users, tabs, extra spaces, tweets with consecutive question marks, and non-English tweets. After preprocessing, the sentiment of each tweet is evaluated using the Stanford CoreNLP software, which assigns a sentiment score ranging from 0 (negative) to 4 (very positive). For tweets containing multiple sentences, the average of the sentiment scores of all sentences is used as the sentiment score of the tweet. These scores are then normalized between 0 to 1 to construct and normalize features based on the sentiment score for further analysis.",
          "relevant_chunks": [
            "set of Tweets are searched strictly according to the stock\nticker. The second set of Tweets are searched according to\nthe company\u2019s products and news. The keywords for the\nsecond dataset are de\ufb01ned according to the top twenty related\nkeywords of the stock ticker according to Google Trend, a\nweb facility shows how often a certain word is searched\nrelative to Google\u2019s total search volume. For example, \u201dElon\nMusk\u201d is among the set of keywords that retrieve the second\ntweets set for Tesla.\nTweets contain irregular symbols, url and emoji etc which\nhas to be preprocessed so that the NLP algorithm can extract\nthe relevant information ef\ufb01ciently. Examples of preprocess-\ning are described as below:\n\u2022 Filter out tweets that contains http or .com Motiva-\ntion: They\u2019re usually ads e.g. #Fhotoroom #iPhone\narXiv:1801.02243v1  [cs.AI]  7 Jan 2018\n\nhttps://www.fhotoroom.com/fhotos/\n\u2022 Remove #hashtag, @user, tabs and extra spaces\n\u2022 Filter out tweets that contain consecutive two question\nmarks (?). The reason is the coding of these tweets is\nusually not recognizable.\n\u2022 Filtering out none-English tweets using Googles\nLangdetect package.\nIII. S ENTIMENT SCORE\nTo translate each tweet into a sentiment score, the Stanford\ncoreNLP software was used. Stanford CoreNLP is designed\nto make linguistic analysis accessible to the general public. It\nprovides named Entity Recognition, co-reference and basic\ndependencies and many other text understanding applica-\ntions. An example that illustrate the basic functionality of\nStanford coreNLP is shown in Figure.1\nFig. 1. Demo of the functionalities provided by Stanford CoreNLP\ncoreNLP can compute a sentiment score for each sentence\nwith value ranging from 0 to 4 , where 0 stands for negative,\nand 4 stands for very positive. For tweets with multiple\nsentences, the average of the sentiment scores of all sentences\nis used as the sentiment score of the Tweets.\nThe number of Tweets varies everyday from a couple of\nhundreds to over ten thousands, depends on if the company\nhas a major event that attracts the public attention. The\nsentiment scores are normalized between 0 to 1, and features\nbased on the sentiment score is constructed and normalized.\nFigure 2 shows the relationship between Tesla stock return\nand stock sentiment score. According the distribution of the\nsentiment score, the sentiment on Tesla is slightly skewed\ntowards positive during the testing period. The price has\nbeen increased signi\ufb01cantly during the testing period, which\n\nThe contribution of this paper is two-fold: First, the pre-\ndicting power of Twitter sentiment is evaluated. Our results\nshow sentiment is more suitable to construct alpha signals\nrather than total return signals and shows predicting power\nespecially when the Twitter volume is high. Second, we\nproposed a trading strategy based on reinforcement learning\n(Q-learning) that takes the sentiment features as part of its\nstates.\nThe paper is constructed as follows: In the second section,\nscraping Tweets from Twitter website and preprocessing the\ndata are described in details. In the third section, assigning\nsentiment scores to the text data is discussed. In the fourth\nsection, feature engineering and prediction based on the\nsentiment score is discussed. In the \ufb01fth section, how the\nreinforcement learning is applied to generate the optimal\ntrading strategy is described.\nII. T WITTER DATA SCRAPING AND PREPROCESSING\nThere are two options of getting the Tweets. First, Twitter\nprovides an API to download the Tweets. However, rate limit\nand history limit make it not an option for this paper. Second,\nscrapping Tweets directly from Twitter website. Using the\nsecond option, the daily Tweets for stocks of interest from\n2015 January to 2017 June were downloaded.\nThe predicting power of Twitter sentiment varies from\nstock to stock. For stocks that are mostly driven by the com-\npany fundamentals and hold by the institutional investors,\nthe predicting power of the Twitter sentiment is limited.\nFor stocks that are priced by the public expectation on the\ncompany\u2019s future growth, Twitter sentiment describes the\ncon\ufb01dence and expectation level of the investors.\nFor this reason, two companies from the same industry,\nTesla and Ford are investigated on how Twitter sentiment\ncould impact the stock price. Tesla is an electronic car\ncompany that shows consecutive negative operating cash \ufb02ow\nand net income but carries very high expectation from the\npublic. Ford, is a traditional auto maker whose stock prices\nhas been stabilized to represent the company fundamentals.\nTo investigate how different key words impact the predict-\ning power of the sentiment score, two Tweet sets, a ticker\nset and a product set, are prepared for each stock. The \ufb01rst\nset of Tweets are searched strictly according to the stock\nticker. The second set of Tweets are searched according to\nthe company\u2019s products and news. The keywords for the\nsecond dataset are de\ufb01ned according to the top twenty related\n\nTrading the Twitter Sentiment with Reinforcement Learning\nCatherine Xiao\ncatherine.xiao1@gmail.com\nWanfeng Chen\nwanfengc@gmail.com\nAbstract\u2014 This paper is to explore the possibility to use\nalternative data and arti\ufb01cial intelligence techniques to trade\nstocks. The ef\ufb01cacy of the daily Twitter sentiment on predicting\nthe stock return is examined using machine learning methods.\nReinforcement learning(Q-learning) is applied to generate the\noptimal trading policy based on the sentiment signal. The\npredicting power of the sentiment signal is more signi\ufb01cant\nif the stock price is driven by the expectation on the company\ngrowth and when the company has a major event that draws\nthe public attention. The optimal trading strategy based on\nreinforcement learning outperforms the trading strategy based\non the machine learning prediction.\nI. INTRODUCTION\nIn a world where traditional \ufb01nancial information is ubiq-\nuitous and the \ufb01nancial models are largely homogeneous,\n\ufb01nding hidden information that has not been priced in from\nalternative data is critical. The recent development in Natural\nLanguage Processing provides such opportunities to look\ninto text data in addition to numerical data. When the\nmarket sets the stock price, it is not uncommon that the\nexpectation of the company growth outweighs the company\nfundamentals. Twitter, a online news and social network\nwhere the users post and interact with messages to express\nviews about certain topics, contains valuable information on\nthe public mood and sentiment. A collection of research [1]\n[6] have shown that there is a positive correlation between\nthe \u201dpublic mood\u201d and the \u201dmarket mood\u201d. Other research[2]\nalso shows that signi\ufb01cant correlation exists between the\nTwitter sentiment and the abnormal return during the peaks\nof the Twitter volume during a major event.\nOnce a signal that has predicting power on the stock\nmarket return is constructed, a trading strategy to express the\nview of the signal is needed. Traditionally, the quantitative \ufb01-\nnance industry relies on backtest, a process where the trading\nstrategies are tuned during the simulations or optimizations.\nReinforcement learning provides a way to \ufb01nd the optimal\npolicy by maximizing the expected future utility. There are\nrecent attempts from the Arti\ufb01cial Intelligence community\nto apply reinforcement learning to asset allocation [5], algo-\nrithmic trading[3][7], and portfolio management[4].\nThe contribution of this paper is two-fold: First, the pre-\ndicting power of Twitter sentiment is evaluated. Our results\nshow sentiment is more suitable to construct alpha signals\nrather than total return signals and shows predicting power",
            0.3904839170000116
          ]
        },
        {
          "answer": "The baseline machine learning prediction approach applies linear logistic regression to a set of stock technical signals to predict the following day's stock return sign (+/-). This baseline is the same as the machine learning model except that only the technical indicator was used as the feature.",
          "relevant_chunks": [
            "and stock sentiment score. According the distribution of the\nsentiment score, the sentiment on Tesla is slightly skewed\ntowards positive during the testing period. The price has\nbeen increased signi\ufb01cantly during the testing period, which\nre\ufb02ected the positive sentiment. The predicting power of\nsentiment score is more signi\ufb01cant when the sentiment is\nmore extreme and less so when the sentiment is neutral.\nIV. T HE SENTIMENT MACHINE LEARNING MODEL\nA. Feature Engineering\nFeature engineering is the process to extract meaningful\ninformation from the raw data in order to improve the\nperformance of machine learning mode. Domain knowledge\nand intuition are often applied to keep the number of the\nFig. 2. Histogram\nfeatures reasonable relative to the training data size. Two\ncategories of features are de\ufb01nes: technical features and\nsentiment features. The technical features include previous\nday\u2019s return and volume, price momentum and volatility. The\nsentiment features include number of Tweets, daily average\nsentiment score, cross-section sentiment volatility, sentiment\nmomentum and reversal.\nB. Machine Learning Prediction Model\nThe logistic regression with L1 regularization and RBF-\nkernel SVM are applied to predict a binary outcome, i.e.\nwhether the stock return will be positive or negative in the\nnext day. Both technical and sentiment-based features carry\nimportant information about the stock price and therefore are\nprovided as the model inputs. Half of the dataset is used for\ntraining and the rest is used for testing.\nThe 3 fold cross validation is applied to learn the model\nhyper-parameters. Speci\ufb01cally, the hyper-parameters C of\nboth models and of RBF-kernel SVM are learned such that\nthe dev set accuracy is maximized. The hyper-parameter C\nin logistic regression determines the degree of regularization.\nSmaller C means more regularization, i.e. high bias and low\nvariance. RBF-kernel SVM has two hyper-parameters, C and\n. C controls the width of soft margin, smaller C allows\nplacing more samples on the wrong side of the margin. is a\nparameter in RBF kernel. A larger means a Gaussian with\nsmaller variance and thus less in\ufb02uence of support vectors.\nTypically, small C and large lead to high bias and low\nvariance.\nTo evaluate if the sentiment feature improves the pre-\ndiction accuracy, a baseline model is de\ufb01ned. The baseline\napplies linear logistic regression to a set of stock technical\nsignals to predict the following days stock return sign (+/).\n\nThe baseline is the same with machine learning except only\nthe technical indicator was used as the feature. The oracle\nmodel of this project is a trader who has insider information\nabout the stock and be able to bet and act correctly on every\nsingle day of the testing period. The oracle model is able\nto achieve 6 times of the initial capital at the end of testing\nperiod.\nFig. 5. The chart shows the trading strategy derived from Q-learning(in\nblue) outperform the backtest result using machine learning features(in red).\nBoth of Q-learning strategy and machine learning strategy outperform the\nbaseline(in green).\nThere are observations that worth a discussion. At the\nbeginning of the testing period, the Q-learning has not learnt\nhow to estimate the expected utility of a certain action yet.\nThe performance of the initial period is more unstable than\n4\n\nlater. Q-learning does better when the state is more common\nbecause it accumulates more experience about the situation\nbut might not take the best action when a outlier state is\npresented. The performance of the q-learning varies during\ndifferent batch due to the random nature of exploitation\nand exploration. In general Q-learning is able to deliver\nbetter performance than using the binary prediction from\nthe machine learning models. Both of the Q-learning and\nmachine learning model outperform the baseline model.\nVI. F UTURE WORK\nThere are many areas that can be improved given more\nresource and data. Below is a list of the improvement that\ncould make this idea more robust.\n\u2022 Use intraday data to test the sentiment signal and Q-\nlearning. By training with more data and trading more\npromptly, we expect both sentiment machine learning\nmodel and Q-learning to do better.\n\u2022 With more data, more features can be considered and\nincorporated into the model.\n\u2022 Apply different function approximators, for example,\nneural net, to better generalize the states and provide\nmore stable behavior\n\u2022 Add another class to the existing binary classi\ufb01er in-\nsigni\ufb01cant price change. The is motivated by preventing\nthe classi\ufb01er to \ufb01t to the noise inherent in stock market\nprice movement, and lumps small, statistically insignif-\nicant upward or downward movements indiscriminately\nwith large ones.\n\u2022 Add crude oil future price as a feature to predict Tesla\nstock alpha return sign\n\u2022 Extend the sentiment analysis to other stocks and Cryp-\ntocurrency, which is an asset class that driven even more\n\nWhen the MDP has many states and actions, it is very likely\nthat a (state, action) pair has not been explored yet so the\nestimate is not accurate. It is too slow to look up a gigantic\ntable and most likely there is not enough training data to\nlearn each of the state individually. Function approximation\nuses features to capture the characteristics of the states and\napplies stochastic gradient descent to update the weights on\neach feature. More speci\ufb01cally, below equation is applied\nto generalize the unseen state in this paper. De\ufb01ne features\n\u03c6(s,a) and weights w, then\n\u02c6Qopt(s,a; w) =w\u00b7\u03c6(s,a) (4)\nFor each (s,a,r,s \u2032), apply stochastic gradient descent to\nupdate the weights.\nw\u2190w\u2212\u03b7[ \u02c6Qopt(s,a; w) \u2212(r+ \u03b3\u02c6Vopt(s\u2032))]\u03c6(s,a) (5)\nwhere \u03b7 is the learning rate, r is the reward and \u03b3 is the\ndiscount factor.\nC. Exploration and Exploitation\nIt is necessary to balance the exploration and exploitation.\nOne might suggest naively to take action only according\nto the optimal policy estimated by maximizing \u02c6Qopt(s,a).\nHowever, this greedy strategy is equivalent to stay in the\ncomfortable zone all the time in life, without gaining new\nexperience and unable to give reasonable prediction when\nencounters unseen situations. Another extreme is to always\nexplore by choosing an action randomly. Without apply-\ning the hard lesson learned and obtaining the rewards,\nthe algorithm can lead to unsatis\ufb01able utility at the end.\nTherefore, in this paper the Epsilon-greedy strategy is applied\nfor exploration. For a certain probability, the algorithm\nacts randomly(exploration), for the rest the algorithm acts\noptimally(exploitation).\nD. Result and Discussion\nFigure 5 shows the cumulative return over 1 year period.\nThe strategy trades daily. The Q-learning states include\nportfolio position, sentiment features and technical indicators\nsuch as price momentum. The machine learning strategy\npredicts the binary movement (+ or -) of next trading day\nprice based on sentiment features and technical indicators.\nThe backtest rule based on the machine learning prediction\nis to long the stock if the prediction is +, short the stock if -.\nThe baseline is the same with machine learning except only\nthe technical indicator was used as the feature. The oracle\nmodel of this project is a trader who has insider information\nabout the stock and be able to bet and act correctly on every",
            0.5440512080000133
          ]
        }
      ],
      "reference_answers": [
        "Yes",
        "Tesla and Ford are investigated on how Twitter sentiment could impact the stock price",
        "linear logistic regression to a set of stock technical signals"
      ]
    },
    "1706.01875.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.0723633966031382,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.14148351648351648,
          "recall": 0.375,
          "f1": 0.17708333333333331
        },
        "rouge-2": {
          "precision": 0.09923076923076923,
          "recall": 0.34523809523809523,
          "f1": 0.12267080745341616
        },
        "rouge-L": {
          "precision": 0.14148351648351648,
          "recall": 0.375,
          "f1": 0.17708333333333331
        }
      },
      "per_question_metrics": [
        {
          "question": "What was the baseline?",
          "generated_answer": "The context provided does not specify a clear \"baseline\" for comparison in the discussion of the offensive speech classifier, the analysis of offensiveness in political discourse, or the datasets used. Therefore, I cannot provide an answer based on the given context.",
          "reference_answer": "stochastic gradient descent, naive bayes, decision tree",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            }
          },
          "bleu_score": 0.004453323527690996,
          "exact_match": 0.0
        },
        {
          "question": "What was their system's performance?",
          "generated_answer": "Their system's performance, as measured by the accuracy and F1-score during the classification of the holdout set, was 89.6% and 89.2%, respectively.",
          "reference_answer": "accuracy and F1-score of 89.6% and 89.2%, respectively accuracy and F1-score of 89.6% and 89.2%, respectively",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.4230769230769231,
              "recall": 0.5,
              "f1": 0.4583333333333333
            },
            "rouge-2": {
              "precision": 0.32,
              "recall": 0.38095238095238093,
              "f1": 0.34782608695652173
            },
            "rouge-L": {
              "precision": 0.4230769230769231,
              "recall": 0.5,
              "f1": 0.4583333333333333
            }
          },
          "bleu_score": 0.2521432424368841,
          "exact_match": 0.0
        },
        {
          "question": "What other political events are included in the database?",
          "generated_answer": "The context provided does not specify other political events included in the database.",
          "reference_answer": "US presidential primaries, Democratic and Republican National Conventions",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            }
          },
          "bleu_score": 0,
          "exact_match": 0.0
        },
        {
          "question": "What classifier did they use?",
          "generated_answer": "They used a 100-estimator entropy-based splitting random forest model as their classifier.",
          "reference_answer": "Random Forest",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.14285714285714285,
              "recall": 1.0,
              "f1": 0.25
            },
            "rouge-2": {
              "precision": 0.07692307692307693,
              "recall": 1.0,
              "f1": 0.14285714285714288
            },
            "rouge-L": {
              "precision": 0.14285714285714285,
              "recall": 1.0,
              "f1": 0.25
            }
          },
          "bleu_score": 0.03285702044797773,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "What was the baseline?",
        "What was their system's performance?",
        "What other political events are included in the database?",
        "What classifier did they use?"
      ],
      "generated_answers": [
        {
          "answer": "The context provided does not specify a clear \"baseline\" for comparison in the discussion of the offensive speech classifier, the analysis of offensiveness in political discourse, or the datasets used. Therefore, I cannot provide an answer based on the given context.",
          "relevant_chunks": [
            "implies that the CrowdFlower dataset is suf\ufb01cient\nfor building an offensive speech classi\ufb01er.\n\u2022Quality of manual labeling has a signi\ufb01cant impact\non the accuracy and precision of the classi\ufb01er. Us-\ning only tweets which had at least 70% of contrib-\nutors agreeing on a class resulted in between 5-7%\nhigher accuracy and up to 5% higher precision.\n\u2022Our classi\ufb01er achieves precision of over 95% and\nrecall of over 85% when considering only high con-\n\ufb01dence samples. This implies that the classi\ufb01er is\nmore likely to underestimate the presence of offen-\nsive speech \u2013 i.e., our results likely provide a lower-\nbound on the quantity of observed offensive speech.\n4 Measurements\nIn this section we quantify and characterize offensive-\nness in the political and general contexts using our offen-\nsive speech classi\ufb01er and the Reddit comments dataset\nwhich considers a random sample of comments made be-\ntween January 2015 and January 2017.\nOffensiveness over time. We \ufb01nd that on average\n8.4% of all political comments are offensive compared to\n7.8% of all apolitical comments. Figure 4 illustrates the\nfraction of offensive political and apolitical comments\nmade during each week in our study. We see that while\nthe fraction of apolitical offensive comments has stayed\nsteady, there has been an increase in the fraction of of-\nfensive political comments starting in July 2016. No-\ntably, this increase is observed after the conclusion of the\nUS presidential primaries and during the period of the\nDemocratic and Republican National Conventions and\ndoes not reduce even after the conclusion of the US presi-\ndential elections held on November 8. Participants in po-\nlitical subreddits were 2.6% more likely to observe offen-\n\nsive comments prior to July 2016 but 14.9% more likely\nto observe offensive comments from July 2016 onwards.\nFigure 4: Fraction of offensive comments identi\ufb01ed in\npolitical and all subreddits.\nReactions to offensive comments. We use the com-\nment score, roughly the difference between up-votes and\ndown-votes received, as a proxy for understanding how\nusers reacted to offensive comments. We \ufb01nd that com-\nments that were offensive: (1) on average, had a higher\nscore than non-offensive comments (average scores: 8.9\nvs. 6.7) and (2) were better received when they were\nposted in the general context than in the political context\n(average scores: 8.6 vs. 9.0). To understand how peo-\nples reactions to offensive comments evolved over time,\n\nout all of 2015 and 2016, we \ufb01nd that online political\ndiscussions became increasingly more offensive as the\ngeneral election campaign intensi\ufb01ed. By comparison,\ndiscussions on non-political subreddits did not become\nincreasingly offensive during this period. Additionally,\nwe \ufb01nd that the presence of offensive comments did not\nsubside even three months after the election.\n2 Datasets\nOur study makes use of multiple datasets in order to iden-\ntify and characterize trends in offensive speech.\nThe CrowdFlower hate speech dataset. The Crowd-\nFlower hate speech dataset [1] contains 14.5K tweets,\neach receiving labels from at least three contributors.\nContributors were allowed to classify each tweet into\none of three classes: Not Offensive (NO), Offensive but\nnot hateful (O), and Offensive and hateful (OH). Of the\n14.5K tweets, only 37.6% had a decisive class \u2013 i.e., the\nsame class was assigned by all contributors. For inde-\ncisive cases, the majority class was selected and a class\ncon\ufb01dence score (fraction of contributors that selected\nthe majority class) was made available. Using this ap-\nproach, 50.4%, 33.1%, and 16.5% of the tweets were cat-\negorized as NO, O, and OH, respectively. Since our goal\nis to identify any offensive speech (not just hate speech),\nwe consolidate assigned classes into Offensive and Not\nOffensive by relabeling OH tweets as Offensive. We use\nthis modi\ufb01ed dataset to train, validate, and test our offen-\nsive speech classi\ufb01er. To the best of our knowledge, this\nis the only dataset that provides offensive and not offen-\nsive annotations to a large dataset.\nOffensive word lists. We also use two offensive\nword lists as auxiliary input to our classi\ufb01er: (1) The\nHatebase hate speech vocabulary [3] consisting of 1122\nhateful words and (2) 422 offensive words banned from\nGoogle\u2019s What Do You Love project [7].\nReddit comments dataset. Finally, after building our\noffensive speech classi\ufb01er using the above datasets, we\nuse it to classify comments made on Reddit. While the\ncomplete Reddit dataset contains 2B comments made be-\ntween the period of January 2015 and January 2017, we\nonly analyze only 168M. We select comments to be ana-\nlyzed using the following process: (1) we exclude com-\nments shorter than 10 characters in length, (2) we ex-\nclude comments made by [deleted] authors, and (3)\n\nMeasuring Offensive Speech in Online Political Discourse\nRishab Nithyanand1, Brian Schaffner2, Phillipa Gill1\n1{rishab, phillipa}@cs.umass.edu, 2schaffne@polsci.umass.edu\nUniversity of Massachusetts, Amherst\nAbstract\nThe Internet and online forums such as Reddit have be-\ncome an increasingly popular medium for citizens to\nengage in political conversations. However, the on-\nline disinhibition effect resulting from the ability to use\npseudonymous identities may manifest in the form of\noffensive speech, consequently making political discus-\nsions more aggressive and polarizing than they already\nare. Such environments may result in harassment and\nself-censorship from its targets. In this paper, we present\npreliminary results from a large-scale temporal measure-\nment aimed at quantifying offensiveness in online politi-\ncal discussions.\nTo enable our measurements, we develop and evalu-\nate an offensive speech classi\ufb01er. We then use this clas-\nsi\ufb01er to quantify and compare offensiveness in the po-\nlitical and general contexts. We perform our study us-\ning a database of over 168M Reddit comments made by\nover 7M pseudonyms between January 2015 and January\n2017 \u2013 a period covering several divisive political events\nincluding the 2016 US presidential elections.\n1 Introduction\nThe apparent rise in political incivility has attracted sub-\nstantial attention from scholars in recent years. These\nstudies have largely focused on the extent to which politi-\ncians and elected of\ufb01cials are increasingly employing\nrhetoric that appears to violate norms of civility [4, 12].\nFor the purposes of our work, we use the incidence of of-\nfensive rhetoric as a stand in for incivility. The 2016 US\npresidential election was an especially noteworthy case\nin this regard, particularly in terms of Donald Trump\u2019s\ncampaign which frequently violated norms of civility\nboth in how he spoke about broad groups in the pub-\nlic (such as Muslims, Mexicans, and African Americans)\nand the attacks he leveled at his opponents [2]. The con-\nsequences of incivility are thought to be crucial to the\nfunctioning of democracy since \u201cpublic civility and in-\nterpersonal politeness sustain social harmony and allow\npeople who disagree with one another to maintain ongo-\ning relationships\u201d [17].\nWhile political incivility appears to be on the rise\namong elites, it is less clear whether this is true among\nthe mass public as well. Is political discourse particularly\nlacking in civility compared to discourse more generally?\nDoes the incivility of mass political discourse respond to\nthe dynamics of political campaigns? Addressing these\nquestions has been dif\ufb01cult for political scientists be-",
            0.40313475000002086
          ]
        },
        {
          "answer": "Their system's performance, as measured by the accuracy and F1-score during the classification of the holdout set, was 89.6% and 89.2%, respectively.",
          "relevant_chunks": [
            "maximum cosine similarity score is used to represent the\ncomment. Equation 1 shows the transformation function\non a string S = {s1,..., sn}where si is the vector rep-\nresenting the ith lemmatized non-stop-word, cos is the\ncosine-similarity function, and H is the hate vector.\nT(S) =max\n1\u2264i\u2264n\n[cos(si,H)] (1)\nIn words, the numerical value assigned to a text is the\ncosine similarity between the hate vector and the vec-\ntor representing the word (in the text) closest to the hate\nvector. This approach allows us to transform a string of\ntext into a single numerical value that captures its se-\nmantic similarity to the most offensive comment. We\nuse these scalars as input to a random forest classi\ufb01er to\nperform classi\ufb01cation into Offensive and Not Offensive\nclasses. Figure 2 shows the proximity of Offensive and\nNon Offensive comments to our constructed hate vector\nafter using t-distributed Stochastic Neighbor Embedding\n(t-SNE) [9] to reduce our 100-dimension vector space\ninto 2 dimensions.\n3.2 Classi\ufb01er evaluation\nWe now present results to (1) validate our choice\nof classi\ufb01er and (2) demonstrate the impact of train-\ning/validation sample count on our classi\ufb01ers perfor-\nmance.\nClassi\ufb01er Accuracy (%) F1-Score (%)\nStochastic Gradient Descent 80.7 80.0\nNaive Bayes 81.5 81.2\nDecision Tree 91.8 91.4\nRandom Forest 92.0 91.9\nTable 1: Average classi\ufb01er performance during 10-fold\ncross-validation on the training/validation set. Results\nshown are for the best performing parameters obtained\nusing a grid search.\nClassi\ufb01er selection methodology. To identify the\nmost suitable classi\ufb01er for classifying the scalars as-\nsociated with each text, we perform evaluations us-\ning the stochastic gradient descent, naive bayes, deci-\nsion tree, and random forest classi\ufb01ers. For each clas-\nsi\ufb01er, we split the CrowdFlower hate speech dataset\ninto a training/validation set (75%), and a holdout set\n(25%). We perform 10-fold cross-validation on the train-\ning/validation set to identify the best classi\ufb01er model and\n\n(a) Classi\ufb01er accuracy.\n(b) Classi\ufb01er precision.\n(c) Classi\ufb01er recall.\nFigure 3: Classi\ufb01er performance on holdout sets while\nvarying holdout set sizes and minimum con\ufb01dence\nthresholds.\nparameters (using a grid search). Based on the results of\n\n(b) Classi\ufb01er precision.\n(c) Classi\ufb01er recall.\nFigure 3: Classi\ufb01er performance on holdout sets while\nvarying holdout set sizes and minimum con\ufb01dence\nthresholds.\nparameters (using a grid search). Based on the results of\nthis evaluation, we select a 100-estimator entropy-based\nsplitting random forest model as our classi\ufb01er. Table 1\nshows the mean accuracy and F1-score for each evalu-\nated classi\ufb01er during the 10-fold cross-validation.\nReal-world classi\ufb01er performance. To evaluate real-\nworld performance of our selected classi\ufb01er ( i.e., per-\nformance in the absence of model and parameter bias),\nwe perform classi\ufb01cation of the holdout set. On this set,\nour classi\ufb01er had an accuracy and F1-score of 89.6% and\n89.2%, respectively. These results show that in addition\nto superior accuracy during training and validation, our\nchosen classi\ufb01er is also robust against over-\ufb01tting.\nImpact of dataset quality and size. To understand\nhow the performance of our chosen classi\ufb01er model and\nparameters are impacted by: (1) the quality and consis-\ntency of manually assigned classes in the CrowdFlower\ndataset and (2) the size of the dataset, we re-evaluate the\nclassi\ufb01er while only considering tweets having a mini-\nmum con\ufb01dence score and varying the size of the holdout\nset. Speci\ufb01cally, our experiments considered con\ufb01dence\nthresholds of 0 (all tweets considered), .35 (only tweets\nwhere at least 35% of contributors agreed on a class were\nconsidered), and .70 (only tweets where at least 70% of\nthe contributors agreed on a class were considered) and\nvaried the holdout set sizes between 5% and 95% of all\ntweets meeting the con\ufb01dence threshold set for the ex-\nperiment.\nThe results illustrated in Figure 3 show the perfor-\nmance of the classi\ufb01er while evaluating the correspond-\ning holdout set. We make several conclusions from these\nresults:\n\u2022Beyond a (fairly low) threshold, the size of the train-\ning and validation set has little impact on classi-\n\ufb01er performance. We see that the accuracy, pre-\ncision, and recall have, at best, marginal improve-\nments with holdout set sizes smaller than 60%. This\nimplies that the CrowdFlower dataset is suf\ufb01cient\nfor building an offensive speech classi\ufb01er.\n\u2022Quality of manual labeling has a signi\ufb01cant impact\non the accuracy and precision of the classi\ufb01er. Us-\ning only tweets which had at least 70% of contrib-\n\nimplies that the CrowdFlower dataset is suf\ufb01cient\nfor building an offensive speech classi\ufb01er.\n\u2022Quality of manual labeling has a signi\ufb01cant impact\non the accuracy and precision of the classi\ufb01er. Us-\ning only tweets which had at least 70% of contrib-\nutors agreeing on a class resulted in between 5-7%\nhigher accuracy and up to 5% higher precision.\n\u2022Our classi\ufb01er achieves precision of over 95% and\nrecall of over 85% when considering only high con-\n\ufb01dence samples. This implies that the classi\ufb01er is\nmore likely to underestimate the presence of offen-\nsive speech \u2013 i.e., our results likely provide a lower-\nbound on the quantity of observed offensive speech.\n4 Measurements\nIn this section we quantify and characterize offensive-\nness in the political and general contexts using our offen-\nsive speech classi\ufb01er and the Reddit comments dataset\nwhich considers a random sample of comments made be-\ntween January 2015 and January 2017.\nOffensiveness over time. We \ufb01nd that on average\n8.4% of all political comments are offensive compared to\n7.8% of all apolitical comments. Figure 4 illustrates the\nfraction of offensive political and apolitical comments\nmade during each week in our study. We see that while\nthe fraction of apolitical offensive comments has stayed\nsteady, there has been an increase in the fraction of of-\nfensive political comments starting in July 2016. No-\ntably, this increase is observed after the conclusion of the\nUS presidential primaries and during the period of the\nDemocratic and Republican National Conventions and\ndoes not reduce even after the conclusion of the US presi-\ndential elections held on November 8. Participants in po-\nlitical subreddits were 2.6% more likely to observe offen-\n\nsive comments prior to July 2016 but 14.9% more likely\nto observe offensive comments from July 2016 onwards.\nFigure 4: Fraction of offensive comments identi\ufb01ed in\npolitical and all subreddits.\nReactions to offensive comments. We use the com-\nment score, roughly the difference between up-votes and\ndown-votes received, as a proxy for understanding how\nusers reacted to offensive comments. We \ufb01nd that com-\nments that were offensive: (1) on average, had a higher\nscore than non-offensive comments (average scores: 8.9\nvs. 6.7) and (2) were better received when they were\nposted in the general context than in the political context\n(average scores: 8.6 vs. 9.0). To understand how peo-\nples reactions to offensive comments evolved over time,",
            2.094084333000012
          ]
        },
        {
          "answer": "The context provided does not specify other political events included in the database.",
          "relevant_chunks": [
            "view electoral victories by opponents as lacking legiti-\nmacy [12]. Thus, from a normative standpoint, the fact\nthat the 2016 campaign sparked a marked increase in the\noffensiveness of political comments posted to Reddit is\nof concern in its own right; that the incidence of offensive\npolitical comments has remained high even three months\nafter the election is all the more troubling.\n\nIn future work, we will extend our analysis of Reddit\nback to 2007 with the aim of formulating a more com-\nplete understanding of the dynamics of political incivil-\nity. For example, we seek to understand whether the\nhigh incidence of offensive speech we \ufb01nd in 2016 is\nunique to this particular campaign or if previous presi-\ndential campaigns witnessed similar spikes in incivility.\nWe will also examine whether there is a more general\nlong-term trend toward offensive online political speech,\nwhich would be consistent with what scholars have found\nwhen studying political elites [6, 16].\nReferences\n[1] CrowdFlower Blog. Hate speech identi\ufb01cation.\nURL: https://www.crowdflower.com/data/h\nate-speech-identification/ (Accessed May\n24, 2017).\n[2] Justin H Gross and Kaylee T Johnson. Twitter\ntaunts and tirades: Negative campaigning in the age\nof trump. PS: Political Science &amp; Politics,\n49(4):748\u2013754, 2016.\n[3] Hatebase. Meet the Hatebase API. URL: https:\n//www.hatebase.org/connect_api (Accessed\nMay 24, 2017).\n[4] Susan Herbst. Rude democracy: Civility and in-\ncivility in American politics. Temple University\nPress, 2010.\n[5] Matthew Honnibal and Mark Johnson. An Im-\nproved Non-monotonic Transition System for De-\npendency Parsing. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1373\u20131378, Lisbon, Portugal,\n2015. Association for Computational Linguistics.\n[6] Kathleen Hall Jamieson and Erika Falk. Continu-\nity and change in civility in the house. In Polar-\nized politics: Congress and the president in a parti-\nsan era, pages 96\u2013108. Washington, DC: CQ Press,\n2000.\n[7] Jamiew. All the dirty words from Google\u2019s \u201cwhat\ndo you love\u201d project. URL: https://gist.g\nithub.com/jamiew/1112488 (Accessed May 24,\n2017).\n[8] K Hazel Kwon and Anatoliy Gruzd. Is aggression\ncontagious online? a case of swearing on donald\ntrump\u2019s campaign videos on youtube. In Proceed-\n\nithub.com/jamiew/1112488 (Accessed May 24,\n2017).\n[8] K Hazel Kwon and Anatoliy Gruzd. Is aggression\ncontagious online? a case of swearing on donald\ntrump\u2019s campaign videos on youtube. In Proceed-\nings of the 50th Hawaii International Conference\non System Sciences, 2017.\n[9] Laurens van der Maaten and Geoffrey Hinton. Vi-\nsualizing Data Using t-SNE. Journal of Machine\nLearning Research, 9(Nov):2579\u20132605, 2008.\n[10] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S\nCorrado, and Jeff Dean. Distributed Representa-\ntions of Words and Phrases and their Composition-\nality. In Advances in neural information processing\nsystems, pages 3111\u20133119, 2013.\n[11] Diana C Mutz. Hearing the other side: Deliber-\native versus participatory democracy. Cambridge\nUniversity Press, 2006.\n[12] Diana C Mutz. In-your-face politics: The con-\nsequences of uncivil media. Princeton University\nPress, 2015.\n[13] Jeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. GloVe: Global Vectors for Word\nRepresentation. In Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1532\u20131543,\n2014.\n[14] Reddit CEO Speaks Out On Violentacrez In\nLeaked Memo: \u2018We Stand for Free Speech\u2019.\nHate speech identi\ufb01cation. URL: https://we\nb.archive.org/web/20170710074932/http:\n//gawker.com/5952349/reddit-ceo-speak\ns-out-on-violentacrez-in-leaked-memo-w\ne-stand-for-free-speech (Accessed May 24,\n2017).\n[15] Radim \u02c7Reh\u02dau\u02c7rek and Petr Sojka. Software Frame-\nwork for Topic Modelling with Large Corpora. In\nProceedings of the LREC 2010 Workshop on New\nChallenges for NLP Frameworks, pages 45\u201350,\nValletta, Malta, 2010. ELRA. http://is.muni.\ncz/publication/884893/en.\n[16] Daniel M Shea and Alex Sproveri. The rise and fall\nof nasty politics in america. PS: Political Science\n&amp; Politics, 45(03):416\u2013421, 2012.\n[17] J Cherie Strachan and Michael R Wolf. Politi-\ncal civility. PS: Political Science &amp; Politics,\n45(03):401\u2013404, 2012.\n[18] Michael R Wolf, J Cherie Strachan, and Daniel M\nShea. Incivility and standing \ufb01rm: A second layer\nof partisan division. PS: Political Science &amp;\nPolitics, 45(03):428\u2013434, 2012.\n\nout all of 2015 and 2016, we \ufb01nd that online political\ndiscussions became increasingly more offensive as the\ngeneral election campaign intensi\ufb01ed. By comparison,\ndiscussions on non-political subreddits did not become\nincreasingly offensive during this period. Additionally,\nwe \ufb01nd that the presence of offensive comments did not\nsubside even three months after the election.\n2 Datasets\nOur study makes use of multiple datasets in order to iden-\ntify and characterize trends in offensive speech.\nThe CrowdFlower hate speech dataset. The Crowd-\nFlower hate speech dataset [1] contains 14.5K tweets,\neach receiving labels from at least three contributors.\nContributors were allowed to classify each tweet into\none of three classes: Not Offensive (NO), Offensive but\nnot hateful (O), and Offensive and hateful (OH). Of the\n14.5K tweets, only 37.6% had a decisive class \u2013 i.e., the\nsame class was assigned by all contributors. For inde-\ncisive cases, the majority class was selected and a class\ncon\ufb01dence score (fraction of contributors that selected\nthe majority class) was made available. Using this ap-\nproach, 50.4%, 33.1%, and 16.5% of the tweets were cat-\negorized as NO, O, and OH, respectively. Since our goal\nis to identify any offensive speech (not just hate speech),\nwe consolidate assigned classes into Offensive and Not\nOffensive by relabeling OH tweets as Offensive. We use\nthis modi\ufb01ed dataset to train, validate, and test our offen-\nsive speech classi\ufb01er. To the best of our knowledge, this\nis the only dataset that provides offensive and not offen-\nsive annotations to a large dataset.\nOffensive word lists. We also use two offensive\nword lists as auxiliary input to our classi\ufb01er: (1) The\nHatebase hate speech vocabulary [3] consisting of 1122\nhateful words and (2) 422 offensive words banned from\nGoogle\u2019s What Do You Love project [7].\nReddit comments dataset. Finally, after building our\noffensive speech classi\ufb01er using the above datasets, we\nuse it to classify comments made on Reddit. While the\ncomplete Reddit dataset contains 2B comments made be-\ntween the period of January 2015 and January 2017, we\nonly analyze only 168M. We select comments to be ana-\nlyzed using the following process: (1) we exclude com-\nments shorter than 10 characters in length, (2) we ex-\nclude comments made by [deleted] authors, and (3)",
            1.086469333000025
          ]
        },
        {
          "answer": "They used a 100-estimator entropy-based splitting random forest model as their classifier.",
          "relevant_chunks": [
            "maximum cosine similarity score is used to represent the\ncomment. Equation 1 shows the transformation function\non a string S = {s1,..., sn}where si is the vector rep-\nresenting the ith lemmatized non-stop-word, cos is the\ncosine-similarity function, and H is the hate vector.\nT(S) =max\n1\u2264i\u2264n\n[cos(si,H)] (1)\nIn words, the numerical value assigned to a text is the\ncosine similarity between the hate vector and the vec-\ntor representing the word (in the text) closest to the hate\nvector. This approach allows us to transform a string of\ntext into a single numerical value that captures its se-\nmantic similarity to the most offensive comment. We\nuse these scalars as input to a random forest classi\ufb01er to\nperform classi\ufb01cation into Offensive and Not Offensive\nclasses. Figure 2 shows the proximity of Offensive and\nNon Offensive comments to our constructed hate vector\nafter using t-distributed Stochastic Neighbor Embedding\n(t-SNE) [9] to reduce our 100-dimension vector space\ninto 2 dimensions.\n3.2 Classi\ufb01er evaluation\nWe now present results to (1) validate our choice\nof classi\ufb01er and (2) demonstrate the impact of train-\ning/validation sample count on our classi\ufb01ers perfor-\nmance.\nClassi\ufb01er Accuracy (%) F1-Score (%)\nStochastic Gradient Descent 80.7 80.0\nNaive Bayes 81.5 81.2\nDecision Tree 91.8 91.4\nRandom Forest 92.0 91.9\nTable 1: Average classi\ufb01er performance during 10-fold\ncross-validation on the training/validation set. Results\nshown are for the best performing parameters obtained\nusing a grid search.\nClassi\ufb01er selection methodology. To identify the\nmost suitable classi\ufb01er for classifying the scalars as-\nsociated with each text, we perform evaluations us-\ning the stochastic gradient descent, naive bayes, deci-\nsion tree, and random forest classi\ufb01ers. For each clas-\nsi\ufb01er, we split the CrowdFlower hate speech dataset\ninto a training/validation set (75%), and a holdout set\n(25%). We perform 10-fold cross-validation on the train-\ning/validation set to identify the best classi\ufb01er model and\n\n(a) Classi\ufb01er accuracy.\n(b) Classi\ufb01er precision.\n(c) Classi\ufb01er recall.\nFigure 3: Classi\ufb01er performance on holdout sets while\nvarying holdout set sizes and minimum con\ufb01dence\nthresholds.\nparameters (using a grid search). Based on the results of\n\n(b) Classi\ufb01er precision.\n(c) Classi\ufb01er recall.\nFigure 3: Classi\ufb01er performance on holdout sets while\nvarying holdout set sizes and minimum con\ufb01dence\nthresholds.\nparameters (using a grid search). Based on the results of\nthis evaluation, we select a 100-estimator entropy-based\nsplitting random forest model as our classi\ufb01er. Table 1\nshows the mean accuracy and F1-score for each evalu-\nated classi\ufb01er during the 10-fold cross-validation.\nReal-world classi\ufb01er performance. To evaluate real-\nworld performance of our selected classi\ufb01er ( i.e., per-\nformance in the absence of model and parameter bias),\nwe perform classi\ufb01cation of the holdout set. On this set,\nour classi\ufb01er had an accuracy and F1-score of 89.6% and\n89.2%, respectively. These results show that in addition\nto superior accuracy during training and validation, our\nchosen classi\ufb01er is also robust against over-\ufb01tting.\nImpact of dataset quality and size. To understand\nhow the performance of our chosen classi\ufb01er model and\nparameters are impacted by: (1) the quality and consis-\ntency of manually assigned classes in the CrowdFlower\ndataset and (2) the size of the dataset, we re-evaluate the\nclassi\ufb01er while only considering tweets having a mini-\nmum con\ufb01dence score and varying the size of the holdout\nset. Speci\ufb01cally, our experiments considered con\ufb01dence\nthresholds of 0 (all tweets considered), .35 (only tweets\nwhere at least 35% of contributors agreed on a class were\nconsidered), and .70 (only tweets where at least 70% of\nthe contributors agreed on a class were considered) and\nvaried the holdout set sizes between 5% and 95% of all\ntweets meeting the con\ufb01dence threshold set for the ex-\nperiment.\nThe results illustrated in Figure 3 show the perfor-\nmance of the classi\ufb01er while evaluating the correspond-\ning holdout set. We make several conclusions from these\nresults:\n\u2022Beyond a (fairly low) threshold, the size of the train-\ning and validation set has little impact on classi-\n\ufb01er performance. We see that the accuracy, pre-\ncision, and recall have, at best, marginal improve-\nments with holdout set sizes smaller than 60%. This\nimplies that the CrowdFlower dataset is suf\ufb01cient\nfor building an offensive speech classi\ufb01er.\n\u2022Quality of manual labeling has a signi\ufb01cant impact\non the accuracy and precision of the classi\ufb01er. Us-\ning only tweets which had at least 70% of contrib-\n\nlyzed using the following process: (1) we exclude com-\nments shorter than 10 characters in length, (2) we ex-\nclude comments made by [deleted] authors, and (3)\nFigure 1: Number of analyzed political and apoliti-\ncal comments belonging to each week between January\n2015 and January 2017.\nwe randomly sample and include 10% of all remaining\ncomments. We categorize comments made in any of 21\npopular political subreddits as political and the remain-\nder as apolitical. Our \ufb01nal dataset contains 129M apo-\nlitical and 39M political comments. Figure 1 shows the\nnumber of comments in our dataset that were made dur-\ning each week included in our study. We see an increas-\ning number of political comments per week starting in\nFebruary 2016 \u2013 the start of the 2016 US presidential\nprimaries.\n3 Offensive Speech Classi\ufb01cation\nIn order to identify offensive speech, we propose a fully\nautomated technique that classi\ufb01es comments into two\nclasses: Offensive and Not Offensive.\n3.1 Classi\ufb01cation approach\nAt a high-level, our approach works as follows:\n\u2022Build a word embedding. We construct a 100-\ndimensional word embedding using all comments\nfrom our complete Reddit dataset (2B comments).\n\u2022Construct a hate vector. We construct a list of of-\nfensive and hateful words identi\ufb01ed from external\ndata and map them into a single vector within the\nhigh-dimensional word embedding.\n\u2022Text transformation and classi\ufb01cation. Finally,\nwe transform text to be classi\ufb01ed into scalars rep-\nresenting their distance from the constructed hate\nvector and use these as input to a Random Forest\nclassi\ufb01er.\nBuilding a word embedding. At a high-level, a word\nembedding maps words into a high-dimensional continu-\nous vector space in such a way that semantic similarities\nbetween words are maintained. This mapping is achieved\n\nFigure 2: Proximity of offensive and non-offensive com-\nments to the hate vector. Dimension reduction is per-\nformed using t-SNE.\nby exploiting the distributional properties of words and\ntheir occurrences in the input corpus.\nRather than using an off-the-shelf word embedding\n(e.g., the GloVe embeddings [13] trained using public do-\nmain data sources such as Wikipedia and news articles),\nwe construct a 100-dimensional embedding using the\ncomplete Reddit dataset (2B comments) as the input cor-\npus. The constructed embedding consists of over 400M",
            0.8766179170000044
          ]
        }
      ],
      "reference_answers": [
        "stochastic gradient descent, naive bayes, decision tree",
        "accuracy and F1-score of 89.6% and 89.2%, respectively accuracy and F1-score of 89.6% and 89.2%, respectively",
        "US presidential primaries, Democratic and Republican National Conventions",
        "Random Forest"
      ]
    },
    "1801.02073.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.08939103733976832,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.5069444444444444,
          "recall": 0.3645833333333333,
          "f1": 0.3972222222222222
        },
        "rouge-2": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0
        },
        "rouge-L": {
          "precision": 0.4861111111111111,
          "recall": 0.3541666666666667,
          "f1": 0.3833333333333333
        }
      },
      "per_question_metrics": [
        {
          "question": "Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?",
          "generated_answer": "Yes.",
          "reference_answer": "Yes No",
          "rouge_scores": {
            "rouge-1": {
              "precision": 1.0,
              "recall": 0.5,
              "f1": 0.6666666666666666
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 1.0,
              "recall": 0.5,
              "f1": 0.6666666666666666
            }
          },
          "bleu_score": 0.1495348781221221,
          "exact_match": 0.0
        },
        {
          "question": "How many question types do they find in the datasets analyzed?",
          "generated_answer": "Seven question types",
          "reference_answer": "seven  7",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.3333333333333333,
              "recall": 0.5,
              "f1": 0.4
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.3333333333333333,
              "recall": 0.5,
              "f1": 0.4
            }
          },
          "bleu_score": 0.11362193664674995,
          "exact_match": 0.0
        },
        {
          "question": "How do they analyze contextual similaries across datasets?",
          "generated_answer": "The provided context does not contain specific details on how they analyze contextual similarities across datasets.",
          "reference_answer": "They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap.",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.1875,
              "recall": 0.09375,
              "f1": 0.125
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.125,
              "recall": 0.0625,
              "f1": 0.08333333333333333
            }
          },
          "bleu_score": 0.005016297250432885,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?",
        "How many question types do they find in the datasets analyzed?",
        "How do they analyze contextual similaries across datasets?"
      ],
      "generated_answers": [
        {
          "answer": "Yes.",
          "relevant_chunks": [
            "Abbreviation\n0 10 20 30 40\nWikiQA SelQA SQuAD InfoboxQA\nFigure 2: Distributions of answer categories in %.\n2Our CNN model shows 95.20% accuracy on their test set.\n3 Answer Retrieval\nThis section describes another selection-based QA\ntask, called answer retrieval, that \ufb01nds the answer\ncontext from a larger dataset, the entire Wikipedia.\nSQUAD provides no mapping of the answer con-\ntexts to Wikipedia, whereas WIKI QA and SELQA\nprovide mappings; however, their data do not come\nfrom the same version of Wikipedia. We propose\nan automatic way of mapping the answer contexts\nfrom all corpora to the same version of Wikipeda3\nso they can be coherently used for answer retrieval.\nEach paragraph in Wikipedia is \ufb01rst indexed by\nLucene using {1,2,3}-grams, where the paragraphs\nare separated by WikiExtractor4 and segmented by\nNLP4J5 (28.7M+ paragraphs are indexed). Each\nanswer sentence from the corpora in Table 2 is then\nqueried to Lucene, and the top-5 ranked paragraphs\nare retrieved. The cosine similarity between each\nsentence in these paragraphs and the answer sen-\ntence is measured forn-grams, say n1,2,3. A weight\nis assigned to each n-gram score, say \u03bb1,2,3, and\nthe weighted sum is measured: t = \u22113\ni=1 \u03bbi \u00b7ni.\nThe \ufb01xed weights of \u03bb1,2,3 = (0.25,0.35,0.4) are\nused for our experiments, which can be improved.\nIf there exists a sentence whose t\u2265\u03b8, the para-\ngraph consisting of that sentence is considered the\nsilver-standard answer passage. Table 2 shows how\nrobust these silver-standard passages are based on\nhuman judgement (\u03c1) and how many passages are\ncollected (\u03b3) for \u03b8 = [0.3,0.5], where the human\njudgement is performed on 50 random samples for\neach case. For answer retrieval, a dataset is created\nby \u03b8 = 0.4, which gives \u03c1 \u226594% accuracy and\n\u03b3p > 90% coverage, respectively.6 Finally, each\nquestion is queried to Lucene and the top-kpara-\ngraphs are retrieved from the entire Wikipedia. If\nthe answer sentence exists within those retrieved\nparagraphs according to the silver-standard, it is\nconsidered correct.\n3enwiki-20160820-pages-articles.xml.bz2\n4github.com/attardi/wikiextractor\n5github.com/emorynlp/nlp4j\n6SQUAD mapping was easier than the others because it was\nbased on a more recent version of Wikipedia.\n\nTrained on\nEvaluated on\nWIKI QA SELQA SQUAD INFOBOX QA\nMAP MRR F1 MAP MRR F1 MAP MRR F1 MAP MRR F1\n\nwidely used for benchmarking the answer selection\ntask. Feng et al. (2015) presented INSURANCE QA\ncomprising 16K+ questions on insurance contexts.\nYang et al. (2015) introduced WIKI QA for answer\nselection and triggering. Jurczyk et al. (2016) cre-\nated SELQA for large real-scale answer triggering.\nRajpurkar et al. (2016) presented SQUAD for an-\nswer extraction and selection as well as for reading\ncomprehension. Finally, Morales et al. (2016) pro-\nvided INFOBOX QA for answer selection.\nThese corpora make it possible to evaluate the ro-\nbustness of statistical question answering learning.\nAlthough all of these corpora target on selection-\nbased QA, they are designed for different purposes\nsuch that it is important to understand the nature of\nthese corpora so a better use of them can be made.\nIn this paper, we make both intrinsic and extrinsic\nanalyses of four latest corpora based on Wikipedia,\nWIKI QA, SELQA, SQUAD, and INFOBOX QA.\nWe \ufb01rst give a thorough intrinsic analysis regarding\ncontextual similarities, question types, and answer\ncategories (Section 2). We then map questions in all\ncorpora to the current version of English Wikipedia\nand benchmark another selection-based QA task,\nanswer retrieval (Section 3). Finally, we present\nan extrinsic analysis through a set of experiments\ncross-testing these corpora using a convolutional\nneural network architecture (Section 4).1\n2 Intrinsic Analysis\nFour publicly available corpora are selected for our\nanalysis. These corpora are based on Wikipedia, so\nmore comparable than the others, and have already\nbeen used for the evaluation of several QA systems.\n1All our resources are publicly available\n: anonymous_url\narXiv:1801.02073v2  [cs.CL]  5 Feb 2018\n\nWIKIQA SELQA SQUAD INFOBOX QA\nSource Bing search queries Crowdsourced Crowdsourced Crowdsourced\nYear 2015 2016 2016 2016\n(AE, AS, AT) (O, O, O) (X, O, O) (O, O, X) (X, O, X)\n(q,c,c/q) (1,242, 12,153, 9.79) (7,904, 95,250, 12.05) (98,202, 496,167, 5.05) (15,271, 271,038, 17.75)\n(w,t) (386,440, 30,191) (3,469,015, 44,099) (19,445,863, 115,092) (5,034,625, 8,323)\n\nAnalysis of Wikipedia-based Corpora for Question Answering\nTomasz Jurczyk1, Amit Deshmane2, Jinho D. Choi1\n1Mathematics and Computer Science, Emory University\n2Infosys Ltd.\n{tomasz.jurczyk,jinho.choi}@emory.edu\namit deshmane@infosys.com\nAbstract\nThis paper gives comprehensive analyses\nof corpora based on Wikipedia for several\ntasks in question answering. Four recent\ncorpora are collected, WIKI QA, SELQA,\nSQUAD, and INFOBOX QA, and \ufb01rst an-\nalyzed intrinsically by contextual similari-\nties, question types, and answer categories.\nThese corpora are then analyzed extrinsi-\ncally by three question answering tasks, an-\nswer retrieval, selection, and triggering. An\nindexing-based method for the creation of a\nsilver-standard dataset for answer retrieval\nusing the entire Wikipedia is also presented.\nOur analysis shows the uniqueness of these\ncorpora and suggests a better use of them\nfor statistical question answering learning.\n1 Introduction\nQuestion answering (QA) has been a blooming re-\nsearch \ufb01eld for the last decade. Selection-based QA\nimplies a family of tasks that \ufb01nd answer contexts\nfrom large data given questions in natural language.\nThree tasks have been proposed for selection-based\nQA. Given a document, answer extraction (Shen\nand Klakow, 2006; Sultan et al., 2016) \ufb01nds answer\nphrases whereas answer selection (Wang et al.,\n2007; Yih et al., 2013; Yu et al., 2014; Wang et al.,\n2016) and answer triggering (Yang et al., 2015;\nJurczyk et al., 2016) \ufb01nd answer sentences instead,\nalthough the presence of the answer context is not\nassumed within the provided document for answer\ntriggering but it is for the other two tasks. Recently,\nvarious QA tasks that are not selection-based have\nbeen proposed (Reddy and Bandyopadhyay, 2006;\nHosseini et al., 2014; Jauhar et al., 2016; Sachan\net al., 2016); however, selection-based QA remains\nstill important because of its practical value to real\napplications (e.g., IBM Watson, MIT START).\nSeveral datasets have been released for selection-\nbased QA. Wang et al. (2007) created theQAS ENT\ndataset consisting of 277 questions, which has been\nwidely used for benchmarking the answer selection\ntask. Feng et al. (2015) presented INSURANCE QA\ncomprising 16K+ questions on insurance contexts.\nYang et al. (2015) introduced WIKI QA for answer",
            0.9477783749999844
          ]
        },
        {
          "answer": "Seven question types",
          "relevant_chunks": [
            "considering their questions are taken from search\nqueries. INFOBOX QA\u2019s average question length is\nrelatively small, due to the restricted information\nthat can be asked from the infoboxes.INFOBOX QA\nand WIKI QA show the least question-answer word\noverlaps over questions and answers (\u2126q and \u2126a in\nTable 1), respectively. In terms of the F1-score for\noverlapping words (\u2126f ), SQUAD gives the least\nportion of overlaps between question-answer pairs\n\nWIKI QA SELQA SQUAD\n(\u03c1,\u03b3c,\u03b3p),t \u2265 0.3 ( 92 .00, 1,203, 96.86) (90.00, 7,446, 94.28) (100.00, 93,928, 95.61)\n(\u03c1,\u03b3c,\u03b3p),t \u2265 0.4 ( 94.00, 1,139, 91.71) (94.00, 7,133, 90.31) (100.00, 93,928, 95.61)\n(\u03c1,\u03b3c,\u03b3p),t \u2265 0.5 (100.00, 1,051, 84.62) (98.00, 6,870, 86.98) (100.00, 93,928, 95.61)\nk= (1,5,10,20) (4.39,12.47,16.59,22.39) (20.01,34.07,40.29,46.40) (19.90,35.08,40.96,46.74)\nTable 2: Statistics of the silver-standard dataset (\ufb01rst three rows) and the accuracies of answer retrieval in % (last row).\n\u03c1: robustness of the silver-standard in %, \u03b3c/p: #/% of retrieved silver-standard passages (coverage).\nalthough WIKI QA comes very close.\nFig. 1 shows the distributions of seven question\ntypes grouped deterministically from the lexicons.\nAlthough these corpora have been independently\ndeveloped, a general trend is found, where thewhat\nquestion type dominates, followed byhow and who,\nfollowed by when and where, and so on.\n0\n15\n30\n45\n60\nWhat How Who When Where Which Why\nWikiQA SelQA SQuAD InfoboxQA\nFigure 1: Distributions of question types in %.\nFig. 2 shows the distributions of answer categories\nautomatically classi\ufb01ed by our Convolutional Neu-\nral Network model trained on the data distributed\nby Li and Roth (2002).2 Interestingly, each corpus\nfocuses on different categories, Numeric for WIK-\nIQA and SELQA, Entity for SQUAD, and Person\nfor INFOBOX QA, which gives enough diversities\nfor statistical learning to build robust models.\nNumeric\nPerson\nEntity\nDescription\nLocation\nAbbreviation\n0 10 20 30 40\nWikiQA SelQA SQuAD InfoboxQA\nFigure 2: Distributions of answer categories in %.\n2Our CNN model shows 95.20% accuracy on their test set.\n3 Answer Retrieval\nThis section describes another selection-based QA\n\nwidely used for benchmarking the answer selection\ntask. Feng et al. (2015) presented INSURANCE QA\ncomprising 16K+ questions on insurance contexts.\nYang et al. (2015) introduced WIKI QA for answer\nselection and triggering. Jurczyk et al. (2016) cre-\nated SELQA for large real-scale answer triggering.\nRajpurkar et al. (2016) presented SQUAD for an-\nswer extraction and selection as well as for reading\ncomprehension. Finally, Morales et al. (2016) pro-\nvided INFOBOX QA for answer selection.\nThese corpora make it possible to evaluate the ro-\nbustness of statistical question answering learning.\nAlthough all of these corpora target on selection-\nbased QA, they are designed for different purposes\nsuch that it is important to understand the nature of\nthese corpora so a better use of them can be made.\nIn this paper, we make both intrinsic and extrinsic\nanalyses of four latest corpora based on Wikipedia,\nWIKI QA, SELQA, SQUAD, and INFOBOX QA.\nWe \ufb01rst give a thorough intrinsic analysis regarding\ncontextual similarities, question types, and answer\ncategories (Section 2). We then map questions in all\ncorpora to the current version of English Wikipedia\nand benchmark another selection-based QA task,\nanswer retrieval (Section 3). Finally, we present\nan extrinsic analysis through a set of experiments\ncross-testing these corpora using a convolutional\nneural network architecture (Section 4).1\n2 Intrinsic Analysis\nFour publicly available corpora are selected for our\nanalysis. These corpora are based on Wikipedia, so\nmore comparable than the others, and have already\nbeen used for the evaluation of several QA systems.\n1All our resources are publicly available\n: anonymous_url\narXiv:1801.02073v2  [cs.CL]  5 Feb 2018\n\nWIKIQA SELQA SQUAD INFOBOX QA\nSource Bing search queries Crowdsourced Crowdsourced Crowdsourced\nYear 2015 2016 2016 2016\n(AE, AS, AT) (O, O, O) (X, O, O) (O, O, X) (X, O, X)\n(q,c,c/q) (1,242, 12,153, 9.79) (7,904, 95,250, 12.05) (98,202, 496,167, 5.05) (15,271, 271,038, 17.75)\n(w,t) (386,440, 30,191) (3,469,015, 44,099) (19,445,863, 115,092) (5,034,625, 8,323)\n\ncan be used for the evaluation of answer triggering.\nSQUAD (Rajpurkar et al., 2016) presents 107K+\ncrowdsourced questions on 536 Wikipedia articles,\nwhere the answer contexts are guaranteed to exist\nwithin the provided paragraph. It contains annota-\ntion of answer phrases as well as the pointers to the\nsentences including the answer phrases; thus, it can\nbe used for both answer extraction and selection.\nThis corpus also provides human accuracy on those\nquestions, setting up a reasonable upper bound for\nmachines. To avoid over\ufb01tting, the evaluation set\nis not publicly available although system outputs\ncan be evaluated by their provided script.\nINFOBOX QA (Morales et al., 2016) gives 15K+\nquestions based on the infoboxes from 150 articles\nin Wikipedia. Each question is crowdsourced and\nassociated with an infobox, where each line of the\ninfobox is considered an answer candidate. This\ncorpus emphasizes the gravity of infoboxes, which\nsummary arguably the most commonly asked infor-\nmation about those articles. Although the nature of\nthis corpus is different from the others, it can also\nbe used to evaluate answer selection.\nAnalysis\nAll corpora provide datasets/splits for answer se-\nlection, whereas only ( WIKI QA, SQUAD) and\n(WIKI QA, SELQA) provide datasets for answer\nextraction and answer triggering, respectively.\nSQUAD is much larger in size although ques-\ntions in this corpus are often paraphrased multi-\nple times. On the contrary, SQUAD\u2019s average\ncandidates per question ( c/q) is the smallest be-\ncause SQUAD extracts answer candidates from\nparagraphs whereas the others extract them from\nsections or infoboxes that consist of bigger con-\ntexts. Although INFOBOX QA is larger than WIK-\nIQA or SELQA, the number of token types (t) in\nINFOBOX QA is smaller than those two, due to the\nrepetitive nature of infoboxes.\nAll corpora show similar average answer candi-\ndate lengths (\u00b5c), except for INFOBOX QA where\neach line in the infobox is considered a candidate.\nSELQA and SQUAD show similar average ques-\ntion lengths (\u00b5q) because of the similarity between\ntheir annotation schemes. It is not surprising that\nWIKI QA\u2019s average question length is the smallest,\nconsidering their questions are taken from search\nqueries. INFOBOX QA\u2019s average question length is\nrelatively small, due to the restricted information\nthat can be asked from the infoboxes.INFOBOX QA\nand WIKI QA show the least question-answer word",
            0.5583340830000054
          ]
        },
        {
          "answer": "The provided context does not contain specific details on how they analyze contextual similarities across datasets.",
          "relevant_chunks": [
            "curacy, which are reasonable. However, WIKI QA\nshows a signi\ufb01cantly lower accuracy of 12.47%;\nthis is because the questions in WIKI QA is about\ntwice shorter than the questions in the other cor-\npora such that not enough lexicons can be extracted\nfrom these questions for the Lucene search.\n4.3 Answer Triggering\nThe results of k= 5 from the answer retrieval task\nin Section 4.2 are used to create the datasets for an-\nswer triggering, where about 65% of the questions\nare not expected to \ufb01nd their answer contexts from\nthe provided paragraphs for SELQA and SQUAD\nand 87.5% are not expected for WIKI QA. Answer\ntriggering is evaluated by the F1 scores as presented\nin Table 3, where three corpora are cross validated.\nThe results on WIKI QA are pretty low as expected\nfrom the poor accuracy on the answer retrieval task.\nTraining on SELQA gives the best models for both\nWIKI QA and SELQA. Training on SQUAD gives\nthe best model for SQUAD although the model\ntrained on SELQA is comparable. Since the answer\ntriggering datasets are about 5 times larger than the\nanswer selection datasets, it is computationally too\nexpensive to combine all data for training. We plan\nto \ufb01nd a strong machine to perform this experiment\nin near future.\n5 Related work\nLately, several deep learning approaches have been\nproposed for question answering. Yu et al. (2014)\npresented a CNN model that recognizes the seman-\ntic similarity between two sentences. Wang and\nNyberg (2015) presented a stacked bidirectional\nLSTM approach to read words in sequence, then\noutputs their similarity scores. Feng et al. (2015)\napplied a general deep learning framework to non-\nfactoid question answering. Santos et al. (2016)\nintroduced an attentive pooling mechanism that led\nto further improvements in selection-based QA.\n6 Conclusion\nWe present a comprehensive comparison study of\nthe existing corpora for selection-based question\nanswering. Our intrinsic analysis provides a better\nunderstanding of the uniqueness or similarity be-\ntween these corpora. Our extrinsic analysis shows\nthe strength or weakness of combining these cor-\npora together for statistical learning. Additionally,\nwe create a silver-standard dataset for answer re-\ntrieval and triggering, which will be publicly avail-\nable. In the future, we will explore different ways\nof improving the quality of our silver-standard\ndatasets by \ufb01ne-tuning the hyper-parameters.\n\nReferences\n\nAnalysis of Wikipedia-based Corpora for Question Answering\nTomasz Jurczyk1, Amit Deshmane2, Jinho D. Choi1\n1Mathematics and Computer Science, Emory University\n2Infosys Ltd.\n{tomasz.jurczyk,jinho.choi}@emory.edu\namit deshmane@infosys.com\nAbstract\nThis paper gives comprehensive analyses\nof corpora based on Wikipedia for several\ntasks in question answering. Four recent\ncorpora are collected, WIKI QA, SELQA,\nSQUAD, and INFOBOX QA, and \ufb01rst an-\nalyzed intrinsically by contextual similari-\nties, question types, and answer categories.\nThese corpora are then analyzed extrinsi-\ncally by three question answering tasks, an-\nswer retrieval, selection, and triggering. An\nindexing-based method for the creation of a\nsilver-standard dataset for answer retrieval\nusing the entire Wikipedia is also presented.\nOur analysis shows the uniqueness of these\ncorpora and suggests a better use of them\nfor statistical question answering learning.\n1 Introduction\nQuestion answering (QA) has been a blooming re-\nsearch \ufb01eld for the last decade. Selection-based QA\nimplies a family of tasks that \ufb01nd answer contexts\nfrom large data given questions in natural language.\nThree tasks have been proposed for selection-based\nQA. Given a document, answer extraction (Shen\nand Klakow, 2006; Sultan et al., 2016) \ufb01nds answer\nphrases whereas answer selection (Wang et al.,\n2007; Yih et al., 2013; Yu et al., 2014; Wang et al.,\n2016) and answer triggering (Yang et al., 2015;\nJurczyk et al., 2016) \ufb01nd answer sentences instead,\nalthough the presence of the answer context is not\nassumed within the provided document for answer\ntriggering but it is for the other two tasks. Recently,\nvarious QA tasks that are not selection-based have\nbeen proposed (Reddy and Bandyopadhyay, 2006;\nHosseini et al., 2014; Jauhar et al., 2016; Sachan\net al., 2016); however, selection-based QA remains\nstill important because of its practical value to real\napplications (e.g., IBM Watson, MIT START).\nSeveral datasets have been released for selection-\nbased QA. Wang et al. (2007) created theQAS ENT\ndataset consisting of 277 questions, which has been\nwidely used for benchmarking the answer selection\ntask. Feng et al. (2015) presented INSURANCE QA\ncomprising 16K+ questions on insurance contexts.\nYang et al. (2015) introduced WIKI QA for answer\n\nMrinmaya Sachan, Kumar Dubey, and Eric Xing. 2016.\nScience question answering using instructional ma-\nterials. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers). Association for Computa-\ntional Linguistics, Berlin, Germany, pages 467\u2013473.\nhttp://anthology.aclweb.org/P16-2076.\nC\u00b4\u0131cero Nogueira dos Santos, Ming Tan, Bing\nXiang, and Bowen Zhou. 2016. Attentive\npooling networks. CoRR abs/1602.03609.\nhttp://arxiv.org/abs/1602.03609.\nDan Shen and Dietrich Klakow. 2006. Exploring cor-\nrelation of dependency relation paths for answer ex-\ntraction. In Proceedings of the 21st International\nConference on Computational Linguistics and the\n44th annual meeting of the Association for Compu-\ntational Linguistics. Association for Computational\nLinguistics, pages 889\u2013896.\nMd Arafat Sultan, Vittorio Castelli, and Radu Florian.\n2016. A joint model for answer sentence ranking\nand answer extraction. Transactions of the Associa-\ntion for Computational Linguistics 4:113\u2013125.\nDi Wang and Eric Nyberg. 2015. A long short-term\nmemory model for answer sentence selection in\nquestion answering. In Proceedings of the 53rd\nAnnual Meeting of the Association for Computa-\ntional Linguistics and the 7th International Joint\nConference on Natural Language Processing (Vol-\nume 2: Short Papers) . Association for Computa-\ntional Linguistics, Beijing, China, pages 707\u2013712.\nhttp://www.aclweb.org/anthology/P15-2116.\nMengqiu Wang, Noah A. Smith, and Teruko Mitamura.\n2007. What is the Jeopardy Model? A Quasi-\nSynchronous Grammar for QA. In Proceedings of\nthe Joint Conference on Empirical Methods in Nat-\nural Language Processing and Computational Natu-\nral Language Learning. EMNLP-CoNLL\u201907, pages\n22\u201332.\nZhiguo Wang, Haitao Mi, and Abraham Ittycheriah.\n2016. Sentence Similarity Learning by Lex-\nical Decomposition and Composition. arXiv\narXiv:1602.07019.\nYi Yang, Wen-tau Yih, and Christopher Meek. 2015.\nWIKIQA: A Challenge Dataset for Open-Domain\nQuestion Answering. In Proceedings of the Con-\nference on Empirical Methods in Natural Language\nProcessing. EMNLP\u201915, pages 2013\u20132018.\nWen-tau Yih, Ming-Wei Chang, Christopher Meek, and\nAndrzej Pastusiak. 2013. Question answering us-\ning enhanced lexical semantic models. In Proceed-\nings of the 51st Annual Meeting of the Associa-\ntion for Computational Linguistics . ACL\u201913, pages\n1744\u20131753.\nLei Yu, Karl Moritz Hermann, Phil Blunsom, and\nStephen Pulman. 2014. Deep Learning for Answer\nSentence Selection. In Proceedings of the NIPS",
            0.5702811669999903
          ]
        }
      ],
      "reference_answers": [
        "Yes No",
        "seven  7",
        "They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap."
      ]
    },
    "1707.00995.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.24096278452395947,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.577922077922078,
          "recall": 0.6785714285714286,
          "f1": 0.582010582010582
        },
        "rouge-2": {
          "precision": 0.42933810375670844,
          "recall": 0.4642857142857143,
          "f1": 0.4126117179741807
        },
        "rouge-L": {
          "precision": 0.5308441558441559,
          "recall": 0.5993303571428572,
          "f1": 0.5251322751322751
        }
      },
      "per_question_metrics": [
        {
          "question": "What misbehavior is identified?",
          "generated_answer": "The identified misbehavior is that if the attention mechanism loses track of the objects in the picture and \"gets lost\", the model still takes the image into account and somehow overrides the information brought by the text-based annotations, leading to totally misleading translations.",
          "reference_answer": "if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.7272727272727273,
              "recall": 0.5,
              "f1": 0.5925925925925926
            },
            "rouge-2": {
              "precision": 0.627906976744186,
              "recall": 0.42857142857142855,
              "f1": 0.5094339622641509
            },
            "rouge-L": {
              "precision": 0.7045454545454546,
              "recall": 0.484375,
              "f1": 0.5740740740740741
            }
          },
          "bleu_score": 0.3958297319440786,
          "exact_match": 0.0
        },
        {
          "question": "Which attention mechanisms do they compare?",
          "generated_answer": "The attention mechanisms compared are the soft attention and the hard stochastic attention mechanisms.",
          "reference_answer": "Soft attention, Hard Stochastic attention, Local Attention",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.42857142857142855,
              "recall": 0.8571428571428571,
              "f1": 0.5714285714285714
            },
            "rouge-2": {
              "precision": 0.23076923076923078,
              "recall": 0.5,
              "f1": 0.3157894736842105
            },
            "rouge-L": {
              "precision": 0.35714285714285715,
              "recall": 0.7142857142857143,
              "f1": 0.4761904761904762
            }
          },
          "bleu_score": 0.08609583710384033,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "What misbehavior is identified?",
        "Which attention mechanisms do they compare?"
      ],
      "generated_answers": [
        {
          "answer": "The identified misbehavior is that if the attention mechanism loses track of the objects in the picture and \"gets lost\", the model still takes the image into account and somehow overrides the information brought by the text-based annotations, leading to totally misleading translations.",
          "relevant_chunks": [
            "are decoding as shown in the right image) and\nthe gating scalar has learned to decide whether\nor not we have to look at the picture (or more\naccurately whether or not we are translating an\nobject). Without this scalar, the translation scores\nundergo a massive drop (as seen in Caglayan\net al. (2016)) which means that the attention\nmechanisms don\u2019t really understand the more\ncomplex relationships between objects, what is\nreally happening in the scene. Surprisingly, the\n\nFigure 7: Representative \ufb01gures of the soft-attention behavior discussed in \u00a75.3\ngating scalar happens to be really low in the\nstochastic attention mechanism: a signi\ufb01cant\namount of sentences don\u2019t have a summed gating\nscalar \u22650.10. The model totally discards the\nimage in the translation process.\nIt is also worth to mention that we use a\nResNet trained on 1.28 million images for a\nclassi\ufb01cation tasks. The features used by the\nattention mechanism are strongly object-oriented\nand the machine could miss important information\nfor a multimodal translation task. We believe\nthat the robust architecture of both encoders\n{\u2190 \u2212\u03a8enc,\u2212 \u2192\u03a8enc}combined with a GRU layer and\nword-embeddings took care of the right trans-\nlation for relationships between objects and\ntime-dependencies. Yet, we noticed a common\nmisbehavior for all our multimodal models: if the\nattention loose track of the objects in the picture\nand \u201dgets lost\u201d, the model still takes it into account\nand somehow overrides the information brought\nby the text-based annotations. The translation\nis then totally mislead. We illustrate with an\nexample:\nRef: Ein Kind sitzt auf den Schultern einer\nFrau und klatscht .\nMono: Ein Kind sitzt auf den Schultern einer\nFrau und schl\u00a8aft .\nSoft: Ein Kind , das sich auf der Schultern\neines Frau reitet , f\u00a8ahrt auf den\nSchultern .\nHard: Ein Kind in der Haltung , w \u00a8ahrend er\nauf den Schultern einer Frau f\u00a8ahrt .\nThe monomodal translation has a sentence-level\nBLEU of 82.16 whilst the soft attention and hard\nstochastic attention scores are of 16.82 and 34.45\nrespectively. Figure 8 shows the attention maps\nfor both mechanism. Nevertheless, one has to\nconcede that the use of images indubitably helps\nthe translation as shown in the score tabular.\nFigure 8: Wrong detection for both Soft attention\n(top) and Hard stochastic attention (bottom)\n6 Conclusion and future work\nWe have tried different attention mechanism and\n\n2016).\nWe also normalize and tokenize English and\nGerman descriptions using the Moses tokenizer\nscripts (Koehn et al., 2007). We use the byte pair\nencoding algorithm on the train set to convert\nspace-separated tokens into subwords (Sennrich\net al., 2016), reducing our vocabulary size to\n9226 and 14957 words for English and German\nrespectively.\nAll variants of our attention model were trained\nwith ADADELTA (Zeiler, 2012), with mini-\nbatches of size 80 for our monomodal (text-only)\nNMT model and 40 for our multimodal NMT. We\napply early stopping for model selection based\non BLEU4 : training is halted if no improvement\non the development set is observed for more than\n20 epochs. We use the metrics BLEU4 (Papineni\net al., 2002), METEOR (Denkowski and Lavie,\n2014) and TER (Snover et al., 2006) to evaluate\nthe quality of our models\u2019 translations.\n5.2 Quantitative results\nWe notice a nice overall progress over Calixto\net al. (2017) multimodal baseline, especially\nwhen using the stochastic attention. With im-\nprovements of +1.51 BLEU and -2.2 TER on both\nprecision-oriented metrics, the model shows a\nstrong similarity of the n-grams of our candidate\ntranslations with respect to the references. The\nmore recall-oriented metrics METEOR scores\n\nModel Test Scores\nBLEU\u2191 METEOR\u2191 TER\u2193\nMonomodal (text only)\nCaglayan et al. (2016) 32.50 49.2\nCalixto et al. (2017) 33.70 52.3 46.7\nNMT 34.11 \u2191+0.41 52.4 \u2191+0.1 46.2 \u2193-0.5\nMultimodal\nCaglayan et al. (2016) 27.82 45.0 -\nHuang et al. (2016) 36.50 54.1 -\nCalixto et al. (2017) 36.50 55.0 43.7\nSoft attention 37.10 \u2191+0.60 54.8 \u2193-0.2 42.8 \u2193-0.9\nLocal attention 37.55 \u2191+1.05 54.8 \u2193-0.2 42.4 \u2193-1.3\nStochastic attention 38.01 \u2191+1.51 55.4 \u2191+0.4 41.5 \u2193-2.2\nSoft attention + grounded image 37.62 \u2191+1.12 55.3 \u2191+0.3 41.8 \u2193-1.9\nStochastic attention + grounded image 38.17 \u2191+1.67 55.4 \u2191+0.4 41.5 \u2193-2.2\nTable 1: Results on the 1000 test triples of the Multi30K dataset. We pick Calixto et al. (2017) scores\nas baseline and report our results accordingly (green for improvement and red for deterioration). In each\nof our experiments, Soft attention is used for text. The comparison is hence with respect to the attention\nmechanism used for the image modality.\nare roughly the same across our models which is\n\nthe translation as shown in the score tabular.\nFigure 8: Wrong detection for both Soft attention\n(top) and Hard stochastic attention (bottom)\n6 Conclusion and future work\nWe have tried different attention mechanism and\ntweaks for the image modality. We showed im-\nprovements and encouraging results overall on the\nFlickr30K Entities dataset. Even though we iden-\nti\ufb01ed some \ufb02aws of the current attention mecha-\nnisms, we can conclude pretty safely that images\nare an helpful resource for the machine in a trans-\nlation task. We are looking forward to try out\nricher and more suitable features for multimodal\ntranslation (ie. dense captioning features). An-\nother interesting approach would be to use visu-\nally grounded word embeddings to capture visual\nnotions of semantic relatedness.\n\n7 Acknowledgements\nThis work was partly supported by the Chist-Era\nproject IGLU with contribution from the Belgian\nFonds de la Recherche Scientique (FNRS), con-\ntract no. R.50.11.15.F, and by the FSO project\nVCYCLE with contribution from the Belgian\nWaloon Region, contract no. 1510501.\nReferences\nJimmy Ba, V olodymyr Mnih, and Koray Kavukcuoglu.\n2015. Multiple object recognition with visual atten-\ntion. In Proceedings of the International Conference\non Learning Representations (ICLR).\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2014. Neural machine translation by\njointly learning to align and translate. CoRR\nabs/1409.0473.\nOzan Caglayan, Walid Aransa, Yaxing Wang,\nMarc Masana, Mercedes Garc \u00b4\u0131a-Mart\u00b4\u0131nez, Fethi\nBougares, Lo \u00a8\u0131c Barrault, and Joost van de Wei-\njer. 2016. Does multimodality help human and\nmachine for translation and image captioning?\nIn Proceedings of the First Conference on Ma-\nchine Translation. Association for Computational\nLinguistics, Berlin, Germany, pages 627\u2013633.\nhttp://www.aclweb.org/anthology/W/W16/W16-\n2358.\nIacer Calixto, Qun Liu, and Nick Campbell. 2017.\nDoubly-attentive decoder for multi-modal neu-\nral machine translation. CoRR abs/1702.01287.\nhttp://arxiv.org/abs/1702.01287.\nKyunghyun Cho, Bart van Merri \u00a8enboer, C \u00b8 alar\nG\u00a8ulc \u00b8ehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using rnn encoder\u2013decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP). Association\nfor Computational Linguistics, Doha, Qatar, pages\n1724\u20131734. http://www.aclweb.org/anthology/D14-\n1179.\nJunyoung Chung, Caglar Gulcehre, Kyunghyun Cho,",
            0.8451679579999904
          ]
        },
        {
          "answer": "The attention mechanisms compared are the soft attention and the hard stochastic attention mechanisms.",
          "relevant_chunks": [
            "of our experiments, Soft attention is used for text. The comparison is hence with respect to the attention\nmechanism used for the image modality.\nare roughly the same across our models which is\nexpected because all attention mechanisms share\nthe same subsequent step at every time-step t,\ni.e. taking into account the attention weights of\nprevious time-step t\u22121 in order to compute the\nnew intermediate hidden state proposal and there-\nfore the new context vector it. Again, the largest\nimprovement is given by the hard stochastic\nattention mechanism (+0.4 METEOR): because it\nis modeled as a decision process according to the\nprevious choices, this may reinforce the idea of\nrecall. We also remark interesting improvements\nwhen using the grounded mechanism, especially\nfor the soft attention. The soft attention may\nbene\ufb01t more of the grounded image because of\nthe wide range of spatial locations it looks at,\nespecially compared to the stochastic attention.\nThis motivates us to dig into more complex\ngrounding techniques in order to give the machine\na deeper understanding of the modalities.\nNote that even though our baseline NMT\nmodel is basically the same as Calixto et al.\n(2017), our experiments results are slightly better.\nThis is probably due to the different use of dropout\nand subwords. We also compared our results to\nCaglayan et al. (2016) because our multimodal\nmodels are nearly identical with the major ex-\nception of the gating scalar (cfr. section 4). This\nmotivated some of our qualitative analysis and\nhesitation towards the current architecture in the\nnext section.\n5.3 Qualitative results\nFor space-saving and ergonomic reasons, we\nonly discuss about the hard stochastic and soft\nattention, the latter being a generalization of the\nlocal attention.\nAs we can see in Figure 7, the soft attention model\nis looking roughly at the same region of the image\nfor every decoding step t. Because the words\n\u201dhund\u201d(dog), \u201dwald\u201d(forest) or \u201dweg\u201d(way) in left\nimage are objects, they bene\ufb01t from a high gating\nscalar. As a matter of fact, the attention mech-\nanism has learned to detect the objects within a\nscene (at every time-step, whichever word we\nare decoding as shown in the right image) and\nthe gating scalar has learned to decide whether\nor not we have to look at the picture (or more\naccurately whether or not we are translating an\n\nare decoding as shown in the right image) and\nthe gating scalar has learned to decide whether\nor not we have to look at the picture (or more\naccurately whether or not we are translating an\nobject). Without this scalar, the translation scores\nundergo a massive drop (as seen in Caglayan\net al. (2016)) which means that the attention\nmechanisms don\u2019t really understand the more\ncomplex relationships between objects, what is\nreally happening in the scene. Surprisingly, the\n\nFigure 7: Representative \ufb01gures of the soft-attention behavior discussed in \u00a75.3\ngating scalar happens to be really low in the\nstochastic attention mechanism: a signi\ufb01cant\namount of sentences don\u2019t have a summed gating\nscalar \u22650.10. The model totally discards the\nimage in the translation process.\nIt is also worth to mention that we use a\nResNet trained on 1.28 million images for a\nclassi\ufb01cation tasks. The features used by the\nattention mechanism are strongly object-oriented\nand the machine could miss important information\nfor a multimodal translation task. We believe\nthat the robust architecture of both encoders\n{\u2190 \u2212\u03a8enc,\u2212 \u2192\u03a8enc}combined with a GRU layer and\nword-embeddings took care of the right trans-\nlation for relationships between objects and\ntime-dependencies. Yet, we noticed a common\nmisbehavior for all our multimodal models: if the\nattention loose track of the objects in the picture\nand \u201dgets lost\u201d, the model still takes it into account\nand somehow overrides the information brought\nby the text-based annotations. The translation\nis then totally mislead. We illustrate with an\nexample:\nRef: Ein Kind sitzt auf den Schultern einer\nFrau und klatscht .\nMono: Ein Kind sitzt auf den Schultern einer\nFrau und schl\u00a8aft .\nSoft: Ein Kind , das sich auf der Schultern\neines Frau reitet , f\u00a8ahrt auf den\nSchultern .\nHard: Ein Kind in der Haltung , w \u00a8ahrend er\nauf den Schultern einer Frau f\u00a8ahrt .\nThe monomodal translation has a sentence-level\nBLEU of 82.16 whilst the soft attention and hard\nstochastic attention scores are of 16.82 and 34.45\nrespectively. Figure 8 shows the attention maps\nfor both mechanism. Nevertheless, one has to\nconcede that the use of images indubitably helps\nthe translation as shown in the score tabular.\nFigure 8: Wrong detection for both Soft attention\n(top) and Hard stochastic attention (bottom)\n6 Conclusion and future work\nWe have tried different attention mechanism and\n\nfrom Bahdanau et al. (2014) where their attention\ntakes as input the previous decoder hidden state\ninstead of the current (intermediate) one as shown\nin equation 7. This mechanism has also been\nsuccessfully investigated for the task of image\ndescription generation (Xu et al., 2015) where\na model generates an image\u2019s description in\nnatural language. It has been used in multimodal\ntranslation as well (Calixto et al., 2017), for which\nit constitutes a state-of-the-art.\nThe idea of the soft attentional model is to\nconsider all the annotations when deriving the\ncontext vector it. It consists of a single feed-\nforward network used to compute an expected\nalignment et between modality annotation al and\nthe target word to be emitted at the current time\nstep t. The inputs are the modality annotations\nand the intermediate representation of REC1 s\u2032\nt:\net,l = vT tanh(Uas\u2032\nt + Waal) (12)\nThe vector et has length Land its l-th item con-\ntains a score of how much attention should be put\non the l-th annotation in order to output the best\nword at time t. We compute normalized scores to\ncreate an attention mask \u03b1t over annotations:\n\u03b1t,i = exp(et,i)\u2211L\nj=1 exp(et,j)\n(13)\nit =\nL\u2211\ni=1\n\u03b1t,iai (14)\nFinally, the modality time-dependent context vec-\ntor it is computed as a weighted sum over the an-\nnotation vectors (equation 14). In the above ex-\npressions, vT, Ua and Wa are trained parameters.\nFigure 1: Die beiden Kinder spielen auf dem\nSpielplatz .\n\nFigure 2: Ein Junge sitzt auf und blickt aus einem\nMikroskop .\n3.2 Hard Stochastic attention\nThis model is a stochastic and sampling-based\nprocess where, at every timestep t, we are making\na hard choice to attend only one annotation. This\ncorresponds to one spatial location in the image.\nHard attention has previously been used in the\ncontext of object recognition (Mnih et al., 2014;\nBa et al., 2015) and later extended to image\ndescription generation (Xu et al., 2015). In the\ncontext of multimodal NMT, we can follow Xu\net al. (2015) because both our models involve the\nsame process on images.\nThe mechanism fatt is now a function that",
            2.4795386669999857
          ]
        }
      ],
      "reference_answers": [
        "if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations",
        "Soft attention, Hard Stochastic attention, Local Attention"
      ]
    },
    "1707.00110.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.011816260923029082,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.1539855072463768,
          "recall": 0.5208333333333333,
          "f1": 0.2033333333333333
        },
        "rouge-2": {
          "precision": 0.014705882352941176,
          "recall": 0.2,
          "f1": 0.0273972602739726
        },
        "rouge-L": {
          "precision": 0.11231884057971014,
          "recall": 0.4583333333333333,
          "f1": 0.15333333333333332
        }
      },
      "per_question_metrics": [
        {
          "question": "Which baseline methods are used?",
          "generated_answer": "The baseline methods used for comparison are models with and without attention.",
          "reference_answer": "standard parametrized attention and a non-attention baseline",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.25,
              "recall": 0.375,
              "f1": 0.3
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.16666666666666666,
              "recall": 0.25,
              "f1": 0.2
            }
          },
          "bleu_score": 0.020448007360218394,
          "exact_match": 0.0
        },
        {
          "question": "Which datasets are used in experiments?",
          "generated_answer": "The experiments used the following datasets:\n\n1. Four large machine translation datasets from WMT'17 on the language pairs:\n   - English-Czech (en-cs, 52M examples)\n   - English-German (en-de, 5.9M examples)\n   - English-Finish (en-fi, 2.6M examples)\n   - English-Turkish (en-tr, 207,373 examples)\n\nThese datasets were part of the WMT'17 translation task, and preprocessed datasets for the WMT'17 task were utilized in the experiments.",
          "reference_answer": "Sequence Copy Task and WMT'17",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.057971014492753624,
              "recall": 0.6666666666666666,
              "f1": 0.10666666666666666
            },
            "rouge-2": {
              "precision": 0.029411764705882353,
              "recall": 0.4,
              "f1": 0.0547945205479452
            },
            "rouge-L": {
              "precision": 0.057971014492753624,
              "recall": 0.6666666666666666,
              "f1": 0.10666666666666666
            }
          },
          "bleu_score": 0.0031845144858397693,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "Which baseline methods are used?",
        "Which datasets are used in experiments?"
      ],
      "generated_answers": [
        {
          "answer": "The baseline methods used for comparison are models with and without attention.",
          "relevant_chunks": [
            "memory matrix. To investigate the trade-off between\nspeed and performance, we compare our technique\nto standard models with and without attention on a\nSequence Copy Task of varying length like in Graves\net al. (2014). We generated 4 training datasets of\n100,000 examples and a validation dataset of 1,000 ex-\namples. The vocabulary size was 20. For each dataset,\nthe sequences had lengths randomly chosen between 0\nto L, forL\u2208{10,50,100,200}unique to each dataset.\n4.1.1 Training Setup\nAll models are implemented using TensorFlow\nbased on the seq2seq implementation of Britz et al.\n(2017)3 and trained on a single machine with a\nNvidia K40m GPU. We use a 2-layer 256-unit, a\nbidirectional LSTM (Hochreiter and Schmidhuber,\n1997) encoder, a 2-layer 256-unit LSTM decoder,\nand 256-dimensional embeddings. For the attention\nbaseline, we use the standard parametrized attention\n(Bahdanau et al., 2014). Dropout of 0.2 (0.8 keep\nprobability) is applied to the input of each cell and\nwe optimize using Adam (Kingma and Ba, 2014) at\na learning rate of 0.0001 and batch size 128. We train\nfor at most 200,000 steps (see Figure 3 for sample\nlearning curves). BLEU scores are calculated on\ntokenized data using themulti-bleu.perlscript in\nMoses.4 We decode using beam search with a beam\n3http://github.com/google/seq2seq\n4http://github.com/moses-smt/mosesdecoder\nLength Model BLEU Time (s)\n20 No Att 99.93 2.03\nK=1 99.52 2.12\nK=4 99.56 2.25\nK=16 99.56 2.21\nK=32 99.57 2.59\nK=64 99.75 2.86\nAtt 99.98 2.86\n50 No Att 97.37 3.90\nK=1 98.86 4.33\nK=4 99.95 4.48\nK=16 99.96 4.58\nK=32 99.96 5.35\nK=64 99.97 5.84\nAtt 99.94 6.46\n100 No Att 73.99 6.33\nK=1 87.42 7.32\nK=4 99.81 7.47\nK=16 99.97 7.50\nK=32 99.99 7.65\nK=64 100.00 7.77\nAtt 100.00 11.00\n200 No Att 32.64 9.10\nK=1 44.22 9.30\nK=4 98.54 9.49\nK=16 99.98 9.53\nK=32 100.00 9.59\nK=64 100.00 9.78\nAtt 100.00 14.28\nTable 1: BLEU scores and computation times with\nvaryingKand sequence length compared to baseline\nmodels with and without attention.\nsize of 10 (Wiseman and Rush, 2016).\n4.1.2 Results\nTable 1 shows the BLEU scores of our model on differ-\nent sequence lengths while varyingK. This is a study\nof the trade-off between computational time and rep-\nresentational power. A largeKallows us to compute\n\na representation of the source sequence, and then\nperforminglocation-basedattention on the memory\nby picking which row of the matrix to attend to.\nStandard location-based attention mechanism, by\ncontrast, predicts a location in the source sequence\nto focus on (Luong et al., 2015; Xu et al., 2015).\n3.2 Position Encodings (PE)\nIn the above formulation, the predictions of attention\ncontexts are symmetric. That is,Ci is not forced to\nbe different fromCj\u0338=i. While we would hope for the\nmodel to learn to generate distinct attention contexts,\nwe now present an extension that pushes the model\ninto this direction. We addposition encodingsto the\nscore matrix that forces the first few context vector\nC1,C2,...to focus on the beginning of the sequence\nand the last few vectors...,CK\u22121,CK to focus on the\nend (thereby encouraging in-between vectors to focus\non the middle).\nExplicitly, we multiply the score vector\u03b1 with\nposition encodingsls\u2208RK:\nCPE =\n|S|\u2211\ns=0\n\u03b1PEhs (8)\n\u03b1PE\ns =softmax(W\u03b1hs\u25e6ls) (9)\nTo obtainls we first calculate a constant matrixL\nwhere we define each element as\nLks=(1\u2212k/K)(1\u2212s/S)+ k\nK\ns\nS, (10)\nadapting a formula from (Sukhbaatar et al., 2015).\nHere, k \u2208{1,2,...,K}is the context vector index\nand Sis the maximum sequence length across all\nsource sequences. The manifold is shown graphically\nin Figure 2. We can see that earlier encoder states are\nupweighted in the first context vectors, and later states\nare upweighted in later vectors. The symmetry of the\nmanifold and its stationary point having value 0.5 both\nfollow from Eq. 10. The elements of the matrix that\nfall beyond the sequence lengths are then masked out\nand the remaining elements are renormalized across\nthe timestep dimension. This results in the jagged\narray of position encodings{lks}.\n\nFigure 2: Surface for the position encodings.\n4 Experiments\n4.1 Toy Copying Experiment\nDue to the reduction of computational time complex-\nity we expect our method to yield performance gains\nespecially for longer sequences and tasks where the\nsource can be compactly represented in a fixed-size\nmemory matrix. To investigate the trade-off between\nspeed and performance, we compare our technique\nto standard models with and without attention on a\nSequence Copy Task of varying length like in Graves\n\ncoding time even on a complex dataset with a large\n6http://data.statmt.org/wmt17/translation-task/preprocessed\n\nK\nK\n(a) Training curves for en-fi\nK\nK (b) Training curves for en-tr\nFigure 4: Comparing training curves for en-fi and en-tr with sigmoid encoder scoring and softmax decoder\nscoring and position encoding. Note that en-tr curves converged very quickly.\nModel Dataset K en-cs en-de en-fi en-tr\nMemory Attention Test 32 19.37 28.82 15.87 -\n64 19.65 29.53 16.49 -\nV alid 32 19.20 26.20 15.90 12.94\n64 19.63 26.39 16.35 13.06\nMemory Attention + PETest 32 19.45 29.53 15.86 -\n64 20.36 30.61 17.03 -\nV alid 32 19.35 26.22 16.31 12.97\n64 19.73 27.31 16.91 13.25\nAttention Test - 19.19 30.99 17.34 -\nV alid - 18.61 28.13 17.16 13.76\nTable 2: BLEU scores on WMT\u201917 translation datasets from the memory attention models and regular attention\nbaselines. We picked the best out of the four scoring function combinations on the validation set. Note that\nen-tr does not have an official test set. Best test scores on each dataset are highlighted.\nModel Decoding Time (s)\nK=32 26.85\nK=64 27.13\nAttention 33.28\nTable 3: Decoding time, averaged across 10 runs, for\nthe en-de validation set (2169 examples) with average\nsequence length of 35. Results are similar for both\nPE and non-PE models.\nvocabulary of 16k. We measured decoding time over\nthe full validation set, not including time used for\nmodel setup and data loading, averaged across 10 runs.\nThe average sequence length for examples in this data\nwas 35, and we expect more significant speedups\nfor tasks with longer sequences, as suggested by our\nexperiments on toy data. Note that in our NMT ex-\namples/experiments,K\u2248T, but we obtain computa-\ntional savings from the fact thatK\u226aD. We may be\nable to setK\u226aT, as in toy copying, and still get very\ngood performance in other tasks. For instance, in sum-\nmarization the source is complex but the representa-\ntion of the source required to perform the task is \u201dsim-\nple\u201d (i.e. all that is needed to generate the abstract).\nFigure 5 shows the effect of using sigmoid and\nsoftmax function in the encoders and decoders. We\nfound that softmax/softmax consistently performs\nbadly, while all other combinations perform about\nequally well. We report results for the best combi-",
            1.1555258339999739
          ]
        },
        {
          "answer": "The experiments used the following datasets:\n\n1. Four large machine translation datasets from WMT'17 on the language pairs:\n   - English-Czech (en-cs, 52M examples)\n   - English-German (en-de, 5.9M examples)\n   - English-Finish (en-fi, 2.6M examples)\n   - English-Turkish (en-tr, 207,373 examples)\n\nThese datasets were part of the WMT'17 translation task, and preprocessed datasets for the WMT'17 task were utilized in the experiments.",
          "relevant_chunks": [
            "memory matrix. To investigate the trade-off between\nspeed and performance, we compare our technique\nto standard models with and without attention on a\nSequence Copy Task of varying length like in Graves\net al. (2014). We generated 4 training datasets of\n100,000 examples and a validation dataset of 1,000 ex-\namples. The vocabulary size was 20. For each dataset,\nthe sequences had lengths randomly chosen between 0\nto L, forL\u2208{10,50,100,200}unique to each dataset.\n4.1.1 Training Setup\nAll models are implemented using TensorFlow\nbased on the seq2seq implementation of Britz et al.\n(2017)3 and trained on a single machine with a\nNvidia K40m GPU. We use a 2-layer 256-unit, a\nbidirectional LSTM (Hochreiter and Schmidhuber,\n1997) encoder, a 2-layer 256-unit LSTM decoder,\nand 256-dimensional embeddings. For the attention\nbaseline, we use the standard parametrized attention\n(Bahdanau et al., 2014). Dropout of 0.2 (0.8 keep\nprobability) is applied to the input of each cell and\nwe optimize using Adam (Kingma and Ba, 2014) at\na learning rate of 0.0001 and batch size 128. We train\nfor at most 200,000 steps (see Figure 3 for sample\nlearning curves). BLEU scores are calculated on\ntokenized data using themulti-bleu.perlscript in\nMoses.4 We decode using beam search with a beam\n3http://github.com/google/seq2seq\n4http://github.com/moses-smt/mosesdecoder\nLength Model BLEU Time (s)\n20 No Att 99.93 2.03\nK=1 99.52 2.12\nK=4 99.56 2.25\nK=16 99.56 2.21\nK=32 99.57 2.59\nK=64 99.75 2.86\nAtt 99.98 2.86\n50 No Att 97.37 3.90\nK=1 98.86 4.33\nK=4 99.95 4.48\nK=16 99.96 4.58\nK=32 99.96 5.35\nK=64 99.97 5.84\nAtt 99.94 6.46\n100 No Att 73.99 6.33\nK=1 87.42 7.32\nK=4 99.81 7.47\nK=16 99.97 7.50\nK=32 99.99 7.65\nK=64 100.00 7.77\nAtt 100.00 11.00\n200 No Att 32.64 9.10\nK=1 44.22 9.30\nK=4 98.54 9.49\nK=16 99.98 9.53\nK=32 100.00 9.59\nK=64 100.00 9.78\nAtt 100.00 14.28\nTable 1: BLEU scores and computation times with\nvaryingKand sequence length compared to baseline\nmodels with and without attention.\nsize of 10 (Wiseman and Rush, 2016).\n4.1.2 Results\nTable 1 shows the BLEU scores of our model on differ-\nent sequence lengths while varyingK. This is a study\nof the trade-off between computational time and rep-\nresentational power. A largeKallows us to compute\n\ncoding time even on a complex dataset with a large\n6http://data.statmt.org/wmt17/translation-task/preprocessed\n\nK\nK\n(a) Training curves for en-fi\nK\nK (b) Training curves for en-tr\nFigure 4: Comparing training curves for en-fi and en-tr with sigmoid encoder scoring and softmax decoder\nscoring and position encoding. Note that en-tr curves converged very quickly.\nModel Dataset K en-cs en-de en-fi en-tr\nMemory Attention Test 32 19.37 28.82 15.87 -\n64 19.65 29.53 16.49 -\nV alid 32 19.20 26.20 15.90 12.94\n64 19.63 26.39 16.35 13.06\nMemory Attention + PETest 32 19.45 29.53 15.86 -\n64 20.36 30.61 17.03 -\nV alid 32 19.35 26.22 16.31 12.97\n64 19.73 27.31 16.91 13.25\nAttention Test - 19.19 30.99 17.34 -\nV alid - 18.61 28.13 17.16 13.76\nTable 2: BLEU scores on WMT\u201917 translation datasets from the memory attention models and regular attention\nbaselines. We picked the best out of the four scoring function combinations on the validation set. Note that\nen-tr does not have an official test set. Best test scores on each dataset are highlighted.\nModel Decoding Time (s)\nK=32 26.85\nK=64 27.13\nAttention 33.28\nTable 3: Decoding time, averaged across 10 runs, for\nthe en-de validation set (2169 examples) with average\nsequence length of 35. Results are similar for both\nPE and non-PE models.\nvocabulary of 16k. We measured decoding time over\nthe full validation set, not including time used for\nmodel setup and data loading, averaged across 10 runs.\nThe average sequence length for examples in this data\nwas 35, and we expect more significant speedups\nfor tasks with longer sequences, as suggested by our\nexperiments on toy data. Note that in our NMT ex-\namples/experiments,K\u2248T, but we obtain computa-\ntional savings from the fact thatK\u226aD. We may be\nable to setK\u226aT, as in toy copying, and still get very\ngood performance in other tasks. For instance, in sum-\nmarization the source is complex but the representa-\ntion of the source required to perform the task is \u201dsim-\nple\u201d (i.e. all that is needed to generate the abstract).\nFigure 5 shows the effect of using sigmoid and\nsoftmax function in the encoders and decoders. We\nfound that softmax/softmax consistently performs\nbadly, while all other combinations perform about\nequally well. We report results for the best combi-\n\nquickly as the attention-based model. Figure 3b shows\nthe effect of varying the encoder and decoder scoring\nfunctions between softmax and sigmoid. All combina-\ntions manage to fit the data, but some converge faster\nthan others. In section 5 we show that distinct align-\nments are learned by different function combinations.\n4.2 Machine Translation\nNext, we explore if the memory-based attention\nmechanism is able to fit complex real-world datasets.\nFor this purpose we use 4 large machine translation\ndatasets of WMT\u2019175 on the following language\npairs: English-Czech (en-cs, 52M examples), English-\nGerman (en-de, 5.9M examples), English-Finish\n(en-fi, 2.6M examples), and English-Turkish (en-tr,\n207,373 examples). We used the newly available pre-\n5statmt.org/wmt17/translation-task.html\nprocessed datasets for the WMT\u201917 task.6 Note that\nour scores may not be directly comparable to other\nwork that performs their own data pre-processing. We\nlearn shared vocabularies of 16,000 subword units\nusing the BPE algorithm (Sennrich et al., 2016).\nWe use newstest2015 as a validation set, and report\nBLEU on newstest2016.\n4.2.1 Training Setup\nWe use a similar setup to the Toy Copy task, but\nuse 512 RNN and embedding units, train using 8\ndistributed workers with 1 GPU each, and train for\nat most 1M steps. We save checkpoints every 30\nminutes during training, and choose the best based\non the validation BLEU score.\n4.2.2 Results\nTable 2 compares our approach with and without\nposition encodings, and with varying values for\nhyperparameterK, to baseline models with regular\nattention mechanism. Learning curves are shown in\nFigure 4. We see that our memory attention model\nwith sufficiently highK performs on-par with, or\nslightly better, than the attention-based baseline model\ndespite its simpler nature. Across the board, models\nwith K = 64performed better than corresponding\nmodels withK= 32, suggesting that using a larger\nnumber of attention vectors can capture a richer under-\nstanding of source sequences. Position encodings also\nseem to consistently improve model performance.\nTable 3 shows that our model results in faster de-\ncoding time even on a complex dataset with a large\n6http://data.statmt.org/wmt17/translation-task/preprocessed\n\nK\nK\n(a) Training curves for en-fi\nK\nK (b) Training curves for en-tr",
            1.0925867500000095
          ]
        }
      ],
      "reference_answers": [
        "standard parametrized attention and a non-attention baseline",
        "Sequence Copy Task and WMT'17"
      ]
    },
    "1706.09673.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.009510956090504261,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.11017543859649122,
          "recall": 0.2579710144927536,
          "f1": 0.13015873015873017
        },
        "rouge-2": {
          "precision": 0.018518518518518517,
          "recall": 0.015151515151515152,
          "f1": 0.016666666666666666
        },
        "rouge-L": {
          "precision": 0.0968421052631579,
          "recall": 0.19130434782608696,
          "f1": 0.10793650793650794
        }
      },
      "per_question_metrics": [
        {
          "question": "Which dataset do they use?",
          "generated_answer": "The context mentions the use of PPDB for noisy phrase pairs learning and the Twitter Paraphrase Corpus for directly training on structural resources from Twitter.",
          "reference_answer": " Paraphrase Database (PPDB) ,  book corpus",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.12,
              "recall": 0.6,
              "f1": 0.19999999999999998
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.08,
              "recall": 0.4,
              "f1": 0.13333333333333333
            }
          },
          "bleu_score": 0.00956240657444202,
          "exact_match": 0.0
        },
        {
          "question": "Do they evaluate their learned representations on downstream tasks?",
          "generated_answer": "Yes, they do evaluate their learned representations on downstream tasks. The context mentions that for transfer learning, models based on word averaging perform well on both the in-domain and out-of-domain textual similarity tasks, and for both sentence similarity and textual entailment tasks, outperforming the LSTM. Additionally, they find that for the sentiment classification task, LSTM (trained on PPDB) beats the averaging models to establish a new state of the art. This indicates that the learned representations are indeed evaluated on various downstream tasks to understand their effectiveness and applicability.",
          "reference_answer": "No No",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            }
          },
          "bleu_score": 0,
          "exact_match": 0.0
        },
        {
          "question": "How do they encourage understanding of literature as part of their objective function?",
          "generated_answer": "The provided context does not discuss or mention encouraging understanding of literature as part of their objective function explicitly.",
          "reference_answer": "They group the existing works in terms of the objective function they optimize - within-tweet relationships, inter-tweet relationships, autoencoder, and weak supervision.",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.21052631578947367,
              "recall": 0.17391304347826086,
              "f1": 0.1904761904761905
            },
            "rouge-2": {
              "precision": 0.05555555555555555,
              "recall": 0.045454545454545456,
              "f1": 0.049999999999999996
            },
            "rouge-L": {
              "precision": 0.21052631578947367,
              "recall": 0.17391304347826086,
              "f1": 0.1904761904761905
            }
          },
          "bleu_score": 0.018970461697070763,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "Which dataset do they use?",
        "Do they evaluate their learned representations on downstream tasks?",
        "How do they encourage understanding of literature as part of their objective function?"
      ],
      "generated_answers": [
        {
          "answer": "The context mentions the use of PPDB for noisy phrase pairs learning and the Twitter Paraphrase Corpus for directly training on structural resources from Twitter.",
          "relevant_chunks": [
            "putational Linguistics. pages 69\u201378.\nJ Ganesh, Manish Gupta, and Vasudeva Varma. 2016.\nInterpreting the syntactic and social elements of the\ntweet representations via elementary property pre-\ndiction tasks. CoRR abs/1611.04887.\nJ Ganesh, Manish Gupta, and Vasudeva Varma. 2017.\nImproving tweet representations using temporal and\nuser context. In Advances in Information Retrieval\n- 39th European Conference on IR Research, ECIR.\npages 575\u2013581.\nJuri Ganitkevitch, Benjamin Van Durme, and Chris\nCallison-Burch. 2013. PPDB: the paraphrase\ndatabase. In Proc. of conf. of the North American\nChapter of the Association of Computational Lin-\nguistics. pages 758\u2013764.\nAnastasia Giachanou and Fabio Crestani. 2016. Like\nit or not: A Survey of Twitter Sentiment Anal-\nysis Methods. ACM Computing Surveys (CSUR)\n49(2):28.\nZellig S Harris. 1954. Distributional Structure. Word\n10(2-3):146\u2013162.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning Distributed Representations of Sentences\nfrom Unlabelled Data. In Proc. of the Conf. of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies. pages 1367\u20131377.\nPo-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\nAlex Acero, and Larry P. Heck. 2013. Learning\nDeep Structured Semantic Models for Web Search\nusing Clickthrough Data. In Proc. of the 22nd ACM\nIntl. Conf. on Information and Knowledge Manage-\nment. pages 2333\u20132338.\nAndrej Karpathy, Justin Johnson, and Fei-Fei Li. 2015.\nVisualizing and understanding recurrent networks.\nCoRR abs/1506.02078.\nTom Kenter, Alexey Borisov, and Maarten de Rijke.\n2016. Siamese CBOW: Optimizing Word Embed-\ndings for Sentence Representations. In Proc. of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics.\nDouwe Kiela, Felix Hill, Anna Korhonen, and Stephen\nClark. 2014. Improving multi-modal representa-\ntions using image dispersion: Why less is some-\ntimes more. In Proc. of the 52nd Annual Meet-\ning of the Association for Computational Linguis-\ntics. pages 835\u2013841.\nYoon Kim. 2014. Convolutional Neural Networks for\nSentence Classi\ufb01cation. In Proc. of the 2014 Conf.\non Empirical Methods in Natural Language Pro-\ncessing. pages 1746\u20131751.\nRyan Kiros, Yukun Zhu, Ruslan Salakhutdinov,\nRichard S. Zemel, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. 2015. Skip-Thought Vec-\n\ncomplex architectures such as CNN, LSTM\nand so on for several downstream applica-\ntions (Wieting et al., 2016a,b). We believe\na theoretical understanding of why word av-\neraging models perform well can help in em-\nbracing these models by linguists.\n\u2022 Models in (Wieting et al., 2016a,b) learn\nfrom noisy phrase pairs of PPDB. Note that\nthe source of the underlying texts is com-\npletely different from Twitter. It can be inter-\nesting to see the effectiveness of such mod-\nels when directly trained on structural re-\nsources from Twitter like Twitter Paraphrase\nCorpus (Xu et al., 2014). The main challenge\nwith this approach is the small size of the an-\nnotated Twitter resources, which can encour-\nage models like (Arora et al., 2017) that work\nwell even when the training data is scarce or\nnonexistent.\n\u2022 Tweets mostly have an accompanying im-\nage which sometimes has visual correspon-\ndence with its textual content (Chen et al.,\n1https://www.microsoft.com/en-us/\nresearch/project/dssm/\n2http://trec.nist.gov/\n2013; Wang et al., 2014) (\u2018visual\u2019 tweet).\nTo the best of our knowledge, there is no\nwork which explores the following question:\ncan we build multimodal representations for\ntweets accompanying correlated visual con-\ntent and compare with traditional bench-\nmarks?. We can leverage insights from mul-\ntimodal skip-gram model (Lazaridou et al.,\n2015) which builds multimodally-enhanced\nword vectors that perform well in the tradi-\ntional semantic benchmarks. However, it\u2019s\nhard to detect visual tweets and learning from\na non-visual tweet can degrade its tweet rep-\nresentation. It would be interesting to see if\na dispersion metric (Kiela et al., 2014) for\ntweets can be explored to overcome this prob-\nlem of building a nondegradable, improved\ntweet representation.\n\u2022 Interpreting the tweet representations to un-\nearth the encoded features responsible for\nits performance on a downstream task is an\nimportant, but a less studied research area.\n(Ganesh et al., 2016)\u2019s work is the \ufb01rst to\nopen the blackbox of vector embeddings for\ntweets. They propose elementary property\nprediction tasks which predicts the accuracy\nto which a given tweet representation en-\ncodes the elementary property (like slang\nwords, hashtags, mentions, etc). The main\ndrawback of the work is that they fail to cor-\n\nprediction tasks which predicts the accuracy\nto which a given tweet representation en-\ncodes the elementary property (like slang\nwords, hashtags, mentions, etc). The main\ndrawback of the work is that they fail to cor-\nrelate their study with downstream applica-\ntions. We believe performing such a correla-\ntion study can clearly highlight the set of el-\nementary features behind the performance of\na particular representation model over other\nfor a given downstream task.\n4 Conclusion\nIn this work we study the problem of learning\nunsupervised tweet representations. We believe\nour survey of the existing works based on the ob-\njective function can give vital perspectives to re-\nsearchers and aid their understanding of the \ufb01eld.\nWe also believe the future research directions stud-\nied in this work can help in breaking the barriers in\nbuilding high quality, general purpose tweet repre-\nsentation models.\n\nReferences\nSanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.\nA simple but tough-to-beat baseline for sentence em-\nbeddings. CoRR .\nYoshua Bengio. 2013a. Deep Learning of Represen-\ntations: Looking Forward. In Proc. of the 1st Intl.\nConf. on Statistical Language and Speech Process-\ning. pages 1\u201337.\nYoshua Bengio, Aaron C. Courville, and Pascal Vin-\ncent. 2013b. Representation Learning: A Review\nand New Perspectives. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence 35(8):1798\u2013\n1828.\nDavid M. Blei, Andrew Y . Ng, and Michael I. Jordan.\n2003. Latent Dirichlet Allocation. Journal of Ma-\nchine Learning Research 3:993\u20131022.\nTao Chen, Dongyuan Lu, Min-Yen Kan, and Peng Cui.\n2013. Understanding and classifying image tweets.\nIn ACM Multimedia Conference, MM \u201913 . pages\n781\u2013784.\nBhuwan Dhingra, Zhong Zhou, Dylan Fitzpatrick,\nMichael Muehl, and William W. Cohen. 2016.\nTweet2Vec: Character-Based Distributed Represen-\ntations for Social Media. In Proc. of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics.\nC\u00b4\u0131cero Nogueira dos Santos and Maira Gatti. 2014.\nDeep convolutional neural networks for sentiment\nanalysis of short texts. In 25th Intl. Conf. on Com-\nputational Linguistics. pages 69\u201378.\nJ Ganesh, Manish Gupta, and Vasudeva Varma. 2016.\nInterpreting the syntactic and social elements of the\ntweet representations via elementary property pre-\ndiction tasks. CoRR abs/1611.04887.",
            0.37898708400001624
          ]
        },
        {
          "answer": "Yes, they do evaluate their learned representations on downstream tasks. The context mentions that for transfer learning, models based on word averaging perform well on both the in-domain and out-of-domain textual similarity tasks, and for both sentence similarity and textual entailment tasks, outperforming the LSTM. Additionally, they find that for the sentiment classification task, LSTM (trained on PPDB) beats the averaging models to establish a new state of the art. This indicates that the learned representations are indeed evaluated on various downstream tasks to understand their effectiveness and applicability.",
          "relevant_chunks": [
            "complex architectures such as CNN, LSTM\nand so on for several downstream applica-\ntions (Wieting et al., 2016a,b). We believe\na theoretical understanding of why word av-\neraging models perform well can help in em-\nbracing these models by linguists.\n\u2022 Models in (Wieting et al., 2016a,b) learn\nfrom noisy phrase pairs of PPDB. Note that\nthe source of the underlying texts is com-\npletely different from Twitter. It can be inter-\nesting to see the effectiveness of such mod-\nels when directly trained on structural re-\nsources from Twitter like Twitter Paraphrase\nCorpus (Xu et al., 2014). The main challenge\nwith this approach is the small size of the an-\nnotated Twitter resources, which can encour-\nage models like (Arora et al., 2017) that work\nwell even when the training data is scarce or\nnonexistent.\n\u2022 Tweets mostly have an accompanying im-\nage which sometimes has visual correspon-\ndence with its textual content (Chen et al.,\n1https://www.microsoft.com/en-us/\nresearch/project/dssm/\n2http://trec.nist.gov/\n2013; Wang et al., 2014) (\u2018visual\u2019 tweet).\nTo the best of our knowledge, there is no\nwork which explores the following question:\ncan we build multimodal representations for\ntweets accompanying correlated visual con-\ntent and compare with traditional bench-\nmarks?. We can leverage insights from mul-\ntimodal skip-gram model (Lazaridou et al.,\n2015) which builds multimodally-enhanced\nword vectors that perform well in the tradi-\ntional semantic benchmarks. However, it\u2019s\nhard to detect visual tweets and learning from\na non-visual tweet can degrade its tweet rep-\nresentation. It would be interesting to see if\na dispersion metric (Kiela et al., 2014) for\ntweets can be explored to overcome this prob-\nlem of building a nondegradable, improved\ntweet representation.\n\u2022 Interpreting the tweet representations to un-\nearth the encoded features responsible for\nits performance on a downstream task is an\nimportant, but a less studied research area.\n(Ganesh et al., 2016)\u2019s work is the \ufb01rst to\nopen the blackbox of vector embeddings for\ntweets. They propose elementary property\nprediction tasks which predicts the accuracy\nto which a given tweet representation en-\ncodes the elementary property (like slang\nwords, hashtags, mentions, etc). The main\ndrawback of the work is that they fail to cor-\n\nprediction tasks which predicts the accuracy\nto which a given tweet representation en-\ncodes the elementary property (like slang\nwords, hashtags, mentions, etc). The main\ndrawback of the work is that they fail to cor-\nrelate their study with downstream applica-\ntions. We believe performing such a correla-\ntion study can clearly highlight the set of el-\nementary features behind the performance of\na particular representation model over other\nfor a given downstream task.\n4 Conclusion\nIn this work we study the problem of learning\nunsupervised tweet representations. We believe\nour survey of the existing works based on the ob-\njective function can give vital perspectives to re-\nsearchers and aid their understanding of the \ufb01eld.\nWe also believe the future research directions stud-\nied in this work can help in breaking the barriers in\nbuilding high quality, general purpose tweet repre-\nsentation models.\n\nReferences\nSanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.\nA simple but tough-to-beat baseline for sentence em-\nbeddings. CoRR .\nYoshua Bengio. 2013a. Deep Learning of Represen-\ntations: Looking Forward. In Proc. of the 1st Intl.\nConf. on Statistical Language and Speech Process-\ning. pages 1\u201337.\nYoshua Bengio, Aaron C. Courville, and Pascal Vin-\ncent. 2013b. Representation Learning: A Review\nand New Perspectives. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence 35(8):1798\u2013\n1828.\nDavid M. Blei, Andrew Y . Ng, and Michael I. Jordan.\n2003. Latent Dirichlet Allocation. Journal of Ma-\nchine Learning Research 3:993\u20131022.\nTao Chen, Dongyuan Lu, Min-Yen Kan, and Peng Cui.\n2013. Understanding and classifying image tweets.\nIn ACM Multimedia Conference, MM \u201913 . pages\n781\u2013784.\nBhuwan Dhingra, Zhong Zhou, Dylan Fitzpatrick,\nMichael Muehl, and William W. Cohen. 2016.\nTweet2Vec: Character-Based Distributed Represen-\ntations for Social Media. In Proc. of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics.\nC\u00b4\u0131cero Nogueira dos Santos and Maira Gatti. 2014.\nDeep convolutional neural networks for sentiment\nanalysis of short texts. In 25th Intl. Conf. on Com-\nputational Linguistics. pages 69\u201378.\nJ Ganesh, Manish Gupta, and Vasudeva Varma. 2016.\nInterpreting the syntactic and social elements of the\ntweet representations via elementary property pre-\ndiction tasks. CoRR abs/1611.04887.\n\nsentence in the corpus. For FastSent and Siamese\nCBOW, the test sentence representation is a sim-\nple average of word vectors obtained after train-\ning. Both of these models are general purpose\nsentence representation models trained on book\ncorpus, yet give a competitive performance over\nprevious models on the tweet semantic similarity\ncomputation task. (Ganesh et al., 2017)\u2019s model\nattempt to exploit these signals directly from Twit-\nter. With the help of attention technique and\nlearned user representation, this log-linear model\nis able to capture salient semantic information\nfrom chronologically adjacent tweets of a target\ntweet in users\u2019 Twitter timeline.\n2.3 Modeling from structured resources\nMotivation: In recent times, building represen-\ntation models based on supervision from richly\nstructured resources such as Paraphrase Database\n(PPDB) (Ganitkevitch et al., 2013) (containing\nnoisy phrase pairs) has yielded high quality sen-\ntence representations. These methods work by\nmaximizing the similarity of the sentences in the\nlearned semantic space.\nModels: CHARAGRAM (Wieting et al., 2016a)\nembeds textual sequences by learning a character-\nbased compositional model that involves addition\nof the vectors of its character n-grams followed by\nan elementwise nonlinearity. This simpler archi-\ntecture trained on PPDB is able to beat models\nwith complex architectures like CNN, LSTM on\nSemEval 2015 Twitter textual similarity task by a\nlarge margin. This result emphasizes the impor-\ntance of character-level models that address differ-\nences due to spelling variation and word choice.\nThe authors in their subsequent work (Wieting\net al., 2016b) conduct a comprehensive analysis\nof models spanning the range of complexity from\nword averaging to LSTMs for its ability to do\ntransfer and supervised learning after optimizing a\nmargin based loss on PPDB. For transfer learning,\nthey \ufb01nd models based on word averaging perform\nwell on both the in-domain and out-of-domain tex-\ntual similarity tasks, beating LSTM model by a\nlarge margin. On the other hand, the word averag-\ning models perform well for both sentence similar-\nity and textual entailment tasks, outperforming the\nLSTM. However, for sentiment classi\ufb01cation task,\nthey \ufb01nd LSTM (trained on PPDB) to beat the av-\neraging models to establish a new state of the art.\nThe above results suggest that structured resources\nplay a vital role in computing general-purpose em-\nbeddings useful in downstream applications.\n2.4 Modeling as an autoencoder",
            1.1155983330000083
          ]
        },
        {
          "answer": "The provided context does not discuss or mention encouraging understanding of literature as part of their objective function explicitly.",
          "relevant_chunks": [
            "complex architectures such as CNN, LSTM\nand so on for several downstream applica-\ntions (Wieting et al., 2016a,b). We believe\na theoretical understanding of why word av-\neraging models perform well can help in em-\nbracing these models by linguists.\n\u2022 Models in (Wieting et al., 2016a,b) learn\nfrom noisy phrase pairs of PPDB. Note that\nthe source of the underlying texts is com-\npletely different from Twitter. It can be inter-\nesting to see the effectiveness of such mod-\nels when directly trained on structural re-\nsources from Twitter like Twitter Paraphrase\nCorpus (Xu et al., 2014). The main challenge\nwith this approach is the small size of the an-\nnotated Twitter resources, which can encour-\nage models like (Arora et al., 2017) that work\nwell even when the training data is scarce or\nnonexistent.\n\u2022 Tweets mostly have an accompanying im-\nage which sometimes has visual correspon-\ndence with its textual content (Chen et al.,\n1https://www.microsoft.com/en-us/\nresearch/project/dssm/\n2http://trec.nist.gov/\n2013; Wang et al., 2014) (\u2018visual\u2019 tweet).\nTo the best of our knowledge, there is no\nwork which explores the following question:\ncan we build multimodal representations for\ntweets accompanying correlated visual con-\ntent and compare with traditional bench-\nmarks?. We can leverage insights from mul-\ntimodal skip-gram model (Lazaridou et al.,\n2015) which builds multimodally-enhanced\nword vectors that perform well in the tradi-\ntional semantic benchmarks. However, it\u2019s\nhard to detect visual tweets and learning from\na non-visual tweet can degrade its tweet rep-\nresentation. It would be interesting to see if\na dispersion metric (Kiela et al., 2014) for\ntweets can be explored to overcome this prob-\nlem of building a nondegradable, improved\ntweet representation.\n\u2022 Interpreting the tweet representations to un-\nearth the encoded features responsible for\nits performance on a downstream task is an\nimportant, but a less studied research area.\n(Ganesh et al., 2016)\u2019s work is the \ufb01rst to\nopen the blackbox of vector embeddings for\ntweets. They propose elementary property\nprediction tasks which predicts the accuracy\nto which a given tweet representation en-\ncodes the elementary property (like slang\nwords, hashtags, mentions, etc). The main\ndrawback of the work is that they fail to cor-\n\ntion model on Twitter-speci\ufb01c contexts such\nas conversation and topical streams. We be-\nlieve such a solution directly exploits seman-\ntic signals (or nuances) from Twitter, unlike\nSTV or Siamese CBOW which are trained on\nbooks corpus.\n\u2022 (dos Santos and Gatti, 2014) propose a super-\nvised, hybrid model exploiting both the char-\nacter and word level information for Twit-\nter sentiment analysis task. Since the set-\ntings when the character level model beats the\nword level model is not well understood yet,\nwe believe it would be interesting to explore\nsuch a hybrid compositional model to build\nunsupervised tweet representations.\n\u2022 Twitter provides a platform for the users to\ninteract with other users. To the best of our\nknowledge, there is no related work that com-\nputes unsupervised tweet representation by\nexploiting the user pro\ufb01le attributes like pro-\n\ufb01le picture, user biography and set of fol-\nlowers, and social interactions like retweet\ncontext (set of surrounding tweets in a users\nretweet stream) and favorite context (set of\nsurrounding tweets in a users favorite tweet\nstream).\n\n\u2022 DSSM (Huang et al., 2013; Shen et al., 2014)\npropose a family of deep models that are\ntrained to maximize the relevance of clicked\ndocuments given a query. Such a ranking loss\nfunction helps the model cater to a wide vari-\nety of applications1 such as web search rank-\ning, ad selection/relevance, question answer-\ning, knowledge inference and machine trans-\nlation. We observe such a loss function has\nnot been explored for building unsupervised\ntweet representations. We believe employing\na ranking loss directly on tweets using a large\nscale microblog dataset 2 can result in repre-\nsentations which can be useful to Twitter ap-\nplications beyond those studied in the tweet\nrepresentation learning literature.\n\u2022 Linguists assume that language is best un-\nderstood as a hierarchical tree of phrases,\nrather than a \ufb02at sequence of words or charac-\nters. It\u2019s dif\ufb01cult to get the syntactic trees for\ntweets as most of them are not grammatically\ncorrect. The average of word vectors model\nhas the most simplest compositional archi-\ntecture with no additional parameters, yet\ndisplays a strong performance outperforming\ncomplex architectures such as CNN, LSTM\nand so on for several downstream applica-\ntions (Wieting et al., 2016a,b). We believe\na theoretical understanding of why word av-\neraging models perform well can help in em-\n\nputational Linguistics. pages 69\u201378.\nJ Ganesh, Manish Gupta, and Vasudeva Varma. 2016.\nInterpreting the syntactic and social elements of the\ntweet representations via elementary property pre-\ndiction tasks. CoRR abs/1611.04887.\nJ Ganesh, Manish Gupta, and Vasudeva Varma. 2017.\nImproving tweet representations using temporal and\nuser context. In Advances in Information Retrieval\n- 39th European Conference on IR Research, ECIR.\npages 575\u2013581.\nJuri Ganitkevitch, Benjamin Van Durme, and Chris\nCallison-Burch. 2013. PPDB: the paraphrase\ndatabase. In Proc. of conf. of the North American\nChapter of the Association of Computational Lin-\nguistics. pages 758\u2013764.\nAnastasia Giachanou and Fabio Crestani. 2016. Like\nit or not: A Survey of Twitter Sentiment Anal-\nysis Methods. ACM Computing Surveys (CSUR)\n49(2):28.\nZellig S Harris. 1954. Distributional Structure. Word\n10(2-3):146\u2013162.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning Distributed Representations of Sentences\nfrom Unlabelled Data. In Proc. of the Conf. of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies. pages 1367\u20131377.\nPo-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\nAlex Acero, and Larry P. Heck. 2013. Learning\nDeep Structured Semantic Models for Web Search\nusing Clickthrough Data. In Proc. of the 22nd ACM\nIntl. Conf. on Information and Knowledge Manage-\nment. pages 2333\u20132338.\nAndrej Karpathy, Justin Johnson, and Fei-Fei Li. 2015.\nVisualizing and understanding recurrent networks.\nCoRR abs/1506.02078.\nTom Kenter, Alexey Borisov, and Maarten de Rijke.\n2016. Siamese CBOW: Optimizing Word Embed-\ndings for Sentence Representations. In Proc. of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics.\nDouwe Kiela, Felix Hill, Anna Korhonen, and Stephen\nClark. 2014. Improving multi-modal representa-\ntions using image dispersion: Why less is some-\ntimes more. In Proc. of the 52nd Annual Meet-\ning of the Association for Computational Linguis-\ntics. pages 835\u2013841.\nYoon Kim. 2014. Convolutional Neural Networks for\nSentence Classi\ufb01cation. In Proc. of the 2014 Conf.\non Empirical Methods in Natural Language Pro-\ncessing. pages 1746\u20131751.\nRyan Kiros, Yukun Zhu, Ruslan Salakhutdinov,\nRichard S. Zemel, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. 2015. Skip-Thought Vec-",
            0.9795578330000012
          ]
        }
      ],
      "reference_answers": [
        " Paraphrase Database (PPDB) ,  book corpus",
        "No No",
        "They group the existing works in terms of the objective function they optimize - within-tweet relationships, inter-tweet relationships, autoencoder, and weak supervision."
      ]
    },
    "1706.08198.pdf": {
      "average_exact_match": 0.0,
      "average_bleu_score": 0.19941029595532586,
      "average_rouge_scores": {
        "rouge-1": {
          "precision": 0.40625,
          "recall": 0.5119047619047619,
          "f1": 0.39729729729729735
        },
        "rouge-2": {
          "precision": 0.26666666666666666,
          "recall": 0.2,
          "f1": 0.2285714285714286
        },
        "rouge-L": {
          "precision": 0.375,
          "recall": 0.4880952380952381,
          "f1": 0.37027027027027026
        }
      },
      "per_question_metrics": [
        {
          "question": "Is pre-training effective in their evaluation?",
          "generated_answer": "Yes, pre-training is effective in their evaluation.",
          "reference_answer": "Yes Yes",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.125,
              "recall": 0.5,
              "f1": 0.2
            },
            "rouge-2": {
              "precision": 0.0,
              "recall": 0.0,
              "f1": 0.0
            },
            "rouge-L": {
              "precision": 0.125,
              "recall": 0.5,
              "f1": 0.2
            }
          },
          "bleu_score": 0.023980296761827107,
          "exact_match": 0.0
        },
        {
          "question": "What parallel corpus did they use?",
          "generated_answer": "They used two parallel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) and NTCIR PatentMT Parallel Corpus.",
          "reference_answer": "Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0, NTCIR PatentMT Parallel Corpus BIBREF1 Asian Scientific Paper Excerpt Corpus, NTCIR PatentMT Parallel Corpus ",
          "rouge_scores": {
            "rouge-1": {
              "precision": 0.6875,
              "recall": 0.5238095238095238,
              "f1": 0.5945945945945946
            },
            "rouge-2": {
              "precision": 0.5333333333333333,
              "recall": 0.4,
              "f1": 0.4571428571428572
            },
            "rouge-L": {
              "precision": 0.625,
              "recall": 0.47619047619047616,
              "f1": 0.5405405405405405
            }
          },
          "bleu_score": 0.3748402951488246,
          "exact_match": 0.0
        }
      ],
      "questions": [
        "Is pre-training effective in their evaluation?",
        "What parallel corpus did they use?"
      ],
      "generated_answers": [
        {
          "answer": "Yes, pre-training is effective in their evaluation.",
          "relevant_chunks": [
            "Corpus Model English-Japanese Japanese-English\n(i) (ii) (iii) (i) (ii) (iii)\nBaseline-NMT 1,141 378 1,045 951 494 1,085\nASPEC +Reconstructor 988 336 1,042 836 418 1,014\n+Reconstructor (Jointly-Training) 1,292 446 1,147 1,106 525 1,821\nBaseline-NMT 2,122 1,015 1,106 2,521 1,073 1,630\nNTCIR +Reconstructor 1,958 922 963 2,187 987 1,422\n+Reconstructor (Jointly-Training) 1,978 916 1,078 2,475 1,107 1,610\nTable 5: Numbers of redundant and unknown word tokens.\nof encoder-decoder-reconstructor almost correctly\nattends input words.\nTable 5 shows a comparison of the number of\nword occurrences for each corpus and model. The\ncolumns show (i) the number of words that ap-\npear more frequently than the counterparts in the\nreference, and (ii) the number of words that ap-\npear more than once but are not included in the\nreference. Note that these numbers do not in-\nclude unknown words, so (iii) shows the num-\nber of unknown words. In all the cases, the\nnumber of occurrence of redundant words is re-\nduced in encoder-decoder-reconstructor. Thus,\nwe con\ufb01rmed that encoder-decoder-reconstructor\nachieves reduction of repeating and missing words\nwhile maintaining the quality of translation.\n6 Conclusion\nIn this paper, we evaluated the encoder-decoder-\nreconstructor on English-Japanese and Japanese-\nEnglish translation tasks. In addition, we evaluate\nthe effectiveness of pre-training by comparing it\nwith a jointly-trained model of forward translation\nand back-translation. Experimental results show\nthat the encoder-decoder-reconstructor offers sig-\nni\ufb01cant improvement in BLEU scores and allevi-\nates the problem of repeating and missing words\nin the translation on English-Japanese translation\ntask, and the encoder-decoder-reconstructor can\nnot be trained well without pre-training, so it\nproves that we have to train the forward transla-\ntion model in a manner similar to the conventional\nattention-based NMT as pre-training.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural Machine Translation by Jointly\nLearning to Align and Translate. Proceedings of\nthe 3rd International Conference on Learning Rep-\nresentations (ICLR), pages 1\u201315.\nKyunghyun Cho, Bart Van Merri \u00a8enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. 2014. Learn-\n\nand missing words in the translation on English-\nJapanese translation task, though the difference\nis not signi\ufb01cant on Japanese-English translation\ntask.\nIn addition, we jointly train a model of for-\nward translation and back-translation without pre-\ntraining, and then evaluate this model. As a re-\nsult, the encoder-decoder-reconstructor can not be\ntrained well without pre-training, so it proves that\nwe have to train the forward translation model in a\nmanner similar to the conventional attention-based\nNMT as pre-training.\nThe main contributions of this paper are as fol-\nlows:\n\u2022Experimental results show that encode-\ndecoder-reconstructor framework achieves\nsigni\ufb01cant improvements in BLEU scores\n(1.0-1.4) for English-Japanese translation\ntask.\n\u2022Experimental results show that encode-\ndecoder-reconstructor framework has to train\nthe forward translation model in a manner\nsimilar to the conventional attention-based\nNMT as pre-training.\n2 Related Works\nSeveral studies have addressed the NMT-speci\ufb01c\nproblem of missing or repeating words. Niehues\net al. (2016) optimized NMT by adding the out-\nputs of PBSMT to the input of NMT. Mi et al.\n(2016) and Feng et al. (2016) introduced a dis-\ntributed version of coverage vector taken from PB-\nSMT to consider which words have been already\ntranslated. All these methods, including ours, em-\nploy information of the source sentence to im-\nprove the quality of translation, but our method\nuses back-translation to ensure that there is no in-\nconsistency. Unlike other methods, once learned,\nour method is identical to the conventional NMT\nmodel, so it does not need any additional parame-\nters such as coverage vector or a PBSMT system\nfor testing.\nThe attention mechanism proposed by Meng\net al. (2016) considers not only the hidden states\nof the encoder but also the hidden states of the\ndecoder so that over-translation can be relaxed.\nIn addition, the attention mechanism proposed by\nFeng et al. (2016) computes a context vector by\nconsidering the previous context vector to pre-\nvent over-translation. These works indirectly re-\nduce repeating and missing words, while we di-\nrectly penalize translation mismatch by consider-\ning back-translation.\nThe encoder-decoder-reconstructor framework\nfor NMT proposed by Tu et al. (2017) optimizes\nNMT by reconstructor using back-translation.\n\n2017) signi\ufb01cantly improves translation accuracy\nby 1.01 points on ASPEC and 1.37 points on NT-\nCIR in English-Japanese translation ( p <0.05).\nHowever, it does not signi\ufb01cantly improve trans-\nlation accuracy in Japanese-English translation. In\naddition, it is proved that the encoder-decoder-\nreconstructor without pre-training worsens rather\n\nBaseline-NMT\n Encoder-Decoder-Reconstructor\nFigure 3: The attention layer in Example 1 : Improvement in under-translation.\nBaseline-NMT\n Encoder-Decoder-Reconstructor\nFigure 4: The attention layer in Example 2 : Improvement in over-translation.\nthan improves translation accuracy.\nTable 4 shows examples of outputs of English-\nJapanese translations. In Example 1, \u201c \u4e71 \u6d41\u7c98\u6027\n\u3068\u6570\u5024\u7c98\u6027\u306e\u5927\u5c0f\u95a2\u4fc2\u306b\u3088\u308a,\u201d (on the ba-\nsis of the relation between turbulent viscosity and\nnumerical viscosity in size) is missing in the out-\nput of baseline-NMT, but \u201c \u4e71 \u6d41\u7c98\u6027 \u3068\u6570\u5024\n\u7684 \u7c98\u6027 \u306e\u95a2\u4fc2 \u3092\u57fa \u306b\u201d (on the basis of the\nrelation between turbulent viscosity and numeri-\ncal viscosity) is present in the output of encoder-\ndecoder-reconstructor. In Example 2, \u201c \u65b0\u751f\u5150\u201d\n(newborn infant) and \u201c30\u6b73\u4ee5\u4e0a\u306e\u201d (of 30 - year -\nold or more) are repeated in the output of baseline-\nNMT, but they appear only once in the output of\nencoder-decoder-reconstructor.\nIn addition, Figures 3 and 4 show the atten-\ntion layer on baseline-NMT and encoder-decoder-\nreconstructor in each example. In Figure 3, al-\nthough the attention layer of baseline NMT attends\ninput word \u201cturbulent\u201d, the decoder does not out-\nput \u201c\u4e71\u6d41\u201d (turbulent) but \u201c\u691c\u8a0e\u201d (examined) at\nthe 13th word. Thus, under-translation may be\nresulted from the hidden layer or the embedding\nlayer instead of the attention layer. In Figure 4, it\nis found that the attention layer of baseline-NMT\nrepeatedly attends input words \u201cnewborn infant\u201d\nand \u201c30 - year - old or more\u201d. Consequently,\nthe decoder repeatedly outputs \u201c \u65b0\u751f\u5150\u201d (new-\nborn infant) and \u201c30\u6b73\u4ee5\u4e0a\u306e\u201d (of 30 - year - old\nor more). On the other hand, the attention layer\n\nCorpus Model English-Japanese Japanese-English\n(i) (ii) (iii) (i) (ii) (iii)\nBaseline-NMT 1,141 378 1,045 951 494 1,085",
            0.4043573330000072
          ]
        },
        {
          "answer": "They used two parallel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) and NTCIR PatentMT Parallel Corpus.",
          "relevant_chunks": [
            "and its weight \u03b1ij is a normalized probability dis-\ntribution. It is computed by\n\u03b1ij = exp(eij)\n\u2211|x|\nk=1 exp(eik)\n(8)\nand\neij = v\u22a4\na tanh(Wasi\u22121 + Uahj) (9)\nwhere va is a weight vector and Wa and Ua are\nweight matrices.\nASPEC NTCIR\ntrain 827,188 1,169,201\ndev 1,504 2,741\ntest 1,556 2,300\nTable 1: Numbers of parallel sentences.\nThe objective function is de\ufb01ned by\nL(\u03b8) = 1\nN\n\u2211N\nn=1\n|y|\u2211\ni=1\nlog p(\u02c6y(n)\ni |y(n)\n<i ,x(n),\u03b8)\n(10)\nwhere N is the number of data and \u03b8 is a model\nparameter.\nIncidentally, as a nonlinear function, the hyper-\nbolic tangent function or the recti\ufb01ed linear unit\nare generally used.\n4 Encoder-Decoder-Reconstructor\n4.1 Architecture\nNext, we describe the encoder-decoder-\nreconstructor framework for NMT proposed\nby Tu et al. (2017) as shown in Figure 2. The\nencoder-decoder-reconstructor consists of two\ncomponents: the standard encoder-decoder as\nan attention-based NMT proposed by Bahdanau\net al. (2015) and the reconstructor which back-\ntranslates from the hidden states of decoder to the\nsource sentence.\nIn their method, the hidden state of the decoder\nis back-translated into the source sequence (x) by\nthe reconstructor for the back-translation. At each\ntime step i, the conditional probability of the out-\nput word \u02c6xi is computed by\np(\u02c6xi|x<i,\u02c6y) =f\u2032(s\u2032\ni,xi\u22121,c\u2032\ni) (11)\nwhere f\u2032is a nonlinear function. The hidden state\ns\u2032\ni of the reconstructor is presented as\ns\u2032\ni = g\u2032(s\u2032\ni\u22121,xi\u22121,c\u2032\ni) (12)\nusing the hidden state s\u2032\ni\u22121 and the source word\nxi\u22121 at the previous time step and the new context\nvector (inverse context vector) c\u2032\ni.\nThe inverse context vector c\u2032\ni is a weighted sum\nof each hidden state sj of the decoder (on forward\ntranslation). It is presented as\nc\u2032\ni =\n|y|\u2211\nj=1\n\u03b1\u2032\nijsj (13)\n\nEnglish-Japanese\nCorpus Model BLEU p-value Hours\nBaseline-NMT 29.75 - 99\nASPEC +Reconstructor 30.76 0.00 149\n+Reconstructor (Jointly-Training) 26.04 - 174\nBaseline-NMT 30.03 - 116\nNTCIR +Reconstructor 31.40 0.00 166\n\n\u308f\u305a\u304b\u3067\u3042\u3063\u305f \u3002\n+Reconstructor \u82e5\u9f62\u306e\u65b0 \u65b0 \u65b0\u751f \u751f \u751f\u5150 \u5150 \u5150\u3067\u306f\u6d3b\u6027\u306f\u9ad8\u304b\u3063\u305f\u304c , 30 \u6b73 \u6b73 \u6b73\u4ee5 \u4ee5 \u4ee5\u4e0a \u4e0a \u4e0a\u306e \u306e \u306e\u5834\u5408\u306b\u306f \u6975\u3081\u3066\n(Jointly-Training) \u8efd\u5ea6\u3067\u3042\u3063\u305f \u3002\nReference \u6d3b\u6027\u306f\u82e5\u3044\u500b\u4f53, \u7279\u306b\u65b0 \u65b0 \u65b0\u751f \u751f \u751f\u5150 \u5150 \u5150\u306e\u7d30\u80de\u3067\u9ad8\u304f, 30 \u6b73 \u6b73 \u6b73\u4ee5 \u4ee5 \u4ee5\u4e0a \u4e0a \u4e0a\u306e \u306e \u306e\u3082\u306e\u3067\u306f\n\u3054\u304f\u308f\u305a\u304b\u3067\u3042\u3063\u305f \u3002\nTable 4: Examples of outputs of English-Japanese translation.\n5.1 Datasets\nWe used two parallel corpora: Asian Scien-\nti\ufb01c Paper Excerpt Corpus (ASPEC) (Nakazawa\net al., 2016) and NTCIR PatentMT Parallel Corpus\n(Goto et al., 2013). Regarding the training data of\nASPEC, we used only the \ufb01rst 1 million sentences\nsorted by sentence-alignment similarity. Japanese\nsentences were segmented by the morphological\nanalyzer MeCab (version 0.996, IPADIC), and En-\nglish sentences were tokenized by tokenizer.perl\nof Moses. Table 1 shows the numbers of the sen-\ntences in each corpus. Note that sentences with\nmore than 40 words were excluded from the train-\ning data.\n5.2 Models\nWe used the attention-based NMT (Bahdanau\net al., 2015) as a baseline-NMT, the encoder-\ndecoder-reconstructor (Tu et al., 2017) and the\nencoder-decoder-reconstructor that jointly trained\nforward translation and back-translation without\npre-training. The RNN used in the experi-\nments had 512 hidden units, 512 embedding units,\n30,000 vocabulary size and 64 batch size. We\nused Adagrad (initial learning rate 0.01) for op-\ntimizing model parameters. We trained our model\non GeForce GTX TITAN X GPU. Note that we\nset the hyper-parameter \u03bb = 1 on the encoder-\ndecoder-reconstructor same as Tu et al. (2017).\n5.3 Results\nTables 2 and 3 show the translation accuracy\nin BLEU scores, the p-value of the signi\ufb01cance\ntest by bootstrap resampling (Koehn, 2004) and\ntraining time in hours until convergence. The\nencoder-decoder-reconstructor (Tu et al., 2017)\nrequires slightly longer time to train than the\nbaseline NMT, but we emphasize that decoding\ntime remains the same with the encoder-decoder-\nreconstructor and baseline-NMT. The results show\nthat the encoder-decoder-reconstructor (Tu et al.,\n2017) signi\ufb01cantly improves translation accuracy\nby 1.01 points on ASPEC and 1.37 points on NT-\nCIR in English-Japanese translation ( p <0.05).\nHowever, it does not signi\ufb01cantly improve trans-\n\nCorpus Model English-Japanese Japanese-English\n(i) (ii) (iii) (i) (ii) (iii)\nBaseline-NMT 1,141 378 1,045 951 494 1,085\nASPEC +Reconstructor 988 336 1,042 836 418 1,014\n+Reconstructor (Jointly-Training) 1,292 446 1,147 1,106 525 1,821\nBaseline-NMT 2,122 1,015 1,106 2,521 1,073 1,630\nNTCIR +Reconstructor 1,958 922 963 2,187 987 1,422\n+Reconstructor (Jointly-Training) 1,978 916 1,078 2,475 1,107 1,610\nTable 5: Numbers of redundant and unknown word tokens.\nof encoder-decoder-reconstructor almost correctly\nattends input words.\nTable 5 shows a comparison of the number of\nword occurrences for each corpus and model. The\ncolumns show (i) the number of words that ap-\npear more frequently than the counterparts in the\nreference, and (ii) the number of words that ap-\npear more than once but are not included in the\nreference. Note that these numbers do not in-\nclude unknown words, so (iii) shows the num-\nber of unknown words. In all the cases, the\nnumber of occurrence of redundant words is re-\nduced in encoder-decoder-reconstructor. Thus,\nwe con\ufb01rmed that encoder-decoder-reconstructor\nachieves reduction of repeating and missing words\nwhile maintaining the quality of translation.\n6 Conclusion\nIn this paper, we evaluated the encoder-decoder-\nreconstructor on English-Japanese and Japanese-\nEnglish translation tasks. In addition, we evaluate\nthe effectiveness of pre-training by comparing it\nwith a jointly-trained model of forward translation\nand back-translation. Experimental results show\nthat the encoder-decoder-reconstructor offers sig-\nni\ufb01cant improvement in BLEU scores and allevi-\nates the problem of repeating and missing words\nin the translation on English-Japanese translation\ntask, and the encoder-decoder-reconstructor can\nnot be trained well without pre-training, so it\nproves that we have to train the forward transla-\ntion model in a manner similar to the conventional\nattention-based NMT as pre-training.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural Machine Translation by Jointly\nLearning to Align and Translate. Proceedings of\nthe 3rd International Conference on Learning Rep-\nresentations (ICLR), pages 1\u201315.\nKyunghyun Cho, Bart Van Merri \u00a8enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. 2014. Learn-",
            0.5871939169999791
          ]
        }
      ],
      "reference_answers": [
        "Yes Yes",
        "Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0, NTCIR PatentMT Parallel Corpus BIBREF1 Asian Scientific Paper Excerpt Corpus, NTCIR PatentMT Parallel Corpus "
      ]
    }
  },
  "average_metrics": {
    "collection": "recursive",
    "average_exact_match": 0.0,
    "average_bleu_score": 0.07525085762770886,
    "average_rouge_scores": {
      "rouge-1": {
        "precision": 0.2333894462945247,
        "recall": 0.4593553885560226,
        "f1": 0.25332751463855924
      },
      "rouge-2": {
        "precision": 0.09624439592740412,
        "recall": 0.21301591692058222,
        "f1": 0.10719192218448681
      },
      "rouge-L": {
        "precision": 0.20857063128434353,
        "recall": 0.39449391418933993,
        "f1": 0.22162905166959104
      }
    }
  }
}